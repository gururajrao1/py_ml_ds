{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gurur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of extracted text:\n",
      "Hands-On\n",
      "Machine Learning\n",
      "with Scikit-Learn\n",
      "& TensorFlow\n",
      "CONCEPTS, TOOLS, AND TECHNIQUES\n",
      "TO BUILD INTELLIGENT SYSTEMS\n",
      "Aurélien Géron\n",
      "Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "Aurélien Géron\n",
      "BBeeiijjiinngg BBoossttoonn FFaarrnnhhaamm SSeebbaassttooppooll TTookkyyoo\n",
      "Hands-On Machine Learning with Scikit-Learn and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2017 Aurélien Géron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‐\n",
      "tutional sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "Editor: Nicole Tache Indexer: Wendy Catalano\n",
      "Production Editor: Nicholas Adams Interior Designer: David Futato\n",
      "Copyeditor: Rachel Monaghan Cover Designer: Randy Comer\n",
      "Proofreader: Charles Roumeliotis Illustrator: Rebecca Demarest\n",
      "March 2017: First Edition\n",
      "Revision History for the First Edition\n",
      "2017-03-10: First Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781491962299 for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow, the cover image, and related trade dress are trademarks of O’Reilly Media,\n",
      "Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subje\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage 1:\n",
      "Hands-On\n",
      "Machine Learning\n",
      "with Scikit-Learn\n",
      "& TensorFlow\n",
      "CONCEPTS, TOOLS, AND TECHNIQUES\n",
      "TO BUILD INTELLIGENT SYSTEMS\n",
      "Aurélien Géron\n",
      "Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "Aurélien Géron\n",
      "BBeeiijjiinngg BBoossttoonn FFaarrnnhhaamm SSeebbaassttooppooll TTookkyyoo\n",
      "Hands-On Machine Learning with Scikit-Learn and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2017 Aurélien Géron. All rights reserved. Printed in the United States of America. Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‐\n",
      "tutional sales department: 800-998-9938 or corporate@oreilly.com. Editor: Nicole Tache Indexer: Wendy Catalano\n",
      "Production Editor: Nicholas Adams Interior Designer: David Futato\n",
      "Copyeditor: Rachel Monaghan Cover Designer: Randy Comer\n",
      "Proofreader: Charles Roumeliotis Illustrator: Rebecca Demarest\n",
      "March 2017: First Edition\n",
      "Revision History for the First Edition\n",
      "2017-03-10: First Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781491962299 for release details. The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow, the cover image, and related trade dress are trademarks of O’Reilly Media,\n",
      "Inc.\n",
      "\n",
      "Q: :: O’Reilly Media, Inc. The O’Reilly logo is registered trademark of\n",
      "A: Concepts, Tools, and Techniques\n",
      "\n",
      "Q: FFaarrnnhhaamm SSeebbaass\n",
      "A: Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "\n",
      "Q: : All rights reserved Imprimés in the United States of America\n",
      "A: Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 2:\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights. 978-1-491-96229-9\n",
      "[LSI]\n",
      "Table of Contents\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\n",
      "Part I. The Fundamentals of Machine Learning\n",
      "1. The Machine Learning Landscape. . . .\n",
      "\n",
      "Q: . . . . . . . . . .\n",
      "A: 978-1-491-96229-9\n",
      "\n",
      "\n",
      "Q: Sollten Sie sich nicht um die Veröffentlichung oder die Veröffentlichung dieser nderung kümmern\n",
      "A: 978-1-491-96229-9\n",
      "\n",
      "\n",
      "Q: is your own risk is subject to open source licenses or the intellectual property rights of others\n",
      "A: If any code samples\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 3:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "What Is Machine Learning? 4\n",
      "Why Use Machine Learning? 4\n",
      "Types of Machine Learning Systems 7\n",
      "Supervised/Unsupervised Learning 8\n",
      "Batch and Online Learning 14\n",
      "Instance-Based Versus Model-Based Learning 17\n",
      "Main Challenges of Machine Learning 22\n",
      "Insufficient Quantity of Training Data 22\n",
      "Nonrepresentative Training Data 24\n",
      "Poor-Quality Data 25\n",
      "Irrelevant Features 25\n",
      "Overfitting the Training Data 26\n",
      "Underfitting the Training Data 28\n",
      "Stepping Back 28\n",
      "Testing and Validating 29\n",
      "Exercises 31\n",
      "2. End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: . . . . . . . . .\n",
      "A: End-to-End Machine Learning Project.\n",
      "\n",
      "Q: Learning 17 Main Challenges of Machine Learning 22 Insufficient Quantity of Training Data 22 Nonre\n",
      "A: Nonrepresentative Training Data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 4:\n",
      "33\n",
      "Working with Real Data 33\n",
      "Look at the Big Picture 35\n",
      "Frame the Problem 35\n",
      "Select a Performance Measure 37\n",
      "iii\n",
      "Check the Assumptions 40\n",
      "Get the Data 40\n",
      "Create the Workspace 40\n",
      "Download the Data 43\n",
      "Take a Quick Look at the Data Structure 45\n",
      "Create a Test Set 49\n",
      "Discover and Visualize the Data to Gain Insights 53\n",
      "Visualizing Geographical Data 53\n",
      "Looking for Correlations 55\n",
      "Experimenting with Attribute Combinations 58\n",
      "Prepare the Data for Machine Learning Algorithms 59\n",
      "Data Cleaning 60\n",
      "Handling Text and Categorical Attributes 62\n",
      "Custom Transformers 64\n",
      "Feature Scaling 65\n",
      "Transformation Pipelines 66\n",
      "Select and Train a Model 68\n",
      "Training and Evaluating on the Training Set 68\n",
      "Better Evaluation Using Cross-Validation 69\n",
      "Fine-Tune Your Model 71\n",
      "Grid Search 72\n",
      "Randomized Search 74\n",
      "Ensemble Methods 74\n",
      "Analyze the Best Models and Their Errors 74\n",
      "Evaluate Your System on the Test Set 75\n",
      "Launch, Monitor, and Maintain Your System 76\n",
      "Try It Out! 77\n",
      "Exercises 77\n",
      "3. Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: 77 Work with Real Data 33 Look at the Big Picture 35 Frame the Problem 35 Select\n",
      "A: \n",
      "iii\n",
      "\n",
      "Q: Fragen: Classification\n",
      "A: \n",
      "iii\n",
      "Check the Assumptions 40\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 5:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . 79\n",
      "MNIST 79\n",
      "Training a Binary Classifier 82\n",
      "Performance Measures 82\n",
      "Measuring Accuracy Using Cross-Validation 83\n",
      "Confusion Matrix 84\n",
      "Precision and Recall 86\n",
      "Precision/Recall Tradeoff 87\n",
      "The ROC Curve 91\n",
      "Multiclass Classification 93\n",
      "Error Analysis 96\n",
      "Multilabel Classification 100\n",
      "Multioutput Classification 101\n",
      "iv | Table of Contents\n",
      "Exercises 102\n",
      "4. Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: 83 Confusion Matrix 84 Precision and Recall 86 Precision/Recall\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 6:\n",
      "105\n",
      "Linear Regression 106\n",
      "The Normal Equation 108\n",
      "Computational Complexity 110\n",
      "Gradient Descent 111\n",
      "Batch Gradient Descent 114\n",
      "Stochastic Gradient Descent 117\n",
      "Mini-batch Gradient Descent 119\n",
      "Polynomial Regression 121\n",
      "Learning Curves 123\n",
      "Regularized Linear Models 127\n",
      "Ridge Regression 127\n",
      "Lasso Regression 130\n",
      "Elastic Net 132\n",
      "Early Stopping 133\n",
      "Logistic Regression 134\n",
      "Estimating Probabilities 134\n",
      "Training and Cost Function 135\n",
      "Decision Boundaries 136\n",
      "Softmax Regression 139\n",
      "Exercises 142\n",
      "5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n",
      "Linear SVM Classification 145\n",
      "Soft Margin Classification 146\n",
      "Nonlinear SVM Classification 149\n",
      "Polynomial Kernel 150\n",
      "Adding Similarity Features 151\n",
      "Gaussian RBF Kernel 152\n",
      "Computational Complexity 153\n",
      "SVM Regression 154\n",
      "Under the Hood 156\n",
      "Decision Function and Predictions 156\n",
      "Training Objective 157\n",
      "Quadratic Programming 159\n",
      "The Dual Problem 160\n",
      "Kernelized SVM 161\n",
      "Online SVMs 164\n",
      "Exercises 165\n",
      "Table of Contents | v\n",
      "6. Decision Trees. . . . . . . . . . . . . . .\n",
      "\n",
      "Q: 153 Polynomial Kernel 150 Adding Similarity Features 151 Gaus\n",
      "A: \n",
      "Gaussian RBF Kernel\n",
      "\n",
      "Q: 114 Stochastic Gradient Descent 117 Mini-batch Gradient Descent\n",
      "A: \n",
      "Batch Gradient Descent 114\n",
      "\n",
      "\n",
      "Q: 156 Decision Function and Predictions 156 Training Objective 157 Quadratic Programming\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 7:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n",
      "Training and Visualizing a Decision Tree 167\n",
      "Making Predictions 169\n",
      "Estimating Class Probabilities 171\n",
      "The CART Training Algorithm 171\n",
      "Computational Complexity 172\n",
      "Gini Impurity or Entropy? 172\n",
      "Regularization Hyperparameters 173\n",
      "Regression 175\n",
      "Instability 177\n",
      "Exercises 178\n",
      "7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Voting Classifiers 181\n",
      "Bagging and Pasting 185\n",
      "Bagging and Pasting in Scikit-Learn 186\n",
      "Out-of-Bag Evaluation 187\n",
      "Random Patches and Random Subspaces 188\n",
      "Random Forests 189\n",
      "Extra-Trees 190\n",
      "Feature Importance 190\n",
      "Boosting 191\n",
      "AdaBoost 192\n",
      "Gradient Boosting 195\n",
      "Stacking 200\n",
      "Exercises 202\n",
      "8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: 187 Random Patches and Random Subspaces 188 Random Forests 189 Extra\n",
      "A: \n",
      "7. Ensemble Learning and Random Forests.\n",
      "\n",
      "Q: 167 171 The CART Training Algorithm 171 Computational Complex\n",
      "A: \n",
      "Estimating Class Probabilities\n",
      "\n",
      "Q: 172 Regularization Hyperparameters 173 Regression 175 Instability\n",
      "A: \n",
      "Regression 175\n",
      "Instability 177\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 8:\n",
      ". . . . . . . . . . . . . . . . . 205\n",
      "The Curse of Dimensionality 206\n",
      "Main Approaches for Dimensionality Reduction 207\n",
      "Projection 207\n",
      "Manifold Learning 210\n",
      "PCA 211\n",
      "Preserving the Variance 211\n",
      "Principal Components 212\n",
      "Projecting Down to d Dimensions 213\n",
      "Using Scikit-Learn 214\n",
      "Explained Variance Ratio 214\n",
      "Choosing the Right Number of Dimensions 215\n",
      "PCA for Compression 216\n",
      "Incremental PCA 217\n",
      "Randomized PCA 218\n",
      "vi | Table of Contents\n",
      "Kernel PCA 218\n",
      "Selecting a Kernel and Tuning Hyperparameters 219\n",
      "LLE 221\n",
      "Other Dimensionality Reduction Techniques 223\n",
      "Exercises 224\n",
      "Part II. Neural Networks and Deep Learning\n",
      "9. Up and Running with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: 214 Preserving the Variance 211 Principal Components 212 Projecting Down\n",
      "A: \n",
      "Using Scikit-Learn\n",
      "\n",
      "Q: 214 Preserving the Variance 211 Principal Components 212 Projecting down\n",
      "A: \n",
      "Using Scikit-Learn\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 9:\n",
      "229\n",
      "Installation 232\n",
      "Creating Your First Graph and Running It in a Session 232\n",
      "Managing Graphs 234\n",
      "Lifecycle of a Node Value 235\n",
      "Linear Regression with TensorFlow 235\n",
      "Implementing Gradient Descent 237\n",
      "Manually Computing the Gradients 237\n",
      "Using autodiff 238\n",
      "Using an Optimizer 239\n",
      "Feeding Data to the Training Algorithm 239\n",
      "Saving and Restoring Models 241\n",
      "Visualizing the Graph and Training Curves Using TensorBoard 242\n",
      "Name Scopes 245\n",
      "Modularity 246\n",
      "Sharing Variables 248\n",
      "Exercises 251\n",
      "10. Introduction to Artificial Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n",
      "From Biological to Artificial Neurons 254\n",
      "Biological Neurons 255\n",
      "Logical Computations with Neurons 256\n",
      "The Perceptron 257\n",
      "Multi-Layer Perceptron and Backpropagation 261\n",
      "Training an MLP with TensorFlow’s High-Level API 264\n",
      "Training a DNN Using Plain TensorFlow 265\n",
      "Construction Phase 265\n",
      "Execution Phase 269\n",
      "Using the Neural Network 270\n",
      "Fine-Tuning Neural Network Hyperparameters 270\n",
      "Number of Hidden Layers 270\n",
      "Number of Neurons per Hidden Layer 272\n",
      "Activation Functions 272\n",
      "Table of Contents | vii\n",
      "Exercises 273\n",
      "11. Training Deep Neural Nets. . . .\n",
      "\n",
      "Q: 232 Creating Your First Graph and Running It in a Session 232\n",
      "A: 229\n",
      "Installation\n",
      "\n",
      "Q: 229 Installation 232 Creating Your First Graph and Running It in a Ses\n",
      "A: \n",
      "Creating Your First Graph and Running It in a Session\n",
      "\n",
      "Q: Neurons 254 Biological Neurons 255 Logical Computations with Neuro\n",
      "A: Artificial\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 10:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n",
      "Vanishing/Exploding Gradients Problems 275\n",
      "Xavier and He Initialization 277\n",
      "Nonsaturating Activation Functions 279\n",
      "Batch Normalization 282\n",
      "Gradient Clipping 286\n",
      "Reusing Pretrained Layers 286\n",
      "Reusing a TensorFlow Model 287\n",
      "Reusing Models from Other Frameworks 288\n",
      "Freezing the Lower Layers 289\n",
      "Caching the Frozen Layers 290\n",
      "Tweaking, Dropping, or Replacing the Upper Layers 290\n",
      "Model Zoos 291\n",
      "Unsupervised Pretraining 291\n",
      "Pretraining on an Auxiliary Task 292\n",
      "Faster Optimizers 293\n",
      "Momentum optimization 294\n",
      "Nesterov Accelerated Gradient 295\n",
      "AdaGrad 296\n",
      "RMSProp 298\n",
      "Adam Optimization 298\n",
      "Learning Rate Scheduling 300\n",
      "Avoiding Overfitting Through Regularization 302\n",
      "Early Stopping 303\n",
      "ℓ and ℓ Regularization 303\n",
      "1 2\n",
      "Dropout 304\n",
      "Max-Norm Regularization 307\n",
      "Data Augmentation 309\n",
      "Practical Guidelines 310\n",
      "Exercises 311\n",
      "12. Distributing TensorFlow Across Devices and Servers. . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: 290 Tweaking, Dropping, or Replacing the Upper Layers\n",
      "A: \n",
      "Model Zoos\n",
      "\n",
      "Q: 287 Reusing Models from Other Frameworks 288 Freezing the Lower Layers 2\n",
      "A: \n",
      "Reusing a TensorFlow Model 287\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 11:\n",
      "313\n",
      "Multiple Devices on a Single Machine 314\n",
      "Installation 314\n",
      "Managing the GPU RAM 317\n",
      "Placing Operations on Devices 318\n",
      "Parallel Execution 321\n",
      "Control Dependencies 323\n",
      "Multiple Devices Across Multiple Servers 323\n",
      "Opening a Session 325\n",
      "viii | Table of Contents\n",
      "The Master and Worker Services 325\n",
      "Pinning Operations Across Tasks 326\n",
      "Sharding Variables Across Multiple Parameter Servers 327\n",
      "Sharing State Across Sessions Using Resource Containers 328\n",
      "Asynchronous Communication Using TensorFlow Queues 329\n",
      "Loading Data Directly from the Graph 335\n",
      "Parallelizing Neural Networks on a TensorFlow Cluster 342\n",
      "One Neural Network per Device 342\n",
      "In-Graph Versus Between-Graph Replication 343\n",
      "Model Parallelism 345\n",
      "Data Parallelism 347\n",
      "Exercises 352\n",
      "13. Convolutional Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "The Architecture of the Visual Cortex 354\n",
      "Convolutional Layer 355\n",
      "Filters 357\n",
      "Stacking Multiple Feature Maps 358\n",
      "TensorFlow Implementation 360\n",
      "Memory Requirements 362\n",
      "Pooling Layer 363\n",
      "CNN Architectures 365\n",
      "LeNet-5 366\n",
      "AlexNet 367\n",
      "GoogLeNet 368\n",
      "ResNet 372\n",
      "Exercises 376\n",
      "14. Recurrent Neural Networks. .\n",
      "\n",
      "Q: Using Multiple Feature Maps 358 TensorFlow Implementation 360 Memory Requi\n",
      "A: 362\n",
      "Pooling Layer 363\n",
      "\n",
      "Q: Servers 323 Opening a Session 325 viii | Table of Content\n",
      "A: \n",
      "Multiple Devices\n",
      "\n",
      "Q: Fragen: Convolutional Neural Networks\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 12:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379\n",
      "Recurrent Neurons 380\n",
      "Memory Cells 382\n",
      "Input and Output Sequences 382\n",
      "Basic RNNs in TensorFlow 384\n",
      "Static Unrolling Through Time 385\n",
      "Dynamic Unrolling Through Time 387\n",
      "Handling Variable Length Input Sequences 387\n",
      "Handling Variable-Length Output Sequences 388\n",
      "Training RNNs 389\n",
      "Training a Sequence Classifier 389\n",
      "Training to Predict Time Series 392\n",
      "Creative RNN 396\n",
      "Deep RNNs 396\n",
      "Table of Contents | ix\n",
      "Distributing a Deep RNN Across Multiple GPUs 397\n",
      "Applying Dropout 399\n",
      "The Difficulty of Training over Many Time Steps 400\n",
      "LSTM Cell 401\n",
      "Peephole Connections 403\n",
      "GRU Cell 404\n",
      "Natural Language Processing 405\n",
      "Word Embeddings 405\n",
      "An Encoder–Decoder Network for Machine Translation 407\n",
      "Exercises 410\n",
      "15. Autoencoders. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: 385 Dynamic Unrolling Through Time 385 Dynamic Unrolling Through Time 3\n",
      "A: 379\n",
      "\n",
      "Q: Long-Language Input Sequences 384 Static Unrolling Through\n",
      "A: 379\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 13:\n",
      ". . . . . . . . . . . . . . . . 411\n",
      "Efficient Data Representations 412\n",
      "Performing PCA with an Undercomplete Linear Autoencoder 413\n",
      "Stacked Autoencoders 415\n",
      "TensorFlow Implementation 416\n",
      "Tying Weights 417\n",
      "Training One Autoencoder at a Time 418\n",
      "Visualizing the Reconstructions 420\n",
      "Visualizing Features 421\n",
      "Unsupervised Pretraining Using Stacked Autoencoders 422\n",
      "Denoising Autoencoders 424\n",
      "TensorFlow Implementation 425\n",
      "Sparse Autoencoders 426\n",
      "TensorFlow Implementation 427\n",
      "Variational Autoencoders 428\n",
      "Generating Digits 431\n",
      "Other Autoencoders 432\n",
      "Exercises 433\n",
      "16. Reinforcement Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437\n",
      "Learning to Optimize Rewards 438\n",
      "Policy Search 440\n",
      "Introduction to OpenAI Gym 441\n",
      "Neural Network Policies 444\n",
      "Evaluating Actions: The Credit Assignment Problem 447\n",
      "Policy Gradients 448\n",
      "Markov Decision Processes 453\n",
      "Temporal Difference Learning and Q-Learning 457\n",
      "Exploration Policies 459\n",
      "Approximate Q-Learning 460\n",
      "Learning to Play Ms. Pac-Man Using Deep Q-Learning 460\n",
      "x | Table of Contents\n",
      "Exercises 469\n",
      "Thank You! 470\n",
      "A. Exercise Solutions.\n",
      "\n",
      "Q: 421 Unsupervised Pretraining Using Stacked Autoencoders 422 Deno\n",
      "A: \n",
      "Visualizing Features\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 14:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\n",
      "B. Machine Learning Project Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497\n",
      "C. SVM Dual Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503\n",
      "D. Autodiff. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Q: Fragen: 471 B Automat\n",
      "A: Machine Learning Project Checklist\n",
      "\n",
      "Q: Fragen\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 15:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507\n",
      "E. Other Popular ANN Architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515\n",
      "Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525\n",
      "Table of Contents | xi\n",
      "Preface\n",
      "The Machine Learning Tsunami\n",
      "In 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\n",
      "network capable of recognizing handwritten digits with state-of-the-art precision\n",
      "(>98%).\n",
      "\n",
      "Q: 0\n",
      "A: \n",
      "\n",
      "\n",
      "Q: Fragen: 507 E Weitere Popular ANN Architectures\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 16:\n",
      "They branded this technique “Deep Learning.” Training a deep neural net\n",
      "was widely considered impossible at the time,2 and most researchers had abandoned\n",
      "the idea since the 1990s. This paper revived the interest of the scientific community\n",
      "and before long many new papers demonstrated that Deep Learning was not only\n",
      "possible, but capable of mind-blowing achievements that no other Machine Learning\n",
      "(ML) technique could hope to match (with the help of tremendous computing power\n",
      "and great amounts of data). This enthusiasm soon extended to many other areas of\n",
      "Machine Learning. Fast-forward 10 years and Machine Learning has conquered the industry: it is now at\n",
      "the heart of much of the magic in today’s high-tech products, ranking your web\n",
      "search results, powering your smartphone’s speech recognition, and recommending\n",
      "videos, beating the world champion at the game of Go. Before you know it, it will be\n",
      "driving your car. Machine Learning in Your Projects\n",
      "So naturally you are excited about Machine Learning and you would love to join the\n",
      "party! Perhaps you would like to give your homemade robot a brain of its own? Make it rec‐\n",
      "ognize faces? Or learn to walk around?\n",
      "\n",
      "Q: ,, and you would love to join the party! a robot? Make it\n",
      "A: Machine Learning\n",
      "\n",
      "Q: the idea of Deep Learning. the idea of Deep Learning, of mind-blowing\n",
      "A: Machine Learning\n",
      "\n",
      "\n",
      "Q: Dieses Paper reanimierte die Interesse der Wissenschaft und hat in der Tat viele neue papers\n",
      "A: Machine Learning\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 17:\n",
      "1 Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/. 2 Despite the fact that Yann Lecun’s deep convolutional neural networks had worked well for image recognition\n",
      "since the 1990s, although they were not as general purpose. xiii\n",
      "Or maybe your company has tons of data (user logs, financial data, production data,\n",
      "machine sensor data, hotline stats, HR reports, etc. ), and more than likely you could\n",
      "unearth some hidden gems if you just knew where to look; for example:\n",
      "• Segment customers and find the best marketing strategy for each group\n",
      "• Recommend products for each client based on what similar clients bought\n",
      "• Detect which transactions are likely to be fraudulent\n",
      "• Predict next year’s revenue\n",
      "• And more\n",
      "Whatever the reason, you have decided to learn Machine Learning and implement it\n",
      "in your projects. Great idea! Objective and Approach\n",
      "This book assumes that you know close to nothing about Machine Learning. Its goal\n",
      "is to give you the concepts, the intuitions, and the tools you need to actually imple‐\n",
      "ment programs capable of learning from data.\n",
      "\n",
      "Q: the best marketing strategy for each group • Detect which transactions are likely to be fraudulent • Pre\n",
      "A: Segment customers\n",
      "\n",
      "Q: s neural networks had worked well for image recognition since the 1990s, although they were not\n",
      "A: as general purpose\n",
      "\n",
      "Q: xiii Or maybe your company has tons of data (user logs, financial\n",
      "A: more than likely you could\n",
      "unearth some hidden gems\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 18:\n",
      "We will cover a large number of techniques, from the simplest and most commonly\n",
      "used (such as linear regression) to some of the Deep Learning techniques that regu‐\n",
      "larly win competitions. Rather than implementing our own toy versions of each algorithm, we will be using\n",
      "actual production-ready Python frameworks:\n",
      "• Scikit-Learn is very easy to use, yet it implements many Machine Learning algo‐\n",
      "rithms efficiently, so it makes for a great entry point to learn Machine Learning. • TensorFlow is a more complex library for distributed numerical computation\n",
      "using data flow graphs. It makes it possible to train and run very large neural net‐\n",
      "works efficiently by distributing the computations across potentially thousands\n",
      "of multi-GPU servers. TensorFlow was created at Google and supports many of\n",
      "their large-scale Machine Learning applications. It was open-sourced in Novem‐\n",
      "ber 2015. The book favors a hands-on approach, growing an intuitive understanding of\n",
      "Machine Learning through concrete working examples and just a little bit of theory. While you can read this book without picking up your laptop, we highly recommend\n",
      "you experiment with the code examples available online as Jupyter notebooks at\n",
      "https://github.com/ageron/handson-ml.\n",
      "\n",
      "Q: Machine Learning. We will cover a large number of techniques, from the simplest and most\n",
      "A: linear regression\n",
      "\n",
      "Q: a variety of techniques, from the simplest and most commonly used (such as linear regression\n",
      "A: Deep Learning techniques\n",
      "\n",
      "Q: questions: Instead of implementing our own toy versions of each algorithm, we will be using\n",
      "A: actual production-ready Python frameworks\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 19:\n",
      "xiv | Preface\n",
      "Prerequisites\n",
      "This book assumes that you have some Python programming experience and that you\n",
      "are familiar with Python’s main scientific libraries, in particular NumPy, Pandas, and\n",
      "Matplotlib. Also, if you care about what’s under the hood you should have a reasonable under‐\n",
      "standing of college-level math as well (calculus, linear algebra, probabilities, and sta‐\n",
      "tistics). If you don’t know Python yet, http://learnpython.org/ is a great place to start. The offi‐\n",
      "cial tutorial on python.org is also quite good. If you have never used Jupyter, Chapter 2 will guide you through installation and the\n",
      "basics: it is a great tool to have in your toolbox. If you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\n",
      "books include a few tutorials. There is also a quick math tutorial for linear algebra. Roadmap\n",
      "This book is organized in two parts. Part I, The Fundamentals of Machine Learning,\n",
      "covers the following topics:\n",
      "• What is Machine Learning? What problems does it try to solve? What are the\n",
      "main categories and fundamental concepts of Machine Learning systems? • The main steps in a typical Machine Learning project. • Learning by fitting a model to data.\n",
      "\n",
      "Q: python.org is a great tool to have in your toolbox.\n",
      "A: Jupyter\n",
      "\n",
      "Q: xiv | Preface Prerequisites This book assumes that you have some Python\n",
      "A: programming experience\n",
      "\n",
      "Q: questions: Also, if you care about what’s under the hood you should have\n",
      "A: a reasonable under‐\n",
      "standing of college-level math\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 20:\n",
      "• Optimizing a cost function. • Handling, cleaning, and preparing data. • Selecting and engineering features. • Selecting a model and tuning hyperparameters using cross-validation. • The main challenges of Machine Learning, in particular underfitting and overfit‐\n",
      "ting (the bias/variance tradeoff). • Reducing the dimensionality of the training data to fight the curse of dimension‐\n",
      "ality. • The most common learning algorithms: Linear and Polynomial Regression,\n",
      "Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\n",
      "Trees, Random Forests, and Ensemble methods. Preface | xv\n",
      "Part II, Neural Networks and Deep Learning, covers the following topics:\n",
      "• What are neural nets? What are they good for? • Building and training neural nets using TensorFlow. • The most important neural net architectures: feedforward neural nets, convolu‐\n",
      "tional nets, recurrent nets, long short-term memory (LSTM) nets, and autoen‐\n",
      "coders. • Techniques for training deep neural nets. • Scaling neural networks for huge datasets. • Reinforcement learning. The first part is based mostly on Scikit-Learn while the second part uses TensorFlow. Don’t jump into deep waters too hastily: while Deep Learning is no\n",
      "doubt one of the most exciting areas in Machine Learning, you\n",
      "should master the fundamentals first.\n",
      "\n",
      "Q: a cost function. • Handling, cleaning, and preparing data. • Handling\n",
      "A: Optimizing\n",
      "\n",
      "Q: Optimizing a cost function • Handling, cleaning, and preparing data\n",
      "A: dimension‐\n",
      "ality\n",
      "\n",
      "Q: generating questions: • Handling, cleaning, and preparing data • Selecting and engineering features\n",
      "A: Optimizing a cost function\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 21:\n",
      "Moreover, most problems\n",
      "can be solved quite well using simpler techniques such as Random\n",
      "Forests and Ensemble methods (discussed in Part I). Deep Learn‐\n",
      "ing is best suited for complex problems such as image recognition,\n",
      "speech recognition, or natural language processing, provided you\n",
      "have enough data, computing power, and patience. Other Resources\n",
      "Many resources are available to learn about Machine Learning. Andrew Ng’s ML\n",
      "course on Coursera and Geoffrey Hinton’s course on neural networks and Deep\n",
      "Learning are amazing, although they both require a significant time investment\n",
      "(think months). There are also many interesting websites about Machine Learning, including of\n",
      "course Scikit-Learn’s exceptional User Guide. You may also enjoy Dataquest, which\n",
      "provides very nice interactive tutorials, and ML blogs such as those listed on Quora. Finally, the Deep Learning website has a good list of resources to learn more. Of course there are also many other introductory books about Machine Learning, in\n",
      "particular:\n",
      "• Joel Grus, Data Science from Scratch (O’Reilly). This book presents the funda‐\n",
      "mentals of Machine Learning, and implements some of the main algorithms in\n",
      "pure Python (from scratch, as the name suggests). • Stephen Marsland, Machine Learning: An Algorithmic Perspective (Chapman and\n",
      "Hall).\n",
      "\n",
      "Q: ,, and Deep Learning is best suited for complex problems such as image recognition, speech\n",
      "A: Deep Learn‐\n",
      "ing\n",
      "\n",
      "Q: : Moreover, most problems can be solved quite well using simple techniques such as Random Forests\n",
      "A: Ensemble methods\n",
      "\n",
      "Q: : Deep Learn ing is best suited for complex problems such as image recognition, speech\n",
      "A: Deep Learn‐\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 22:\n",
      "This book is a great introduction to Machine Learning, covering a wide\n",
      "xvi | Preface\n",
      "range of topics in depth, with code examples in Python (also from scratch, but\n",
      "using NumPy). • Sebastian Raschka, Python Machine Learning (Packt Publishing). Also a great\n",
      "introduction to Machine Learning, this book leverages Python open source libra‐\n",
      "ries (Pylearn 2 and Theano). • Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\n",
      "Data (AMLBook). A rather theoretical approach to ML, this book provides deep\n",
      "insights, in particular on the bias/variance tradeoff (see Chapter 4). • Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach, 3rd\n",
      "Edition (Pearson). This is a great (and huge) book covering an incredible amount\n",
      "of topics, including Machine Learning. It helps put ML into perspective. Finally, a great way to learn is to join ML competition websites such as Kaggle.com\n",
      "this will allow you to practice your skills on real-world problems, with help and\n",
      "insights from some of the best ML professionals out there. Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "Italic\n",
      "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
      "\n",
      "Q: book covering an incredible amount of topics, including Machine Learning. It provides deep insights, in particular\n",
      "A: Artificial Intelligence: A Modern Approach\n",
      "\n",
      "Q: Fragen: This book is a great introduction to Machine Learning, covering a wide xvi\n",
      "A: \n",
      "\n",
      "\n",
      "Q: • Sebastian Raschka, Python Machine Learning (Packt Publishing) Also a great introduction\n",
      "A: \n",
      "xvi | Preface\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 23:\n",
      "Constant width\n",
      "Used for program listings, as well as within paragraphs to refer to program ele‐\n",
      "ments such as variable or function names, databases, data types, environment\n",
      "variables, statements and keywords. Constant width bold\n",
      "Shows commands or other text that should be typed literally by the user. Constant width italic\n",
      "Shows text that should be replaced with user-supplied values or by values deter‐\n",
      "mined by context. This element signifies a tip or suggestion. Preface | xvii\n",
      "This element signifies a general note. This element indicates a warning or caution. Using Code Examples\n",
      "Supplemental material (code examples, exercises, etc.) is available for download at\n",
      "https://github.com/ageron/handson-ml. This book is here to help you get your job done. In general, if example code is offered\n",
      "with this book, you may use it in your programs and documentation. You do not\n",
      "need to contact us for permission unless you’re reproducing a significant portion of\n",
      "the code. For example, writing a program that uses several chunks of code from this\n",
      "book does not require permission. Selling or distributing a CD-ROM of examples\n",
      "from O’Reilly books does require permission. Answering a question by citing this\n",
      "book and quoting example code does not require permission.\n",
      "\n",
      "Q: a tip or suggestion. This element signifies a general note. This element indicates\n",
      "A: Constant width italic\n",
      "\n",
      "Q: ments,, statements and keywords Constant width ments, Questions: Cons\n",
      "A: \n",
      "book\n",
      "\n",
      "Q: italic Shows text that should be replaced with user-supplied values or by values\n",
      "A: Constant width\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 24:\n",
      "Incorporating a signifi‐\n",
      "cant amount of example code from this book into your product’s documentation does\n",
      "require permission. We appreciate, but do not require, attribution. An attribution usually includes the\n",
      "title, author, publisher, and ISBN. For example: “Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow by Aurélien Géron (O’Reilly). Copyright 2017 Aurélien\n",
      "Géron, 978-1-491-96229-9.”\n",
      "If you feel your use of code examples falls outside fair use or the permission given\n",
      "above, feel free to contact us at permissions@oreilly.com. O’Reilly Safari\n",
      "Safari (formerly Safari Books Online) is a membership-based\n",
      "training and reference platform for enterprise, government,\n",
      "educators, and individuals. Members have access to thousands of books, training videos, Learning Paths, interac‐\n",
      "tive tutorials, and curated playlists from over 250 publishers, including O’Reilly\n",
      "Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\n",
      "sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\n",
      "xviii | Preface\n",
      "John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\n",
      "Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\n",
      "Course Technology, among others. For more information, please visit http://oreilly.com/safari.\n",
      "\n",
      "Q: cant amount of example code from this book into your product’s documentation does require permission\n",
      "A: Incorporating a signifi‐\n",
      "\n",
      "Q: cant amount of example code from this book into your product’s documentation does require permission We\n",
      "A: Incorporating a signifi‐\n",
      "\n",
      "Q: We appreciate, but do not require, attribution An attribution usually includes the title, author,\n",
      "A: publisher, and ISBN\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 25:\n",
      "How to Contact Us\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "O’Reilly Media, Inc.\n",
      "1005 Gravenstein Highway North\n",
      "Sebastopol, CA 95472\n",
      "800-998-9938 (in the United States or Canada)\n",
      "707-829-0515 (international or local)\n",
      "707-829-0104 (fax)\n",
      "We have a web page for this book, where we list errata, examples, and any additional\n",
      "information. You can access this page at http://bit.ly/hands-on-machine-learning-\n",
      "with-scikit-learn-and-tensorflow. To comment or ask technical questions about this book, send email to bookques‐\n",
      "tions@oreilly.com. For more information about our books, courses, conferences, and news, see our web‐\n",
      "site at http://www.oreilly.com. Find us on Facebook: http://facebook.com/oreilly\n",
      "Follow us on Twitter: http://twitter.com/oreillymedia\n",
      "Watch us on YouTube: http://www.youtube.com/oreillymedia\n",
      "Acknowledgments\n",
      "I would like to thank my Google colleagues, in particular the YouTube video classifi‐\n",
      "cation team, for teaching me so much about Machine Learning. I could never have\n",
      "started this project without them. Special thanks to my personal ML gurus: Clément\n",
      "Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James Pack, Alexander\n",
      "Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn, Rich Washington,\n",
      "and everyone at YouTube Paris.\n",
      "\n",
      "Q: ,, and other ML gurus,, and others at YouTube Paris.\n",
      "A: Special thanks to my personal ML gurus: Clément\n",
      "Courbet\n",
      "\n",
      "Q: Gagne questions: How to Contact Us Please address comments and questions concerning this book to the publisher:\n",
      "A: \n",
      "800-998-9938\n",
      "\n",
      "Q: Generieren Sie Fragen: Sie können diese Seite at http://bit.ly/hands-\n",
      "A: \n",
      "with-scikit-learn-and-tensorflow\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 26:\n",
      "I am incredibly grateful to all the amazing people who took time out of their busy\n",
      "lives to review my book in so much detail. Thanks to Pete Warden for answering all\n",
      "my TensorFlow questions, reviewing Part II, providing many interesting insights, and\n",
      "of course for being part of the core TensorFlow team. You should definitely check out\n",
      "Preface | xix\n",
      "his blog! Many thanks to Lukas Biewald for his very thorough review of Part II: he left\n",
      "no stone unturned, tested all the code (and caught a few errors), made many great\n",
      "suggestions, and his enthusiasm was contagious. You should check out his blog and\n",
      "his cool robots! Thanks to Justin Francis, who also reviewed Part II very thoroughly,\n",
      "catching errors and providing great insights, in particular in Chapter 16. Check out\n",
      "his posts on TensorFlow! Huge thanks as well to David Andrzejewski, who reviewed Part I and provided\n",
      "incredibly useful feedback, identifying unclear sections and suggesting how to\n",
      "improve them. Check out his website! Thanks to Grégoire Mesnil, who reviewed\n",
      "Part II and contributed very interesting practical advice on training neural networks.\n",
      "\n",
      "Q: and, and, and, and provided incredibly useful feedback, identifying unclear\n",
      "A: David Andrzejewski\n",
      "\n",
      "Q: :, reviewing Part II, providing many interesting insights, and of course for being part of\n",
      "A: Pete Warden\n",
      "\n",
      "Q: questions: Thanks to Pete Warden for answering all my TensorFlow questions, reviewing Part\n",
      "A: \n",
      "Preface | xix\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 27:\n",
      "Thanks as well to Eddy Hung, Salim Sémaoune, Karim Matrah, Ingrid von Glehn,\n",
      "Iain Smears, and Vincent Guilbeau for reviewing Part I and making many useful sug‐\n",
      "gestions. And I also wish to thank my father-in-law, Michel Tessier, former mathe‐\n",
      "matics teacher and now a great translator of Anton Chekhov, for helping me iron out\n",
      "some of the mathematics and notations in this book and reviewing the linear algebra\n",
      "Jupyter notebook. And of course, a gigantic “thank you” to my dear brother Sylvain, who reviewed every\n",
      "single chapter, tested every line of code, provided feedback on virtually every section,\n",
      "and encouraged me from the first line to the last. Love you, bro! Many thanks as well to O’Reilly’s fantastic staff, in particular Nicole Tache, who gave\n",
      "me insightful feedback, always cheerful, encouraging, and helpful. Thanks as well to\n",
      "Marie Beaugureau, Ben Lorica, Mike Loukides, and Laurel Ruma for believing in this\n",
      "project and helping me define its scope.\n",
      "\n",
      "Q: , and for helping me define its scope. Thanks to Eddy Hung, Salim S\n",
      "A: \n",
      "project\n",
      "\n",
      "Q: s, and, for helping me iron out some of the mathematics and notations in this\n",
      "A: Michel Tessier\n",
      "\n",
      "Q: a huge “thank you” to my dear brother Sylvain, who reviewed every chapter\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 28:\n",
      "Thanks to Matt Hacker and all of the Atlas\n",
      "team for answering all my technical questions regarding formatting, asciidoc, and\n",
      "LaTeX, and thanks to Rachel Monaghan, Nick Adams, and all of the production team\n",
      "for their final review and their hundreds of corrections. Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\n",
      "three wonderful kids, Alexandre, Rémi, and Gabrielle, for encouraging me to work\n",
      "hard on this book, asking many questions (who said you can’t teach neural networks\n",
      "to a seven-year-old? ), and even bringing me cookies and coffee. What more can one\n",
      "dream of? xx | Preface\n",
      "PART I\n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "CHAPTER 1\n",
      "The Machine Learning Landscape\n",
      "When most people hear “Machine Learning,” they picture a robot: a dependable but‐\n",
      "ler or a deadly Terminator depending on who you ask. But Machine Learning is not\n",
      "just a futuristic fantasy, it’s already here. In fact, it has been around for decades in\n",
      "some specialized applications, such as Optical Character Recognition (OCR).\n",
      "\n",
      "Q: ,, and, questions: Thanks to Matt Hacker and all of the Atlas\n",
      "A: formatting, asciidoc, and\n",
      "LaTeX\n",
      "\n",
      "Q: Thanks to Matt Hacker and all of the Atlas team for answering all my technical questions regarding formatting,\n",
      "A: \n",
      "LaTeX\n",
      "\n",
      "Q: PART I The Fundamentals of Machine Learning CHAPTER 1 The Machine Learning Landscape When most\n",
      "A: When most people hear “Machine Learning,” they picture a robot\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 29:\n",
      "But the\n",
      "first ML application that really became mainstream, improving the lives of hundreds\n",
      "of millions of people, took over the world back in the 1990s: it was the spam filter. Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning\n",
      "(it has actually learned so well that you seldom need to flag an email as spam any‐\n",
      "more). It was followed by hundreds of ML applications that now quietly power hun‐\n",
      "dreds of products and features that you use regularly, from better recommendations\n",
      "to voice search. Where does Machine Learning start and where does it end? What exactly does it\n",
      "mean for a machine to learn something? If I download a copy of Wikipedia, has my\n",
      "computer really “learned” something? Is it suddenly smarter? In this chapter we will\n",
      "start by clarifying what Machine Learning is and why you may want to use it. Then, before we set out to explore the Machine Learning continent, we will take a\n",
      "look at the map and learn about the main regions and the most notable landmarks:\n",
      "supervised versus unsupervised learning, online versus batch learning, instance-\n",
      "based versus model-based learning.\n",
      "\n",
      "Q: , the world back in the 1990s: it was the spam filter. It was the\n",
      "A: the\n",
      "first ML application\n",
      "\n",
      "Q: , questions: But the first ML application that really became mainstream, improving the lives of\n",
      "A: it was the spam filter\n",
      "\n",
      "Q: questions: Not exactly a self-aware Skynet, but it does technically qualify as\n",
      "A: Machine Learning\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 30:\n",
      "Then we will look at the workflow of a typical ML\n",
      "project, discuss the main challenges you may face, and cover how to evaluate and\n",
      "fine-tune a Machine Learning system. This chapter introduces a lot of fundamental concepts (and jargon) that every data\n",
      "scientist should know by heart. It will be a high-level overview (the only chapter\n",
      "without much code), all rather simple, but you should make sure everything is\n",
      "crystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\n",
      "get started! 3\n",
      "If you already know all the Machine Learning basics, you may want\n",
      "to skip directly to Chapter 2. If you are not sure, try to answer all\n",
      "the questions listed at the end of the chapter before moving on. What Is Machine Learning? Machine Learning is the science (and art) of programming computers so they can\n",
      "learn from data. Here is a slightly more general definition:\n",
      "[Machine Learning is the] field of study that gives computers the ability to learn\n",
      "without being explicitly programmed.\n",
      "\n",
      "Q: , and discuss the main challenges you may face., and discuss the workflow of a\n",
      "A: ML\n",
      "project\n",
      "\n",
      "Q: questions: Then we will look at the workflow of a typical ML project, discuss\n",
      "A: the main challenges you may face\n",
      "\n",
      "Q: : This chapter introduces a lot of fundamental concepts (and jargon) that every\n",
      "A: data\n",
      "scientist\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 31:\n",
      "—Arthur Samuel, 1959\n",
      "And a more engineering-oriented one:\n",
      "A computer program is said to learn from experience E with respect to some task T\n",
      "and some performance measure P, if its performance on T, as measured by P, improves\n",
      "with experience E.\n",
      "—Tom Mitchell, 1997\n",
      "For example, your spam filter is a Machine Learning program that can learn to flag\n",
      "spam given examples of spam emails (e.g., flagged by users) and examples of regular\n",
      "(nonspam, also called “ham”) emails. The examples that the system uses to learn are\n",
      "called the training set. Each training example is called a training instance (or sample). In this case, the task T is to flag spam for new emails, the experience E is the training\n",
      "data, and the performance measure P needs to be defined; for example, you can use\n",
      "the ratio of correctly classified emails. This particular performance measure is called\n",
      "accuracy and it is often used in classification tasks. If you just download a copy of Wikipedia, your computer has a lot more data, but it is\n",
      "not suddenly better at any task. Thus, it is not Machine Learning. Why Use Machine Learning?\n",
      "\n",
      "Q: —Arthur Samuel, 1959 And a more engineering-oriented one: A machine learning\n",
      "A: Machine Learning\n",
      "\n",
      "Q: —Arthur Samuel, 1959 And a more engineering-oriented one: A computer program\n",
      "A: \n",
      "A computer program is said to learn from experience E\n",
      "\n",
      "Q: Questions: The examples that the system uses to learn are called the training set . Chaque training\n",
      "A: spam\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 32:\n",
      "Consider how you would write a spam filter using traditional programming techni‐\n",
      "ques (Figure 1-1):\n",
      "1. First you would look at what spam typically looks like. You might notice that\n",
      "some words or phrases (such as “4U,” “credit card,” “free,” and “amazing”) tend to\n",
      "come up a lot in the subject. Perhaps you would also notice a few other patterns\n",
      "in the sender’s name, the email’s body, and so on. 4 | Chapter 1: The Machine Learning Landscape\n",
      "2. You would write a detection algorithm for each of the patterns that you noticed,\n",
      "and your program would flag emails as spam if a number of these patterns are\n",
      "detected. 3. You would test your program, and repeat steps 1 and 2 until it is good enough. Figure 1-1. The traditional approach\n",
      "Since the problem is not trivial, your program will likely become a long list of com‐\n",
      "plex rules—pretty hard to maintain. In contrast, a spam filter based on Machine Learning techniques automatically learns\n",
      "which words and phrases are good predictors of spam by detecting unusually fre‐\n",
      "quent patterns of words in the spam examples compared to the ham examples\n",
      "(Figure 1-2).\n",
      "\n",
      "Q: , and you would test your program, and repeat steps 1 and 2 until it is good enough\n",
      "A: 3\n",
      "\n",
      "Q: 1 Firstly, you would look at what spam typically looks like.\n",
      "A: traditional programming techni‐\n",
      "ques\n",
      "\n",
      "Q: Einige hnliches sind bei Spam häufig zu eher eher\n",
      "A: \n",
      "ques\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 33:\n",
      "The program is much shorter, easier to maintain, and most likely more\n",
      "accurate. Figure 1-2. Machine Learning approach\n",
      "Why Use Machine Learning? | 5\n",
      "Moreover, if spammers notice that all their emails containing “4U” are blocked, they\n",
      "might start writing “For U” instead. A spam filter using traditional programming\n",
      "techniques would need to be updated to flag “For U” emails. If spammers keep work‐\n",
      "ing around your spam filter, you will need to keep writing new rules forever. In contrast, a spam filter based on Machine Learning techniques automatically noti‐\n",
      "ces that “For U” has become unusually frequent in spam flagged by users, and it starts\n",
      "flagging them without your intervention (Figure 1-3). Figure 1-3. Automatically adapting to change\n",
      "Another area where Machine Learning shines is for problems that either are too com‐\n",
      "plex for traditional approaches or have no known algorithm. For example, consider\n",
      "speech recognition: say you want to start simple and write a program capable of dis‐\n",
      "tinguishing the words “one” and “two.” You might notice that the word “two” starts\n",
      "with a high-pitch sound (“T”), so you could hardcode an algorithm that measures\n",
      "high-pitch sound intensity and use that to distinguish ones and twos.\n",
      "\n",
      "Q: is much shorter, easier to maintain, and most likely more accurate. Figure 1-2. Machine\n",
      "A: \n",
      "\n",
      "\n",
      "Q: Fragen:\n",
      "A: \n",
      "\n",
      "\n",
      "Q: : Figure 1-2 Machine Learning approach Why Use Machine Learning? | 5 Moreover, if\n",
      "A: if spammers notice that all their emails containing “4U”\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 34:\n",
      "Obviously this\n",
      "technique will not scale to thousands of words spoken by millions of very different\n",
      "people in noisy environments and in dozens of languages. The best solution (at least\n",
      "today) is to write an algorithm that learns by itself, given many example recordings\n",
      "for each word. Finally, Machine Learning can help humans learn (Figure 1-4): ML algorithms can be\n",
      "inspected to see what they have learned (although for some algorithms this can be\n",
      "tricky). For instance, once the spam filter has been trained on enough spam, it can\n",
      "easily be inspected to reveal the list of words and combinations of words that it\n",
      "believes are the best predictors of spam. Sometimes this will reveal unsuspected cor‐\n",
      "relations or new trends, and thereby lead to a better understanding of the problem. Applying ML techniques to dig into large amounts of data can help discover patterns\n",
      "that were not immediately apparent. This is called data mining. 6 | Chapter 1: The Machine Learning Landscape\n",
      "Figure 1-4.\n",
      "\n",
      "Q: ,,,,, and, to generate questions: This technique\n",
      "A: Machine Learning\n",
      "\n",
      "Q: questions: This technique will not scale to thousands of words spoken by millions of very different people in\n",
      "A: noisy environments and in dozens of languages\n",
      "\n",
      "Q: : to write an algorithm that learns by itself, given many examples for each word.\n",
      "A: Machine Learning\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 35:\n",
      "Machine Learning can help humans learn\n",
      "To summarize, Machine Learning is great for:\n",
      "• Problems for which existing solutions require a lot of hand-tuning or long lists of\n",
      "rules: one Machine Learning algorithm can often simplify code and perform bet‐\n",
      "ter. • Complex problems for which there is no good solution at all using a traditional\n",
      "approach: the best Machine Learning techniques can find a solution. • Fluctuating environments: a Machine Learning system can adapt to new data. • Getting insights about complex problems and large amounts of data. Types of Machine Learning Systems\n",
      "There are so many different types of Machine Learning systems that it is useful to\n",
      "classify them in broad categories based on:\n",
      "• Whether or not they are trained with human supervision (supervised, unsuper‐\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      "• Whether or not they can learn incrementally on the fly (online versus batch\n",
      "learning)\n",
      "• Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "These criteria are not exclusive; you can combine them in any way you like.\n",
      "\n",
      "Q: learn from humans. resume, Machine Learning is great for: • Problems\n",
      "A: long lists of\n",
      "rules\n",
      "\n",
      "Q: Machine Learning can help humans learn Machine Learning.  Problems for which existing solutions require\n",
      "A: a lot of hand-tuning or long lists of\n",
      "rules\n",
      "\n",
      "Q: Complex problems for which there is no good solution at all using traditional approaches: the best Machine Learning\n",
      "A: techniques can find a solution\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 36:\n",
      "For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\n",
      "Types of Machine Learning Systems | 7\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system. Let’s look at each of these criteria a bit more closely. Supervised/Unsupervised Learning\n",
      "Machine Learning systems can be classified according to the amount and type of\n",
      "supervision they get during training. There are four major categories: supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\n",
      "ing. Supervised learning\n",
      "In supervised learning, the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels (Figure 1-5). Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "A typical supervised learning task is classification. The spam filter is a good example\n",
      "of this: it is trained with many example emails along with their class (spam or ham),\n",
      "and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car,\n",
      "given a set of features (mileage, age, brand, etc.) called predictors.\n",
      "\n",
      "Q: ,,,,, filter may learn on the fly using a\n",
      "A: deep neural net\n",
      "\n",
      "Q: : a new spam filter may learn on the fly using a deep neural net\n",
      "A: ‐\n",
      "\n",
      "Q: : What are the most important questions?\n",
      "A: criteria\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 37:\n",
      "This sort of task is\n",
      "called regression (Figure 1-6).1 To train the system, you need to give it many examples\n",
      "of cars, including both their predictors and their labels (i.e., their prices). 1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\n",
      "fact that the children of tall people tend to be shorter than their parents. Since children were shorter, he called\n",
      "this regression to the mean. This name was then applied to the methods he used to analyze correlations\n",
      "between variables. 8 | Chapter 1: The Machine Learning Landscape\n",
      "In Machine Learning an attribute is a data type (e.g., “Mileage”),\n",
      "while a feature has several meanings depending on the context, but\n",
      "generally means an attribute plus its value (e.g., “Mileage =\n",
      "15,000”). Many people use the words attribute and feature inter‐\n",
      "changeably, though. Figure 1-6. Regression\n",
      "Note that some regression algorithms can be used for classification as well, and vice\n",
      "versa. For example, Logistic Regression is commonly used for classification, as it can\n",
      "output a value that corresponds to the probability of belonging to a given class (e.g.,\n",
      "20% chance of being spam).\n",
      "\n",
      "Q: : This sort of task is called regression (Figure 1-6).1 This sort of task is\n",
      "A: \n",
      "fact that the children of tall people tend to be shorter than their parents\n",
      "\n",
      "Q: : This sort of task is called regression (Figure 1-6).1 To train the system,\n",
      "A: you need to give it many examples\n",
      "of cars\n",
      "\n",
      "Q: Dieser ungewöhnliche Name ist ein statistischer Begriff, der von Francis Galton\n",
      "A: regression\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 38:\n",
      "Here are some of the most important supervised learning algorithms (covered in this\n",
      "book):\n",
      "• k-Nearest Neighbors\n",
      "• Linear Regression\n",
      "• Logistic Regression\n",
      "• Support Vector Machines (SVMs)\n",
      "• Decision Trees and Random Forests\n",
      "• Neural networks2\n",
      "2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\n",
      "machines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining. Types of Machine Learning Systems | 9\n",
      "Unsupervised learning\n",
      "In unsupervised learning, as you might guess, the training data is unlabeled\n",
      "(Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning\n",
      "Here are some of the most important unsupervised learning algorithms (we will\n",
      "cover dimensionality reduction in Chapter 8):\n",
      "• Clustering\n",
      "—k-Means\n",
      "—Hierarchical Cluster Analysis (HCA)\n",
      "—Expectation Maximization\n",
      "• Visualization and dimensionality reduction\n",
      "—Principal Component Analysis (PCA)\n",
      "—Kernel PCA\n",
      "—Locally-Linear Embedding (LLE)\n",
      "—t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
      "• Association rule learning\n",
      "—Apriori\n",
      "—Eclat\n",
      "For example, say you have a lot of data about your blog’s visitors. You may want to\n",
      "run a clustering algorithm to try to detect groups of similar visitors (Figure 1-8).\n",
      "\n",
      "Q: : • Clustering —k-Means —Hierarchical Cluster Analysis\n",
      "A: k-Nearest Neighbors\n",
      "\n",
      "Q: : Here are some of the most important supervised learning algorithms (covered in this book):\n",
      "A: k-Nearest Neighbors\n",
      "\n",
      "Q: Unsupervised learning In unsupervised learning, as you might guess, the training data is unlab\n",
      "A: k-Nearest Neighbors\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 39:\n",
      "At\n",
      "no point do you tell the algorithm which group a visitor belongs to: it finds those\n",
      "connections without your help. For example, it might notice that 40% of your visitors\n",
      "are males who love comic books and generally read your blog in the evening, while\n",
      "20% are young sci-fi lovers who visit during the weekends, and so on. If you use a\n",
      "hierarchical clustering algorithm, it may also subdivide each group into smaller\n",
      "groups. This may help you target your posts for each group. 10 | Chapter 1: The Machine Learning Landscape\n",
      "Figure 1-8. Clustering\n",
      "Visualization algorithms are also good examples of unsupervised learning algorithms:\n",
      "you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep‐\n",
      "resentation of your data that can easily be plotted (Figure 1-9). These algorithms try\n",
      "to preserve as much structure as they can (e.g., trying to keep separate clusters in the\n",
      "input space from overlapping in the visualization), so you can understand how the\n",
      "data is organized and perhaps identify unsuspected patterns. Figure 1-9.\n",
      "\n",
      "Q: the group a visitor belongs to: it finds those connections without your help . At no\n",
      "A: no point do you tell the algorithm\n",
      "\n",
      "Q: the algorithm which group a visitor belongs to: it finds those connections without your help.\n",
      "A: \n",
      "no point\n",
      "\n",
      "Q: Frequently asked questions: How do you subdivide each group into smaller groups?\n",
      "A: \n",
      "hierarchical clustering algorithm\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 40:\n",
      "Example of a t-SNE visualization highlighting semantic clusters3\n",
      "3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds,\n",
      "and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), “T-SNE visual‐\n",
      "ization of the semantic word space.”\n",
      "Types of Machine Learning Systems | 11\n",
      "A related task is dimensionality reduction, in which the goal is to simplify the data\n",
      "without losing too much information. One way to do this is to merge several correla‐\n",
      "ted features into one. For example, a car’s mileage may be very correlated with its age,\n",
      "so the dimensionality reduction algorithm will merge them into one feature that rep‐\n",
      "resents the car’s wear and tear. This is called feature extraction. It is often a good idea to try to reduce the dimension of your train‐\n",
      "ing data using a dimensionality reduction algorithm before you\n",
      "feed it to another Machine Learning algorithm (such as a super‐\n",
      "vised learning algorithm). It will run much faster, the data will take\n",
      "up less disk and memory space, and in some cases it may also per‐\n",
      "form better.\n",
      "\n",
      "Q: space semantic clusters3 3 This is a t-SNE visualization highlighting\n",
      "A: \n",
      "ization of the semantic word space\n",
      "\n",
      "Q: a semantic word space? 3 Notice how animals are rather well separated from vehicles, how horses\n",
      "A: horses are close to deer but far from birds\n",
      "\n",
      "Q: space. from Socher, Ganjoo, Manning, and Ng (2013),\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 41:\n",
      "Yet another important unsupervised task is anomaly detection—for example, detect‐\n",
      "ing unusual credit card transactions to prevent fraud, catching manufacturing defects,\n",
      "or automatically removing outliers from a dataset before feeding it to another learn‐\n",
      "ing algorithm. The system is trained with normal instances, and when it sees a new\n",
      "instance it can tell whether it looks like a normal one or whether it is likely an anom‐\n",
      "aly (see Figure 1-10). Figure 1-10. Anomaly detection\n",
      "Finally, another common unsupervised task is association rule learning, in which the\n",
      "goal is to dig into large amounts of data and discover interesting relations between\n",
      "attributes. For example, suppose you own a supermarket. Running an association rule\n",
      "on your sales logs may reveal that people who purchase barbecue sauce and potato\n",
      "chips also tend to buy steak. Thus, you may want to place these items close to each\n",
      "other. 12 | Chapter 1: The Machine Learning Landscape\n",
      "Semisupervised learning\n",
      "Some algorithms can deal with partially labeled training data, usually a lot of unla‐\n",
      "beled data and a little bit of labeled data. This is called semisupervised learning\n",
      "(Figure 1-11). Some photo-hosting services, such as Google Photos, are good examples of this.\n",
      "\n",
      "Q: detection Anomaly detection Another common unsupervised task is association rule learning . association rule\n",
      "A: to dig into large amounts of data and discover interesting relations between\n",
      "attributes\n",
      "\n",
      "Q: ing ing ing ing algorithm The system is trained with normal instances\n",
      "A: anomaly detection\n",
      "\n",
      "Q: aly (see Figure 1-10) Figure 1-10 Figure 1-10 Figure 1-10 Figure 1\n",
      "A: anom‐\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 42:\n",
      "Once\n",
      "you upload all your family photos to the service, it automatically recognizes that the\n",
      "same person A shows up in photos 1, 5, and 11, while another person B shows up in\n",
      "photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\n",
      "the system needs is for you to tell it who these people are. Just one label per person,4\n",
      "and it is able to name everyone in every photo, which is useful for searching photos. Figure 1-11. Semisupervised learning\n",
      "Most semisupervised learning algorithms are combinations of unsupervised and\n",
      "supervised algorithms. For example, deep belief networks (DBNs) are based on unsu‐\n",
      "pervised components called restricted Boltzmann machines (RBMs) stacked on top of\n",
      "one another. RBMs are trained sequentially in an unsupervised manner, and then the\n",
      "whole system is fine-tuned using supervised learning techniques. Reinforcement Learning\n",
      "Reinforcement Learning is a very different beast. The learning system, called an agent\n",
      "in this context, can observe the environment, select and perform actions, and get\n",
      "rewards in return (or penalties in the form of negative rewards, as in Figure 1-12).\n",
      "\n",
      "Q: , and it can name everyone in every photo, which is useful for searching photos. Figure 1\n",
      "A: Just one label per person\n",
      "\n",
      "Q: the service, the same person A shows up in photos 1, 5, and 11, while another person\n",
      "A: Once\n",
      "you upload all your family photos\n",
      "\n",
      "Q: : This is the unsupervised part of the algorithm (clustering) Now all the system\n",
      "A: all\n",
      "the system needs is for you to tell it who these people are\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 43:\n",
      "It\n",
      "must then learn by itself what is the best strategy, called a policy, to get the most\n",
      "reward over time. A policy defines what action the agent should choose when it is in a\n",
      "given situation. 4 That’s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\n",
      "mixes up two people who look alike, so you need to provide a few labels per person and manually clean up\n",
      "some clusters. Types of Machine Learning Systems | 13\n",
      "Figure 1-12. Reinforcement Learning\n",
      "For example, many robots implement Reinforcement Learning algorithms to learn\n",
      "how to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\n",
      "Learning: it made the headlines in March 2016 when it beat the world champion Lee\n",
      "Sedol at the game of Go. It learned its winning policy by analyzing millions of games,\n",
      "and then playing many games against itself. Note that learning was turned off during\n",
      "the games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning\n",
      "Another criterion used to classify Machine Learning systems is whether or not the\n",
      "system can learn incrementally from a stream of incoming data.\n",
      "\n",
      "Q: Machine Learning Systems the policy it learned in March 2016 when it beat the world champion Lee Se\n",
      "A: \n",
      "given situation. 4\n",
      "\n",
      "Q: the agent should choose when it is in a given situation.\n",
      "A: A policy\n",
      "\n",
      "Q: The system works perfectly. 4 That’s when the system works perfectly.\n",
      "A: It\n",
      "must then learn by itself\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 44:\n",
      "Batch learning\n",
      "In batch learning, the system is incapable of learning incrementally: it must be trained\n",
      "using all the available data. This will generally take a lot of time and computing\n",
      "resources, so it is typically done offline. First the system is trained, and then it is\n",
      "launched into production and runs without learning anymore; it just applies what it\n",
      "has learned. This is called offline learning. If you want a batch learning system to know about new data (such as a new type of\n",
      "spam), you need to train a new version of the system from scratch on the full dataset\n",
      "(not just the new data, but also the old data), then stop the old system and replace it\n",
      "with the new one. Fortunately, the whole process of training, evaluating, and launching a Machine\n",
      "Learning system can be automated fairly easily (as shown in Figure 1-3), so even a\n",
      "14 | Chapter 1: The Machine Learning Landscape\n",
      "batch learning system can adapt to change. Simply update the data and train a new\n",
      "version of the system from scratch as often as needed.\n",
      "\n",
      "Q: : Batch learning the old data, then stop the old system and replace it with the new\n",
      "A: new version of the system from scratch on the full dataset\n",
      "\n",
      "\n",
      "Q: , time and computing resources, so it is typically done offline.\n",
      "A: batch learning\n",
      "\n",
      "Q: questions: This will generally take a lot of time and computing resources, so it is typically\n",
      "A: offline\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 45:\n",
      "This solution is simple and often works fine, but training using the full set of data can\n",
      "take many hours, so you would typically train a new system only every 24 hours or\n",
      "even just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐\n",
      "dict stock prices), then you need a more reactive solution. Also, training on the full set of data requires a lot of computing resources (CPU,\n",
      "memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\n",
      "you automate your system to train from scratch every day, it will end up costing you a\n",
      "lot of money. If the amount of data is huge, it may even be impossible to use a batch\n",
      "learning algorithm. Finally, if your system needs to be able to learn autonomously and it has limited\n",
      "resources (e.g., a smartphone application or a rover on Mars), then carrying around\n",
      "large amounts of training data and taking up a lot of resources to train for hours\n",
      "every day is a showstopper. Fortunately, a better option in all these cases is to use algorithms that are capable of\n",
      "learning incrementally.\n",
      "\n",
      "Q: ,, but it can be difficult to use a batch learning algorithm. This solution is\n",
      "A: If the amount of data is huge\n",
      "\n",
      "Q: a solution that works fine. simple and often works fine, but training using the full\n",
      "A: \n",
      "take many hours\n",
      "\n",
      "Q: dict stock prices), then you need a more reactive solution.\n",
      "A: If your system needs to adapt to rapidly changing data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 46:\n",
      "Online learning\n",
      "In online learning, you train the system incrementally by feeding it data instances\n",
      "sequentially, either individually or by small groups called mini-batches. Each learning\n",
      "step is fast and cheap, so the system can learn about new data on the fly, as it arrives\n",
      "(see Figure 1-13). Figure 1-13. Online learning\n",
      "Online learning is great for systems that receive data as a continuous flow (e.g., stock\n",
      "prices) and need to adapt to change rapidly or autonomously. It is also a good option\n",
      "Types of Machine Learning Systems | 15\n",
      "if you have limited computing resources: once an online learning system has learned\n",
      "about new data instances, it does not need them anymore, so you can discard them\n",
      "(unless you want to be able to roll back to a previous state and “replay” the data). This\n",
      "can save a huge amount of space. Online learning algorithms can also be used to train systems on huge datasets that\n",
      "cannot fit in one machine’s main memory (this is called out-of-core learning). The\n",
      "algorithm loads part of the data, runs a training step on that data, and repeats the\n",
      "process until it has run on all of the data (see Figure 1-14).\n",
      "\n",
      "Q: learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine\n",
      "A: Online learning algorithms\n",
      "\n",
      "Q: Chaque learning step is fast and cheap, so the system can learn about new data on the fly\n",
      "A: \n",
      "\n",
      "\n",
      "Q: -- learning Online learning is great for systems that receive data as a continuous flow (\n",
      "A: stock\n",
      "prices\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 47:\n",
      "This whole process is usually done offline (i.e., not on the live sys‐\n",
      "tem), so online learning can be a confusing name. Think of it as\n",
      "incremental learning. Figure 1-14. Using online learning to handle huge datasets\n",
      "One important parameter of online learning systems is how fast they should adapt to\n",
      "changing data: this is called the learning rate. If you set a high learning rate, then your\n",
      "system will rapidly adapt to new data, but it will also tend to quickly forget the old\n",
      "data (you don’t want a spam filter to flag only the latest kinds of spam it was shown). Conversely, if you set a low learning rate, the system will have more inertia; that is, it\n",
      "will learn more slowly, but it will also be less sensitive to noise in the new data or to\n",
      "sequences of nonrepresentative data points. A big challenge with online learning is that if bad data is fed to the system, the sys‐\n",
      "tem’s performance will gradually decline. If we are talking about a live system, your\n",
      "clients will notice.\n",
      "\n",
      "Q: is usually done offline (i.e., not on the live sys\n",
      "A: \n",
      "tem\n",
      "\n",
      "Q: : This whole process is usually done offline (i.e., not on the live\n",
      "A: sys‐\n",
      "tem\n",
      "\n",
      "Q: : Think of it as incremental learning Figure 1-14\n",
      "A: online learning\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 48:\n",
      "For example, bad data could come from a malfunctioning sensor\n",
      "on a robot, or from someone spamming a search engine to try to rank high in search\n",
      "16 | Chapter 1: The Machine Learning Landscape\n",
      "results. To reduce this risk, you need to monitor your system closely and promptly\n",
      "switch learning off (and possibly revert to a previously working state) if you detect a\n",
      "drop in performance. You may also want to monitor the input data and react to\n",
      "abnormal data (e.g., using an anomaly detection algorithm). Instance-Based Versus Model-Based Learning\n",
      "One more way to categorize Machine Learning systems is by how they generalize. Most Machine Learning tasks are about making predictions. This means that given a\n",
      "number of training examples, the system needs to be able to generalize to examples it\n",
      "has never seen before. Having a good performance measure on the training data is\n",
      "good, but insufficient; the true goal is to perform well on new instances. There are two main approaches to generalization: instance-based learning and\n",
      "model-based learning. Instance-based learning\n",
      "Possibly the most trivial form of learning is simply to learn by heart.\n",
      "\n",
      "Q: , using an anomaly detection algorithm). Instance-Based learning Possibly the most trivial\n",
      "A: \n",
      "abnormal data\n",
      "\n",
      "Q: a malfunctioning sensor on a robot, or someone spamming a search engine\n",
      "A: instance-based learning and\n",
      "model-based learning\n",
      "\n",
      "Q: a drop in performance., questions: To reduce this risk, you need to\n",
      "A: monitor your system closely and promptly\n",
      "switch learning off\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 49:\n",
      "If you were to\n",
      "create a spam filter this way, it would just flag all emails that are identical to emails\n",
      "that have already been flagged by users—not the worst solution, but certainly not the\n",
      "best. Instead of just flagging emails that are identical to known spam emails, your spam\n",
      "filter could be programmed to also flag emails that are very similar to known spam\n",
      "emails. This requires a measure of similarity between two emails. A (very basic) simi‐\n",
      "larity measure between two emails could be to count the number of words they have\n",
      "in common. The system would flag an email as spam if it has many words in com‐\n",
      "mon with a known spam email. This is called instance-based learning: the system learns the examples by heart, then\n",
      "generalizes to new cases using a similarity measure (Figure 1-15). Figure 1-15. Instance-based learning\n",
      "Types of Machine Learning Systems | 17\n",
      "Model-based learning\n",
      "Another way to generalize from a set of examples is to build a model of these exam‐\n",
      "ples, then use that model to make predictions. This is called model-based learning\n",
      "(Figure 1-16). Figure 1-16.\n",
      "\n",
      "Q: a question: If you were to create a spam filter this way, it would flag\n",
      "A: all emails that are identical to emails\n",
      "\n",
      "Q: : If you were to create a spam filter this way, it would flag all emails that\n",
      "A: identical to emails\n",
      "\n",
      "Q: flagging that are very similar to known spam emails.\n",
      "A: Model-based learning\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 50:\n",
      "Model-based learning\n",
      "For example, suppose you want to know if money makes people happy, so you down‐\n",
      "load the Better Life Index data from the OECD’s website as well as stats about GDP\n",
      "per capita from the IMF’s website. Then you join the tables and sort by GDP per cap‐\n",
      "ita. Table 1-1 shows an excerpt of what you get. Table 1-1. Does money make people happier? Country GDP per capita (USD) Life satisfaction\n",
      "Hungary 12,240 4.9\n",
      "Korea 27,195 5.8\n",
      "France 37,675 6.5\n",
      "Australia 50,962 7.3\n",
      "United States 55,805 7.2\n",
      "Let’s plot the data for a few random countries (Figure 1-17). 18 | Chapter 1: The Machine Learning Landscape\n",
      "Figure 1-17. Do you see a trend here? There does seem to be a trend here! Although the data is noisy (i.e., partly random), it\n",
      "looks like life satisfaction goes up more or less linearly as the country’s GDP per cap‐\n",
      "ita increases. So you decide to model life satisfaction as a linear function of GDP per\n",
      "capita. This step is called model selection: you selected a linear model of life satisfac‐\n",
      "tion with just one attribute, GDP per capita (Equation 1-1). Equation 1-1.\n",
      "\n",
      "Q: tion the data for a few random countries (Figure 1-17). tion\n",
      "A: Let’s plot\n",
      "\n",
      "Q: the tables and sort by GDP per cap italy italy ita\n",
      "A: ‐\n",
      "\n",
      "Q: questions: Table 1-1 Does money make people happier? Country GDP per capita (USD) Life\n",
      "A: satisfaction\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 51:\n",
      "A simple linear model\n",
      "life_satisfaction=θ +θ ×GDP_per_capita\n",
      "0 1\n",
      "This model has two model parameters, θ and θ .5 By tweaking these parameters, you\n",
      "0 1\n",
      "can make your model represent any linear function, as shown in Figure 1-18. Figure 1-18. A few possible linear models\n",
      "5 By convention, the Greek letter θ (theta) is frequently used to represent model parameters. Types of Machine Learning Systems | 19\n",
      "Before you can use your model, you need to define the parameter values θ and θ . 0 1\n",
      "How can you know which values will make your model perform best? To answer this\n",
      "question, you need to specify a performance measure. You can either define a utility\n",
      "function (or fitness function) that measures how good your model is, or you can define\n",
      "a cost function that measures how bad it is. For linear regression problems, people\n",
      "typically use a cost function that measures the distance between the linear model’s\n",
      "predictions and the training examples; the objective is to minimize this distance.\n",
      "\n",
      "Q: 0 1 How can you know which values will make your model perform best? 0 1\n",
      "A: you need to define the parameter values θ and θ\n",
      "\n",
      "Q: 0 1 can make your model represent any linear function, as shown in Figure 1-18 Figure\n",
      "A: By tweaking these parameters\n",
      "\n",
      "Q: Questions: Figure 1-18 A few possible linear models 5 By convention, the Greek letter  (\n",
      "A: theta\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 52:\n",
      "This is where the Linear Regression algorithm comes in: you feed it your training\n",
      "examples and it finds the parameters that make the linear model fit best to your data. This is called training the model. In our case the algorithm finds that the optimal\n",
      "parameter values are θ = 4.85 and θ = 4.91 × 10–5. 0 1\n",
      "Now the model fits the training data as closely as possible (for a linear model), as you\n",
      "can see in Figure 1-19. Figure 1-19. The linear model that fits the training data best\n",
      "You are finally ready to run the model to make predictions. For example, say you\n",
      "want to know how happy Cypriots are, and the OECD data does not have the answer. Fortunately, you can use your model to make a good prediction: you look up Cyprus’s\n",
      "GDP per capita, find $22,587, and then apply your model and find that life satisfac‐\n",
      "tion is likely to be somewhere around 4.85 + 22,587 × 4.91 × 10-5 = 5.96.\n",
      "\n",
      "Q: tion is likely to be somewhere around 4.85 + 22,587  4.91\n",
      "A: life satisfac‐\n",
      "tion\n",
      "\n",
      "Q: Hier genüüs die questions: Hier füfügen Sie Fragen: Hier fü\n",
      "A: ‐\n",
      "\n",
      "Q: = 4.85 and  = 4.91  10–5  10–\n",
      "A: θ\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 53:\n",
      "To whet your appetite, Example 1-1 shows the Python code that loads the data, pre‐\n",
      "pares it,6 creates a scatterplot for visualization, and then trains a linear model and\n",
      "makes a prediction.7\n",
      "6 The code assumes that prepare_country_stats() is already defined: it merges the GDP and life satisfaction\n",
      "data into a single Pandas dataframe. 7 It’s okay if you don’t understand all the code yet; we will present Scikit-Learn in the following chapters. 20 | Chapter 1: The Machine Learning Landscape\n",
      "Example 1-1.\n",
      "\n",
      "Q: the code: 1-1 shows the Python code that loads the data, pre pares\n",
      "A: \n",
      "pares it,6\n",
      "\n",
      "Q: you don’t understand the code yet; we will present Scikit-Learn in\n",
      "A: It’s okay\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: questions: 7 It’s okay if you don’t understand all the code yet;\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 54:\n",
      "Training and running a linear model using Scikit-Learn\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "# Load the data\n",
      "oecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\n",
      "gdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
      "encoding='latin1', na_values=\"n/a\")\n",
      "# Prepare the data\n",
      "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
      "X = np.c_[country_stats[\"GDP per capita\"]]\n",
      "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
      "# Visualize the data\n",
      "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
      "plt.show()\n",
      "# Select a linear model\n",
      "lin_reg_model = sklearn.linear_model.LinearRegression()\n",
      "# Train the model\n",
      "lin_reg_model.fit(X, y)\n",
      "# Make a prediction for Cyprus\n",
      "X_new = [[22587]] # Cyprus' GDP per capita\n",
      "print(lin_reg_model.predict(X_new)) # outputs [[ 5.96242338]]\n",
      "If you had used an instance-based learning algorithm instead, you\n",
      "would have found that Slovenia has the closest GDP per capita to\n",
      "that of Cyprus ($20,732), and since the OECD data tells us that\n",
      "Slovenians’ life satisfaction is 5.7, you would have predicted a life\n",
      "satisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\n",
      "two next closest countries, you will find Portugal and Spain with\n",
      "life satisfactions of 5.1 and 6.5, respectively. Averaging these three\n",
      "values, you get 5.77, which is pretty close to your model-based pre‐\n",
      "diction.\n",
      "\n",
      "Q: a linear model using Scikit-Learn import matplotlib.py\n",
      "A: pyplot as plt\n",
      "\n",
      "Q: a question: If you zoom out and look at the two next closest countries, you will\n",
      "A: Portugal and Spain\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 55:\n",
      "This simple algorithm is called k-Nearest Neighbors regres‐\n",
      "sion (in this example, k = 3). Replacing the Linear Regression model with k-Nearest Neighbors\n",
      "regression in the previous code is as simple as replacing this line:\n",
      "clf = sklearn.linear_model.LinearRegression()\n",
      "with this one:\n",
      "clf = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n",
      "Types of Machine Learning Systems | 21\n",
      "If all went well, your model will make good predictions. If not, you may need to use\n",
      "more attributes (employment rate, health, air pollution, etc. ), get more or better qual‐\n",
      "ity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres‐\n",
      "sion model). In summary:\n",
      "• You studied the data. • You selected a model. • You trained it on the training data (i.e., the learning algorithm searched for the\n",
      "model parameter values that minimize a cost function). • Finally, you applied the model to make predictions on new cases (this is called\n",
      "inference), hoping that this model will generalize well. This is what a typical Machine Learning project looks like. In Chapter 2 you will\n",
      "experience this first-hand by going through an end-to-end project.\n",
      "\n",
      "Q: ,, the model. • You selected a model. • You trained it on\n",
      "A: training data\n",
      "\n",
      "Q: : This simple algorithm is called k-Nearest Neighbors regres\n",
      "A: ‐\n",
      "sion\n",
      "\n",
      "Q: : clf = sklearn.linear_model.Linear\n",
      "A: \n",
      "clf = sklearn.linear_model.LinearRegression\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 56:\n",
      "We have covered a lot of ground so far: you now know what Machine Learning is\n",
      "really about, why it is useful, what some of the most common categories of ML sys‐\n",
      "tems are, and what a typical project workflow looks like. Now let’s look at what can go\n",
      "wrong in learning and prevent you from making accurate predictions. Main Challenges of Machine Learning\n",
      "In short, since your main task is to select a learning algorithm and train it on some\n",
      "data, the two things that can go wrong are “bad algorithm” and “bad data.” Let’s start\n",
      "with examples of bad data. Insufficient Quantity of Training Data\n",
      "For a toddler to learn what an apple is, all it takes is for you to point to an apple and\n",
      "say “apple” (possibly repeating this procedure a few times). Now the child is able to\n",
      "recognize apples in all sorts of colors and shapes. Genius. Machine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\n",
      "ing algorithms to work properly.\n",
      "\n",
      "Q: ,, why it is useful, what some of the most common categories of ML\n",
      "A: sys‐\n",
      "tems\n",
      "\n",
      "Q: questions: You now know what Machine Learning is really about, why it is useful, what some\n",
      "A: what some of the most common categories of ML sys‐\n",
      "tems are\n",
      "\n",
      "Q: in learning and prevent you from making accurate predictions Main Challenges of Machine Learning Main Challenges of\n",
      "A: what can go\n",
      "wrong\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 57:\n",
      "Even for very simple problems you typically need\n",
      "thousands of examples, and for complex problems such as image or speech recogni‐\n",
      "tion you may need millions of examples (unless you can reuse parts of an existing\n",
      "model). 22 | Chapter 1: The Machine Learning Landscape\n",
      "The Unreasonable Effectiveness of Data\n",
      "In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric\n",
      "Brill showed that very different Machine Learning algorithms, including fairly simple\n",
      "ones, performed almost identically well on a complex problem of natural language\n",
      "disambiguation8 once they were given enough data (as you can see in Figure 1-20). Figure 1-20. The importance of data versus algorithms9\n",
      "As the authors put it: “these results suggest that we may want to reconsider the trade-\n",
      "off between spending time and money on algorithm development versus spending it\n",
      "on corpus development.”\n",
      "The idea that data matters more than algorithms for complex problems was further\n",
      "popularized by Peter Norvig et al.\n",
      "\n",
      "Q: and were almost identically well on a complex problem of natural language disambiguation8\n",
      "A: Machine Learning algorithms\n",
      "\n",
      "Q: tion, including fairly simple algorithms, performed almost identically well on a complex problem\n",
      "A: Machine Learning algorithms\n",
      "\n",
      "Q: algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural\n",
      "A: language\n",
      "disambiguation\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 58:\n",
      "in a paper titled “The Unreasonable Effectiveness\n",
      "of Data” published in 2009.10 It should be noted, however, that small- and medium-\n",
      "sized datasets are still very common, and it is not always easy or cheap to get extra\n",
      "training data, so don’t abandon algorithms just yet. 8 For example, knowing whether to write “to,” “two,” or “too” depending on the context. 9 Figure reproduced with permission from Banko and Brill (2001), “Learning Curves for Confusion Set Disam‐\n",
      "biguation.”\n",
      "10 “The Unreasonable Effectiveness of Data,” Peter Norvig et al. (2009). Main Challenges of Machine Learning | 23\n",
      "Nonrepresentative Training Data\n",
      "In order to generalize well, it is crucial that your training data be representative of the\n",
      "new cases you want to generalize to. This is true whether you use instance-based\n",
      "learning or model-based learning. For example, the set of countries we used earlier for training the linear model was not\n",
      "perfectly representative; a few countries were missing. Figure 1-21 shows what the\n",
      "data looks like when you add the missing countries. Figure 1-21.\n",
      "\n",
      "Q: ,,,. 10 “The Unreasonable Effectiveness of Data”\n",
      "A: 9\n",
      "\n",
      "Q: ,,,,, “to,” “two,” or “too\n",
      "A: 8\n",
      "\n",
      "Q: 8 Whether to write “to,” “two,” or “too” depending on the\n",
      "A: context\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 59:\n",
      "A more representative training sample\n",
      "If you train a linear model on this data, you get the solid line, while the old model is\n",
      "represented by the dotted line. As you can see, not only does adding a few missing\n",
      "countries significantly alter the model, but it makes it clear that such a simple linear\n",
      "model is probably never going to work well. It seems that very rich countries are not\n",
      "happier than moderately rich countries (in fact they seem unhappier), and conversely\n",
      "some poor countries seem happier than many rich countries. By using a nonrepresentative training set, we trained a model that is unlikely to make\n",
      "accurate predictions, especially for very poor and very rich countries. It is crucial to use a training set that is representative of the cases you want to general‐\n",
      "ize to. This is often harder than it sounds: if the sample is too small, you will have\n",
      "sampling noise (i.e., nonrepresentative data as a result of chance), but even very large\n",
      "samples can be nonrepresentative if the sampling method is flawed. This is called\n",
      "sampling bias.\n",
      "\n",
      "Q: a model that is unlikely to make accurate predictions, especially for very poor and very rich countries\n",
      "A: nonrepresentative training set\n",
      "\n",
      "Q: Uns a question: A more representative training sample If you train a linear model on this\n",
      "A: the solid line\n",
      "\n",
      "Q: a simple linear model is probably never going to work well. It seems that very rich countries are\n",
      "A: not\n",
      "happier than moderately rich countries\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 60:\n",
      "A Famous Example of Sampling Bias\n",
      "Perhaps the most famous example of sampling bias happened during the US presi‐\n",
      "dential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\n",
      "conducted a very large poll, sending mail to about 10 million people. It got 2.4 million\n",
      "answers, and predicted with high confidence that Landon would get 57% of the votes. 24 | Chapter 1: The Machine Learning Landscape\n",
      "Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest’s\n",
      "sampling method:\n",
      "• First, to obtain the addresses to send the polls to, the Literary Digest used tele‐\n",
      "phone directories, lists of magazine subscribers, club membership lists, and the\n",
      "like. All of these lists tend to favor wealthier people, who are more likely to vote\n",
      "Republican (hence Landon). • Second, less than 25% of the people who received the poll answered. Again, this\n",
      "introduces a sampling bias, by ruling out people who don’t care much about poli‐\n",
      "tics, people who don’t like the Literary Digest, and other key groups. This is a spe‐\n",
      "cial type of sampling bias called nonresponse bias.\n",
      "\n",
      "Q: poli tics, people who don’t like the Literary Digest, and other\n",
      "A: nonresponse bias\n",
      "\n",
      "Q: Questions: A Famous Example of Sampling Bias Perhaps the most famous example of sampling bias\n",
      "A: \n",
      "dential election in 1936\n",
      "\n",
      "Q: : It got 2,4 million answers, and predicted with high confidence that Landon would get 5\n",
      "A: nonresponse bias\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 61:\n",
      "Here is another example: say you want to build a system to recognize funk music vid‐\n",
      "eos. One way to build your training set is to search “funk music” on YouTube and use\n",
      "the resulting videos. But this assumes that YouTube’s search engine returns a set of\n",
      "videos that are representative of all the funk music videos on YouTube. In reality, the\n",
      "search results are likely to be biased toward popular artists (and if you live in Brazil\n",
      "you will get a lot of “funk carioca” videos, which sound nothing like James Brown). On the other hand, how else can you get a large training set? Poor-Quality Data\n",
      "Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\n",
      "quality measurements), it will make it harder for the system to detect the underlying\n",
      "patterns, so your system is less likely to perform well. It is often well worth the effort\n",
      "to spend time cleaning up your training data. The truth is, most data scientists spend\n",
      "a significant part of their time doing just that.\n",
      "\n",
      "Q: vid eos. You want to build a system to recognize funk\n",
      "A: ‐\n",
      "\n",
      "Q: funk music vid eos.\n",
      "A: ‐\n",
      "\n",
      "Q: videos on YouTube. This assumes that YouTube’s search engine returns a set of videos\n",
      "A: \n",
      "videos that are representative of all the funk music videos on YouTube\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 62:\n",
      "For example:\n",
      "• If some instances are clearly outliers, it may help to simply discard them or try to\n",
      "fix the errors manually. • If some instances are missing a few features (e.g., 5% of your customers did not\n",
      "specify their age), you must decide whether you want to ignore this attribute alto‐\n",
      "gether, ignore these instances, fill in the missing values (e.g., with the median\n",
      "age), or train one model with the feature and one model without it, and so on. Irrelevant Features\n",
      "As the saying goes: garbage in, garbage out. Your system will only be capable of learn‐\n",
      "ing if the training data contains enough relevant features and not too many irrelevant\n",
      "ones. A critical part of the success of a Machine Learning project is coming up with a\n",
      "good set of features to train on. This process, called feature engineering, involves:\n",
      "Main Challenges of Machine Learning | 25\n",
      "• Feature selection: selecting the most useful features to train on among existing\n",
      "features. • Feature extraction: combining existing features to produce a more useful one (as\n",
      "we saw earlier, dimensionality reduction algorithms can help). • Creating new features by gathering new data.\n",
      "\n",
      "Q: the problem: • If some instances are clearly outliers, it may help to simply discard\n",
      "A: \n",
      "fix the errors manually\n",
      "\n",
      "Q: If some instances are clearly outliers, it may help to simply discard them or try to\n",
      "A: \n",
      "fix the errors manually\n",
      "\n",
      "Q: If some instances are missing some features (e.g., 5% of your customers did\n",
      "A: \n",
      "specify their age\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 63:\n",
      "Now that we have looked at many examples of bad data, let’s look at a couple of exam‐\n",
      "ples of bad algorithms. Overfitting the Training Data\n",
      "Say you are visiting a foreign country and the taxi driver rips you off. You might be\n",
      "tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\n",
      "something that we humans do all too often, and unfortunately machines can fall into\n",
      "the same trap if we are not careful. In Machine Learning this is called overfitting: it\n",
      "means that the model performs well on the training data, but it does not generalize\n",
      "well. Figure 1-22 shows an example of a high-degree polynomial life satisfaction model\n",
      "that strongly overfits the training data. Even though it performs much better on the\n",
      "training data than the simple linear model, would you really trust its predictions? Figure 1-22. Overfitting the training data\n",
      "Complex models such as deep neural networks can detect subtle patterns in the data,\n",
      "but if the training set is noisy, or if it is too small (which introduces sampling noise),\n",
      "then the model is likely to detect patterns in the noise itself.\n",
      "\n",
      "Q: algorithms that overfit the training data. that overfits the training data. of\n",
      "A: polynomial life satisfaction model\n",
      "\n",
      "Q: Exam ples of bad algorithms Overfitting the Training Data Say you are visiting a\n",
      "A: foreign country\n",
      "\n",
      "Q: a taxi driver rips you off.\n",
      "A: Say you are visiting a foreign country\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 64:\n",
      "Obviously these patterns\n",
      "will not generalize to new instances. For example, say you feed your life satisfaction\n",
      "model many more attributes, including uninformative ones such as the country’s\n",
      "name. In that case, a complex model may detect patterns like the fact that all coun‐\n",
      "tries in the training data with a w in their name have a life satisfaction greater than 7:\n",
      "New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\n",
      "26 | Chapter 1: The Machine Learning Landscape\n",
      "are you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\n",
      "this pattern occurred in the training data by pure chance, but the model has no way\n",
      "to tell whether a pattern is real or simply the result of noise in the data. Overfitting happens when the model is too complex relative to the\n",
      "amount and noisiness of the training data.\n",
      "\n",
      "Q: patterns will not generalize to new instances. For example, say you feed your life satisfaction model\n",
      "A: patterns\n",
      "will not generalize to new instances\n",
      "\n",
      "Q: questions: These patterns will not generalize to new instances., say you feed your model\n",
      "A: Obviously these patterns\n",
      "will not generalize to new instances\n",
      "\n",
      "Q: your model questions: Do you feed your model many more attributes, including uninformative\n",
      "A: say you feed your life satisfaction\n",
      "model many more attributes\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 65:\n",
      "The possible solutions\n",
      "are:\n",
      "• To simplify the model by selecting one with fewer parameters\n",
      "(e.g., a linear model rather than a high-degree polynomial\n",
      "model), by reducing the number of attributes in the training\n",
      "data or by constraining the model\n",
      "• To gather more training data\n",
      "• To reduce the noise in the training data (e.g., fix data errors\n",
      "and remove outliers)\n",
      "Constraining a model to make it simpler and reduce the risk of overfitting is called\n",
      "regularization. For example, the linear model we defined earlier has two parameters,\n",
      "θ and θ . This gives the learning algorithm two degrees of freedom to adapt the model\n",
      "0 1\n",
      "to the training data: it can tweak both the height (θ ) and the slope (θ ) of the line. If\n",
      "0 1\n",
      "we forced θ = 0, the algorithm would have only one degree of freedom and would\n",
      "1\n",
      "have a much harder time fitting the data properly: all it could do is move the line up\n",
      "or down to get as close as possible to the training instances, so it would end up\n",
      "around the mean. A very simple model indeed!\n",
      "\n",
      "Q: to simplify the model by selecting one with fewer parameters (e.g., a\n",
      "A: linear model\n",
      "\n",
      "Q: To simplify the model by selecting one with fewer parameters (e.g., a\n",
      "A: linear model\n",
      "\n",
      "Q: and  This gives the learning algorithm two degrees of freedom to adapt the model 0\n",
      "A: \n",
      "θ and θ\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 66:\n",
      "If we allow the algorithm to modify θ\n",
      "1\n",
      "but we force it to keep it small, then the learning algorithm will effectively have some‐\n",
      "where in between one and two degrees of freedom. It will produce a simpler model\n",
      "than with two degrees of freedom, but more complex than with just one. You want to\n",
      "find the right balance between fitting the data perfectly and keeping the model simple\n",
      "enough to ensure that it will generalize well. Figure 1-23 shows three models: the dotted line represents the original model that\n",
      "was trained with a few countries missing, the dashed line is our second model trained\n",
      "with all countries, and the solid line is a linear model trained with the same data as\n",
      "the first model but with a regularization constraint. You can see that regularization\n",
      "forced the model to have a smaller slope, which fits a bit less the training data that the\n",
      "model was trained on, but actually allows it to generalize better to new examples. Main Challenges of Machine Learning | 27\n",
      "Figure 1-23. Regularization reduces the risk of overfitting\n",
      "The amount of regularization to apply during learning can be controlled by a hyper‐\n",
      "parameter.\n",
      "\n",
      "Q: the model to have a smaller slope, which fits a bit less the training data that\n",
      "A: regularization\n",
      "\n",
      "Q: 1 but we force it to keep it small, then the learning algorithm will effectively have some\n",
      "A: some‐\n",
      "where in between one and two degrees of freedom\n",
      "\n",
      "Q: a simpler model than with two degrees of freedom, but more complex than with just one.\n",
      "A: θ\n",
      "1\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 67:\n",
      "A hyperparameter is a parameter of a learning algorithm (not of the\n",
      "model). As such, it is not affected by the learning algorithm itself; it must be set prior\n",
      "to training and remains constant during training. If you set the regularization hyper‐\n",
      "parameter to a very large value, you will get an almost flat model (a slope close to\n",
      "zero); the learning algorithm will almost certainly not overfit the training data, but it\n",
      "will be less likely to find a good solution. Tuning hyperparameters is an important\n",
      "part of building a Machine Learning system (you will see a detailed example in the\n",
      "next chapter). Underfitting the Training Data\n",
      "As you might guess, underfitting is the opposite of overfitting: it occurs when your\n",
      "model is too simple to learn the underlying structure of the data. For example, a lin‐\n",
      "ear model of life satisfaction is prone to underfit; reality is just more complex than\n",
      "the model, so its predictions are bound to be inaccurate, even on the training exam‐\n",
      "ples.\n",
      "\n",
      "Q: the training data. The learning algorithm will almost certainly not overfit the training data .\n",
      "A: hyperparameter\n",
      "\n",
      "Q: Fragen: A hyperparameter is a parameter of a learning algorithm (not of the model\n",
      "A: \n",
      "model\n",
      "\n",
      "Q: the learning algorithm itself; it must be set prior to training and remains constant during training.\n",
      "A: hyperparameter\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 68:\n",
      "The main options to fix this problem are:\n",
      "• Selecting a more powerful model, with more parameters\n",
      "• Feeding better features to the learning algorithm (feature engineering)\n",
      "• Reducing the constraints on the model (e.g., reducing the regularization hyper‐\n",
      "parameter)\n",
      "Stepping Back\n",
      "By now you already know a lot about Machine Learning. However, we went through\n",
      "so many concepts that you may be feeling a little lost, so let’s step back and look at the\n",
      "big picture:\n",
      "28 | Chapter 1: The Machine Learning Landscape\n",
      "• Machine Learning is about making machines get better at some task by learning\n",
      "from data, instead of having to explicitly code rules. • There are many different types of ML systems: supervised or not, batch or online,\n",
      "instance-based or model-based, and so on. • In a ML project you gather data in a training set, and you feed the training set to\n",
      "a learning algorithm. If the algorithm is model-based it tunes some parameters to\n",
      "fit the model to the training set (i.e., to make good predictions on the training set\n",
      "itself), and then hopefully it will be able to make good predictions on new cases\n",
      "as well.\n",
      "\n",
      "Q: the problem: • Selecting a more powerful model, with more parameters • Feeding\n",
      "A: better features to the learning algorithm\n",
      "\n",
      "Q: the regularization hyper parameter) Stepping Back By now you already know a lot about\n",
      "A: Machine Learning\n",
      "\n",
      "Q: questions:, Machine Learning is about making machines get better at some task by learning from\n",
      "A: data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 69:\n",
      "If the algorithm is instance-based, it just learns the examples by heart and\n",
      "uses a similarity measure to generalize to new instances. • The system will not perform well if your training set is too small, or if the data is\n",
      "not representative, noisy, or polluted with irrelevant features (garbage in, garbage\n",
      "out). Lastly, your model needs to be neither too simple (in which case it will\n",
      "underfit) nor too complex (in which case it will overfit). There’s just one last important topic to cover: once you have trained a model, you\n",
      "don’t want to just “hope” it generalizes to new cases. You want to evaluate it, and fine-\n",
      "tune it if necessary. Let’s see how. Testing and Validating\n",
      "The only way to know how well a model will generalize to new cases is to actually try\n",
      "it out on new cases. One way to do that is to put your model in production and moni‐\n",
      "tor how well it performs. This works well, but if your model is horribly bad, your\n",
      "users will complain—not the best idea. A better option is to split your data into two sets: the training set and the test set.\n",
      "\n",
      "Q: new cases. You want to evaluate it, and fine- tune it if necessary. Let\n",
      "A: once you have trained a model\n",
      "\n",
      "Q: The system will not perform well if your training set is too small, or if the\n",
      "A: data is\n",
      "not representative, noisy, or polluted with irrelevant features\n",
      "\n",
      "Q: • The system will not perform well if your training set is too small, or if the\n",
      "A: data is\n",
      "not representative, noisy, or polluted with irrelevant features\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 70:\n",
      "As\n",
      "these names imply, you train your model using the training set, and you test it using\n",
      "the test set. The error rate on new cases is called the generalization error (or out-of-\n",
      "sample error), and by evaluating your model on the test set, you get an estimation of\n",
      "this error. This value tells you how well your model will perform on instances it has\n",
      "never seen before. If the training error is low (i.e., your model makes few mistakes on the training set)\n",
      "but the generalization error is high, it means that your model is overfitting the train‐\n",
      "ing data. It is common to use 80% of the data for training and hold out 20%\n",
      "for testing. Testing and Validating | 29\n",
      "So evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\n",
      "tating between two models (say a linear model and a polynomial model): how can\n",
      "you decide? One option is to train both and compare how well they generalize using\n",
      "the test set. Now suppose that the linear model generalizes better, but you want to apply some\n",
      "regularization to avoid overfitting.\n",
      "\n",
      "Q: the generalization error (or out-of- sample error) and you test it using the\n",
      "A: The error rate on new cases\n",
      "\n",
      "Q: Frequently asked questions: As these names imply, you train your model using the training set,\n",
      "A: to train both and compare how well they generalize using\n",
      "the test set\n",
      "\n",
      "Q: is called the generalization error (or out-of- sample error). This error is\n",
      "A: The error rate on new cases\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 71:\n",
      "The question is: how do you choose the value of\n",
      "the regularization hyperparameter? One option is to train 100 different models using\n",
      "100 different values for this hyperparameter. Suppose you find the best hyperparame‐\n",
      "ter value that produces a model with the lowest generalization error, say just 5% error. So you launch this model into production, but unfortunately it does not perform as\n",
      "well as expected and produces 15% errors. What just happened? The problem is that you measured the generalization error multiple times on the test\n",
      "set, and you adapted the model and hyperparameters to produce the best model for\n",
      "that set. This means that the model is unlikely to perform as well on new data. A common solution to this problem is to have a second holdout set called the valida‐\n",
      "tion set. You train multiple models with various hyperparameters using the training\n",
      "set, you select the model and hyperparameters that perform best on the validation set,\n",
      "and when you’re happy with your model you run a single final test against the test set\n",
      "to get an estimate of the generalization error.\n",
      "\n",
      "Q: you train 100 different models using 100 different values for this hyperparameter using 100 different values for this\n",
      "A: the regularization hyperparameter\n",
      "\n",
      "Q: question: How do you choose the value of the regularization hyperparameter?\n",
      "A: to train 100 different models using\n",
      "100 different values\n",
      "\n",
      "Q: questions: Suppose you find the best hyperparame ter value that produces a\n",
      "A: model with the lowest generalization error\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 72:\n",
      "To avoid “wasting” too much training data in validation sets, a common technique is\n",
      "to use cross-validation: the training set is split into complementary subsets, and each\n",
      "model is trained against a different combination of these subsets and validated\n",
      "against the remaining parts. Once the model type and hyperparameters have been\n",
      "selected, a final model is trained using these hyperparameters on the full training set,\n",
      "and the generalized error is measured on the test set. No Free Lunch Theorem\n",
      "A model is a simplified version of the observations. The simplifications are meant to\n",
      "discard the superfluous details that are unlikely to generalize to new instances. How‐\n",
      "ever, to decide what data to discard and what data to keep, you must make assump‐\n",
      "tions. For example, a linear model makes the assumption that the data is\n",
      "fundamentally linear and that the distance between the instances and the straight line\n",
      "is just noise, which can safely be ignored. In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely\n",
      "no assumption about the data, then there is no reason to prefer one model over any\n",
      "other. This is called the No Free Lunch (NFL) theorem.\n",
      "\n",
      "Q: the data, and the generalized error is measured on the test set. No Free Lunch The\n",
      "A: Once the model type and hyperparameters have been\n",
      "selected\n",
      "\n",
      "Q: validation sets, and each model is trained against a different combination of these subsets and\n",
      "A: cross-validation: the training set is split into complementary subsets\n",
      "\n",
      "Q: theorem A model is a simplified version of the observations.\n",
      "A: No Free Lunch\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 73:\n",
      "For some datasets the best\n",
      "11 “The Lack of A Priori Distinctions Between Learning Algorithms,” D. Wolperts (1996). 30 | Chapter 1: The Machine Learning Landscape\n",
      "model is a linear model, while for other datasets it is a neural network. There is no\n",
      "model that is a priori guaranteed to work better (hence the name of the theorem). The\n",
      "only way to know for sure which model is best is to evaluate them all. Since this is not\n",
      "possible, in practice you make some reasonable assumptions about the data and you\n",
      "evaluate only a few reasonable models. For example, for simple tasks you may evalu‐\n",
      "ate linear models with various levels of regularization, and for a complex problem you\n",
      "may evaluate various neural networks. Exercises\n",
      "In this chapter we have covered some of the most important concepts in Machine\n",
      "Learning. In the next chapters we will dive deeper and write more code, but before we\n",
      "do, make sure you know how to answer the following questions:\n",
      "1. How would you define Machine Learning? 2. Can you name four types of problems where it shines? 3. What is a labeled training set? 4.\n",
      "\n",
      "Q: theorem. The only way to know which model is best is to evaluate them all\n",
      "A: There is no\n",
      "model that is a priori guaranteed to work better\n",
      "\n",
      "Q: s,s, and other questions: For some datasets the best 11 “The Lack\n",
      "A: evaluate them all\n",
      "\n",
      "Q: : Wolperts (1996) 30 | Chapter 1: The Machine Learning Landscape model is\n",
      "A: a linear model\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 74:\n",
      "What are the two most common supervised tasks? 5. Can you name four common unsupervised tasks? 6. What type of Machine Learning algorithm would you use to allow a robot to\n",
      "walk in various unknown terrains? 7. What type of algorithm would you use to segment your customers into multiple\n",
      "groups? 8. Would you frame the problem of spam detection as a supervised learning prob‐\n",
      "lem or an unsupervised learning problem? 9. What is an online learning system? 10. What is out-of-core learning? 11. What type of learning algorithm relies on a similarity measure to make predic‐\n",
      "tions? 12. What is the difference between a model parameter and a learning algorithm’s\n",
      "hyperparameter? 13. What do model-based learning algorithms search for? What is the most common\n",
      "strategy they use to succeed? How do they make predictions? 14. Can you name four of the main challenges in Machine Learning? 15. If your model performs great on the training data but generalizes poorly to new\n",
      "instances, what is happening? Can you name three possible solutions? 16. What is a test set and why would you want to use it? Exercises | 31\n",
      "17. What is the purpose of a validation set? 18.\n",
      "\n",
      "Q: ? What is the difference between a model parameter and a learning algorithm’s hyperpara\n",
      "A: 12\n",
      "\n",
      "Q: four common unsupervised tasks? 6\n",
      "A: 5\n",
      "\n",
      "Q: What type of algorithm would you use to allow a robot to walk in various unknown terrains\n",
      "A: Machine Learning\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 75:\n",
      "What can go wrong if you tune hyperparameters using the test set? 19. What is cross-validation and why would you prefer it to a validation set? Solutions to these exercises are available in Appendix A. 32 | Chapter 1: The Machine Learning Landscape\n",
      "CHAPTER 2\n",
      "End-to-End Machine Learning Project\n",
      "In this chapter, you will go through an example project end to end, pretending to be a\n",
      "recently hired data scientist in a real estate company.1 Here are the main steps you will\n",
      "go through:\n",
      "1. Look at the big picture. 2. Get the data. 3. Discover and visualize the data to gain insights. 4. Prepare the data for Machine Learning algorithms. 5. Select a model and train it. 6. Fine-tune your model. 7. Present your solution. 8. Launch, monitor, and maintain your system. Working with Real Data\n",
      "When you are learning about Machine Learning it is best to actually experiment with\n",
      "real-world data, not just artificial datasets. Fortunately, there are thousands of open\n",
      "datasets to choose from, ranging across all sorts of domains.\n",
      "\n",
      "Q: ? 19. What is cross-validation and why would you prefer it to a validation set\n",
      "A: What can go wrong if you tune hyperparameters using the test set\n",
      "\n",
      "Q: ? 19 What is cross-validation and why would you prefer it to a validation set\n",
      "A: What can go wrong if you tune hyperparameters using the test set\n",
      "\n",
      "Q: : What is cross-validation and why would you prefer it to a validation set?\n",
      "A: 19\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 76:\n",
      "Here are a few places\n",
      "you can look to get data:\n",
      "• Popular open data repositories:\n",
      "1 The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\n",
      "project, not to learn anything about the real estate business. 33\n",
      "—UC Irvine Machine Learning Repository\n",
      "—Kaggle datasets\n",
      "—Amazon’s AWS datasets\n",
      "• Meta portals (they list open data repositories):\n",
      "—http://dataportals.org/\n",
      "—http://opendatamonitor.eu/\n",
      "—http://quandl.com/\n",
      "• Other pages listing many popular open data repositories:\n",
      "—Wikipedia’s list of Machine Learning datasets\n",
      "—Quora.com question\n",
      "—Datasets subreddit\n",
      "In this chapter we chose the California Housing Prices dataset from the StatLib repos‐\n",
      "itory2 (see Figure 2-1). This dataset was based on data from the 1990 California cen‐\n",
      "sus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\n",
      "time), but it has many qualities for learning, so we will pretend it is recent data. We\n",
      "also added a categorical attribute and removed a few features for teaching purposes. Figure 2-1. California housing prices\n",
      "2 The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions,” Statistics\n",
      "& Probability Letters 33, no. 3 (1997): 291–297.\n",
      "\n",
      "Q: questions: Here are some places you can look to get data: 1 The example project is completely\n",
      "A: fictitious\n",
      "\n",
      "Q: questions: Here are some places you can look to get data: • Popular open data repositori\n",
      "A: \n",
      "1\n",
      "\n",
      "Q: —Wikipedia’s list of Open Data repositories —Quor\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 77:\n",
      "34 | Chapter 2: End-to-End Machine Learning Project\n",
      "Look at the Big Picture\n",
      "Welcome to Machine Learning Housing Corporation! The first task you are asked to\n",
      "perform is to build a model of housing prices in California using the California cen‐\n",
      "sus data. This data has metrics such as the population, median income, median hous‐\n",
      "ing price, and so on for each block group in California. Block groups are the smallest\n",
      "geographical unit for which the US Census Bureau publishes sample data (a block\n",
      "group typically has a population of 600 to 3,000 people). We will just call them “dis‐\n",
      "tricts” for short. Your model should learn from this data and be able to predict the median housing\n",
      "price in any district, given all the other metrics. Since you are a well-organized data scientist, the first thing you do\n",
      "is to pull out your Machine Learning project checklist. You can\n",
      "start with the one in Appendix B; it should work reasonably well\n",
      "for most Machine Learning projects but make sure to adapt it to\n",
      "your needs.\n",
      "\n",
      "Q: questions: 34 | Chapter 2: End-to-End Machine Learning Project Look at the\n",
      "A: Big Picture\n",
      "\n",
      "Q: Fragen: 34 | Chapter 2: End-to-End Machine Learning Project Look at the Big\n",
      "A: Machine Learning project checklist\n",
      "\n",
      "Q: , median income, median hous ing price, and so on. This data\n",
      "A: California cen‐\n",
      "sus data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 78:\n",
      "In this chapter we will go through many checklist\n",
      "items, but we will also skip a few, either because they are self-\n",
      "explanatory or because they will be discussed in later chapters. Frame the Problem\n",
      "The first question to ask your boss is what exactly is the business objective; building a\n",
      "model is probably not the end goal. How does the company expect to use and benefit\n",
      "from this model? This is important because it will determine how you frame the\n",
      "problem, what algorithms you will select, what performance measure you will use to\n",
      "evaluate your model, and how much effort you should spend tweaking it. Your boss answers that your model’s output (a prediction of a district’s median hous‐\n",
      "ing price) will be fed to another Machine Learning system (see Figure 2-2), along\n",
      "with many other signals.3 This downstream system will determine whether it is worth\n",
      "investing in a given area or not. Getting this right is critical, as it directly affects reve‐\n",
      "nue. 3 A piece of information fed to a Machine Learning system is often called a signal in reference to Shannon’s\n",
      "information theory: you want a high signal/noise ratio.\n",
      "\n",
      "Q: questions: that your model’s output will be fed to another Machine Learning system (see\n",
      "A: Your boss\n",
      "\n",
      "Q: questions: In this chapter we will go through many checklist items, but also skip a few,\n",
      "A: self-\n",
      "explanatory\n",
      "\n",
      "Q: Frame the Problem - What is the business objective?\n",
      "A: building a\n",
      "model is probably not the end goal\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 79:\n",
      "Look at the Big Picture | 35\n",
      "Figure 2-2. A Machine Learning pipeline for real estate investments\n",
      "Pipelines\n",
      "A sequence of data processing components is called a data pipeline. Pipelines are very\n",
      "common in Machine Learning systems, since there is a lot of data to manipulate and\n",
      "many data transformations to apply. Components typically run asynchronously. Each component pulls in a large amount\n",
      "of data, processes it, and spits out the result in another data store, and then some time\n",
      "later the next component in the pipeline pulls this data and spits out its own output,\n",
      "and so on. Each component is fairly self-contained: the interface between components\n",
      "is simply the data store. This makes the system quite simple to grasp (with the help of\n",
      "a data flow graph), and different teams can focus on different components. Moreover,\n",
      "if a component breaks down, the downstream components can often continue to run\n",
      "normally (at least for a while) by just using the last output from the broken compo‐\n",
      "nent. This makes the architecture quite robust. On the other hand, a broken component can go unnoticed for some time if proper\n",
      "monitoring is not implemented.\n",
      "\n",
      "Q: | 35 Figure 2-2. A Machine Learning pipeline for real estate investments Pipelines Questions: Look\n",
      "A: Look at the Big Picture\n",
      "\n",
      "Q: Questions: Look at the Big Picture | 35 Figure 2-2 A machine learning pipeline for real estate investments\n",
      "A: \n",
      "\n",
      "\n",
      "Q: Pipelines Pipelines Pipelines Pipelines Pipelines Pipelines Pipelines Pipelines Pipelines Pipe\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 80:\n",
      "The data gets stale and the overall system’s perfor‐\n",
      "mance drops. The next question to ask is what the current solution looks like (if any). It will often\n",
      "give you a reference performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually\n",
      "by experts: a team gathers up-to-date information about a district (excluding median\n",
      "housing prices), and they use complex rules to come up with an estimate. This is\n",
      "costly and time-consuming, and their estimates are not great; their typical error rate\n",
      "is about 15%. Okay, with all this information you are now ready to start designing your system. First, you need to frame the problem: is it supervised, unsupervised, or Reinforce‐\n",
      "ment Learning? Is it a classification task, a regression task, or something else? Should\n",
      "36 | Chapter 2: End-to-End Machine Learning Project\n",
      "you use batch learning or online learning techniques? Before you read on, pause and\n",
      "try to answer these questions for yourself. Have you found the answers?\n",
      "\n",
      "Q: the problem: the data gets stale and the overall system’s perfor\n",
      "A: \n",
      "mance drops\n",
      "\n",
      "Q: Fragen: Die Daten schwinden und die Perfor mance der Gesamtsysteme\n",
      "A: \n",
      "\n",
      "\n",
      "Q: questions: What the current solution looks like (if any)\n",
      "A: It will often\n",
      "give you a reference performance\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 81:\n",
      "Let’s see: it is clearly a typical supervised learning task\n",
      "since you are given labeled training examples (each instance comes with the expected\n",
      "output, i.e., the district’s median housing price). Moreover, it is also a typical regres‐\n",
      "sion task, since you are asked to predict a value. More specifically, this is a multivari‐\n",
      "ate regression problem since the system will use multiple features to make a prediction\n",
      "(it will use the district’s population, the median income, etc.). In the first chapter, you\n",
      "predicted life satisfaction based on just one feature, the GDP per capita, so it was a\n",
      "univariate regression problem. Finally, there is no continuous flow of data coming in\n",
      "the system, there is no particular need to adjust to changing data rapidly, and the data\n",
      "is small enough to fit in memory, so plain batch learning should do just fine. If the data was huge, you could either split your batch learning\n",
      "work across multiple servers (using the MapReduce technique, as\n",
      "we will see later), or you could use an online learning technique\n",
      "instead. Select a Performance Measure\n",
      "Your next step is to select a performance measure.\n",
      "\n",
      "Q: a typical supervised learning task, since you are given labeled training examples (e\n",
      "A: the district’s median housing price\n",
      "\n",
      "Q: : Let’s see: it is clearly a typical supervised learning task, since you\n",
      "A: you are given labeled training examples\n",
      "\n",
      "Q: questions: Moreover, it is also a typical regres sion task,\n",
      "A: predict a value\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 82:\n",
      "A typical performance measure for\n",
      "regression problems is the Root Mean Square Error (RMSE). It measures the standard\n",
      "deviation4 of the errors the system makes in its predictions. For example, an RMSE\n",
      "equal to 50,000 means that about 68% of the system’s predictions fall within $50,000\n",
      "of the actual value, and about 95% of the predictions fall within $100,000 of the actual\n",
      "value.5 Equation 2-1 shows the mathematical formula to compute the RMSE. Equation 2-1. Root Mean Square Error (RMSE)\n",
      "m\n",
      "RMSE  ,h = 1 ∑ h  i −yi 2\n",
      "m\n",
      "i=1\n",
      "4 The standard deviation, generally denoted σ (the Greek letter sigma), is the square root of the variance, which\n",
      "is the average of the squared deviation from the mean. 5 When a feature has a bell-shaped normal distribution (also called a Gaussian distribution), which is very com‐\n",
      "mon, the “68-95-99.7” rule applies: about 68% of the values fall within 1σ of the mean, 95% within 2σ, and\n",
      "99.7% within 3σ.\n",
      "\n",
      "Q: ,  (the Greek letter sigma) is the square root of the variance\n",
      "A: σ\n",
      "\n",
      "Q: Equation 2-1 shows the mathematical formula to compute the RMSE4 of the errors the system\n",
      "A: \n",
      "m\n",
      "i=1\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 83:\n",
      "Look at the Big Picture | 37\n",
      "Notations\n",
      "This equation introduces several very common Machine Learning notations that we\n",
      "will use throughout this book:\n",
      "• m is the number of instances in the dataset you are measuring the RMSE on. —For example, if you are evaluating the RMSE on a validation set of 2,000 dis‐\n",
      "tricts, then m = 2,000. • x(i) is a vector of all the feature values (excluding the label) of the ith instance in\n",
      "the dataset, and y(i) is its label (the desired output value for that instance). —For example, if the first district in the dataset is located at longitude –118.29°,\n",
      "latitude 33.91°, and it has 1,416 inhabitants with a median income of $38,372,\n",
      "and the median house value is $156,400 (ignoring the other features for now),\n",
      "then:\n",
      "−118.29\n",
      "33.91\n",
      "1 =\n",
      "1,416\n",
      "38,372\n",
      "and:\n",
      "1\n",
      "y =156,400\n",
      "• X is a matrix containing all the feature values (excluding labels) of all instances in\n",
      "the dataset.\n",
      "\n",
      "Q: the RMSE on a validation set of 2,000 dis tricts,\n",
      "A: m = 2,000\n",
      "\n",
      "Q: questions: Look at the Big Picture | 37 Notations This equation introduces several very common Machine\n",
      "A: Machine Learning\n",
      "\n",
      "Q: —For example, if you are evaluating the RMSE on a validation\n",
      "A: m = 2,000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 84:\n",
      "There is one row per instance and the ith row is equal to the transpose\n",
      "of x(i), noted (x(i))T.6\n",
      "—For example, if the first district is as just described, then the matrix X looks\n",
      "like this:\n",
      "1 T\n",
      "2 T\n",
      "−118.29 33.91 1,416 38,372\n",
      "= ⋮ =\n",
      "⋮ ⋮ ⋮ ⋮\n",
      "1999 T\n",
      "2000 T\n",
      "6 Recall that the transpose operator flips a column vector into a row vector (and vice versa). 38 | Chapter 2: End-to-End Machine Learning Project\n",
      "• h is your system’s prediction function, also called a hypothesis. When your system\n",
      "is given an instance’s feature vector x(i), it outputs a predicted value ŷ(i) = h(x(i))\n",
      "for that instance (ŷ is pronounced “y-hat”). —For example, if your system predicts that the median housing price in the first\n",
      "district is $158,400, then ŷ(1) = h(x(1)) = 158,400. The prediction error for this\n",
      "district is ŷ(1) – y(1) = 2,000.\n",
      "\n",
      "Q: h is your system’s prediction function, also called a hypothesis. —For example\n",
      "A: if your system predicts that the median housing price in the first\n",
      "district\n",
      "\n",
      "Q: h is your system’s prediction function, also called a hypothesis.\n",
      "A: first\n",
      "district\n",
      "\n",
      "Q: questions:\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 85:\n",
      "• RMSE(X,h) is the cost function measured on the set of examples using your\n",
      "hypothesis h.\n",
      "We use lowercase italic font for scalar values (such as m or y(i)) and function names\n",
      "(such as h), lowercase bold font for vectors (such as x(i)), and uppercase bold font for\n",
      "matrices (such as X). Even though the RMSE is generally the preferred performance measure for regression\n",
      "tasks, in some contexts you may prefer to use another function. For example, suppose\n",
      "that there are many outlier districts. In that case, you may consider using the Mean\n",
      "Absolute Error (also called the Average Absolute Deviation; see Equation 2-2):\n",
      "Equation 2-2. Mean Absolute Error\n",
      "m\n",
      "MAE  ,h = 1 ∑ h  i −yi\n",
      "m\n",
      "i=1\n",
      "Both the RMSE and the MAE are ways to measure the distance between two vectors:\n",
      "the vector of predictions and the vector of target values. Various distance measures,\n",
      "or norms, are possible:\n",
      "• Computing the root of a sum of squares (RMSE) corresponds to the Euclidian\n",
      "norm: it is the notion of distance you are familiar with. It is also called the ℓ\n",
      "2\n",
      "norm, noted ∥ · ∥ (or just ∥ · ∥).\n",
      "\n",
      "Q: RMSE(X,h) is the cost function measured on the set of examples using your\n",
      "A: \n",
      "hypothesis h\n",
      "\n",
      "Q: questions: • RMSE(X,h) is the cost function measured on the set of\n",
      "A: \n",
      "hypothesis h\n",
      "\n",
      "Q: the preferred performance measure for regression tasks, but in some contexts you may prefer to use another\n",
      "A: RMSE\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 86:\n",
      "2\n",
      "• Computing the sum of absolutes (MAE) corresponds to the ℓ norm, noted ∥ · ∥ . 1 1\n",
      "It is sometimes called the Manhattan norm because it measures the distance\n",
      "between two points in a city if you can only travel along orthogonal city blocks. • More generally, the ℓ norm of a vector v containing n elements is defined as\n",
      "k\n",
      "1\n",
      "∥  ∥ = v k + v k+⋯+ v k k. ℓ just gives the cardinality of the vector (i.e.,\n",
      "k 0 1 n 0\n",
      "the number of elements), and ℓ gives the maximum absolute value in the vector. ∞\n",
      "• The higher the norm index, the more it focuses on large values and neglects small\n",
      "ones. This is why the RMSE is more sensitive to outliers than the MAE. But when\n",
      "Look at the Big Picture | 39\n",
      "outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\n",
      "very well and is generally preferred. Check the Assumptions\n",
      "Lastly, it is good practice to list and verify the assumptions that were made so far (by\n",
      "you or others); this can catch serious issues early on.\n",
      "\n",
      "Q: the l norm, noted\n",
      "A: \n",
      "k\n",
      "1\n",
      "\n",
      "\n",
      "Q: the l norm, noted    1 1 It is sometimes called the\n",
      "A: v k + v k+⋯+ v k k\n",
      "\n",
      "Q: v k + v k++ v k k\n",
      "A: \n",
      "k\n",
      "1\n",
      "∥  ∥\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 87:\n",
      "For example, the district prices\n",
      "that your system outputs are going to be fed into a downstream Machine Learning\n",
      "system, and we assume that these prices are going to be used as such. But what if the\n",
      "downstream system actually converts the prices into categories (e.g., “cheap,”\n",
      "“medium,” or “expensive”) and then uses those categories instead of the prices them‐\n",
      "selves? In this case, getting the price perfectly right is not important at all; your sys‐\n",
      "tem just needs to get the category right. If that’s so, then the problem should have\n",
      "been framed as a classification task, not a regression task. You don’t want to find this\n",
      "out after working on a regression system for months. Fortunately, after talking with the team in charge of the downstream system, you are\n",
      "confident that they do indeed need the actual prices, not just categories. Great! You’re\n",
      "all set, the lights are green, and you can start coding now! Get the Data\n",
      "It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\n",
      "through the following code examples in a Jupyter notebook. The full Jupyter note‐\n",
      "book is available at https://github.com/ageron/handson-ml.\n",
      "\n",
      "Q: , and you can start coding now! Get the Data It’s time to get your\n",
      "A: You’re\n",
      "all set, the lights are green\n",
      "\n",
      "Q: ,,,,, that your system outputs are going to be\n",
      "A: the district prices\n",
      "\n",
      "Q: ,,,, a classification task, not a regression task?\n",
      "A: sys‐\n",
      "tem just needs to get the category right\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 88:\n",
      "Create the Workspace\n",
      "First you will need to have Python installed. It is probably already installed on your\n",
      "system. If not, you can get it at https://www.python.org/.7\n",
      "Next you need to create a workspace directory for your Machine Learning code and\n",
      "datasets. Open a terminal and type the following commands (after the $ prompts):\n",
      "$ export ML_PATH=\"$HOME/ml\" # You can change the path if you prefer\n",
      "$ mkdir -p $ML_PATH\n",
      "You will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\n",
      "Scikit-Learn. If you already have Jupyter running with all these modules installed,\n",
      "you can safely skip to “Download the Data” on page 43. If you don’t have them yet,\n",
      "there are many ways to install them (and their dependencies). You can use your sys‐\n",
      "tem’s packaging system (e.g., apt-get on Ubuntu, or MacPorts or HomeBrew on\n",
      "7 The latest version of Python 3 is recommended. Python 2.7+ should work fine too, but it is deprecated.\n",
      "\n",
      "Q: : Create the Workspace First you will need Python installed. It is probably already installed on your\n",
      "A: \n",
      "system\n",
      "\n",
      "Q: questions: Create the Workspace First you will need Python installed It is probably already installed on your\n",
      "A: \n",
      "system\n",
      "\n",
      "Q: questions: It is probably already installed on your system If not, you can get it at https\n",
      "A: www.python.org/.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 89:\n",
      "40 | Chapter 2: End-to-End Machine Learning Project\n",
      "macOS), install a Scientific Python distribution such as Anaconda and use its packag‐\n",
      "ing system, or just use Python’s own packaging system, pip, which is included by\n",
      "default with the Python binary installers (since Python 2.7.9).8 You can check to see if\n",
      "pip is installed by typing the following command:\n",
      "$ pip3 --version\n",
      "pip 9.0.1 from [...]/lib/python3.5/site-packages (python 3.5)\n",
      "You should make sure you have a recent version of pip installed, at the very least >1.4\n",
      "to support binary module installation (a.k.a. wheels). To upgrade the pip module,\n",
      "type:9\n",
      "$ pip3 install --upgrade pip\n",
      "Collecting pip\n",
      "[...]\n",
      "Successfully installed pip-9.0.1\n",
      "Creating an Isolated Environment\n",
      "If you would like to work in an isolated environment (which is strongly recom‐\n",
      "mended so you can work on different projects without having conflicting library ver‐\n",
      "sions), install virtualenv by running the following pip command:\n",
      "$ pip3 install --user --upgrade virtualenv\n",
      "Collecting virtualenv\n",
      "[...]\n",
      "Successfully installed virtualenv\n",
      "Now you can create an isolated Python environment by typing:\n",
      "$ cd $ML_PATH\n",
      "$ virtualenv env\n",
      "Using base prefix '[...]'\n",
      "New python executable in [...]/ml/env/bin/python3.5\n",
      "Also creating executable in [...]/ml/env/bin/python\n",
      "Installing setuptools, pip, wheel...done.\n",
      "\n",
      "Q: pip-9.0.1 Creating an Isolated Environment If you would like to work in\n",
      "A: virtualenv\n",
      "\n",
      "Q: questions: 40 | Chapter 2: End-to-End Machine Learning Project macOS),\n",
      "A: \n",
      "\n",
      "\n",
      "Q: '[...]' New python executable in [...]/ml\n",
      "A: env/bin/python3.5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 90:\n",
      "Now every time you want to activate this environment, just open a terminal and type:\n",
      "$ cd $ML_PATH\n",
      "$ source env/bin/activate\n",
      "While the environment is active, any package you install using pip will be installed in\n",
      "this isolated environment, and Python will only have access to these packages (if you\n",
      "also want access to the system’s site packages, you should create the environment\n",
      "8 We will show the installation steps using pip in a bash shell on a Linux or macOS system. You may need to\n",
      "adapt these commands to your own system. On Windows, we recommend installing Anaconda instead. 9 You may need to have administrator rights to run this command; if so, try prefixing it with sudo. Get the Data | 41\n",
      "using virtualenv’s --system-site-packages option). Check out virtualenv’s docu‐\n",
      "mentation for more information. Now you can install all the required modules and their dependencies using this sim‐\n",
      "ple pip command:\n",
      "$ pip3 install --upgrade jupyter matplotlib numpy pandas scipy scikit-learn\n",
      "Collecting jupyter\n",
      "Downloading jupyter-1.0.0-py2.py3-none-any.whl\n",
      "Collecting matplotlib\n",
      "[...]\n",
      "To check your installation, try to import every module like this:\n",
      "$ python3 -c \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\"\n",
      "There should be no output and no error.\n",
      "\n",
      "Q: jupyter matplotlib numpy pandas scipy sci\n",
      "A: \n",
      "\n",
      "\n",
      "Q: this environment, and you should create the environment 8 We will show the installation steps using pip\n",
      "A: this isolated environment\n",
      "\n",
      "Q: questions: You may need to adapt these commands to your own system On Windows, we recommend installing\n",
      "A: Anaconda\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 91:\n",
      "Now you can fire up Jupyter by typing:\n",
      "$ jupyter notebook\n",
      "[I 15:24 NotebookApp] Serving notebooks from local directory: [...]/ml\n",
      "[I 15:24 NotebookApp] 0 active kernels\n",
      "[I 15:24 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n",
      "[I 15:24 NotebookApp] Use Control-C to stop this server and shut down all\n",
      "kernels (twice to skip confirmation). A Jupyter server is now running in your terminal, listening to port 8888. You can visit\n",
      "this server by opening your web browser to http://localhost:8888/ (this usually hap‐\n",
      "pens automatically when the server starts). You should see your empty workspace\n",
      "directory (containing only the env directory if you followed the preceding virtualenv\n",
      "instructions). Now create a new Python notebook by clicking on the New button and selecting the\n",
      "appropriate Python version10 (see Figure 2-3). This does three things: first, it creates a new notebook file called Untitled.ipynb in\n",
      "your workspace; second, it starts a Jupyter Python kernel to run this notebook; and\n",
      "third, it opens this notebook in a new tab. You should start by renaming this note‐\n",
      "book to “Housing” (this will automatically rename the file to Housing.ipynb) by click‐\n",
      "ing Untitled and typing the new name.\n",
      "\n",
      "Q: a new Python notebook by clicking on the New button and selecting the appropriate Python version10 (\n",
      "A: \n",
      "$ jupyter notebook\n",
      "\n",
      "\n",
      "Q: : jupyter.com: jupyter.com:\n",
      "A: \n",
      "$ jupyter notebook\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 92:\n",
      "10 Note that Jupyter can handle multiple versions of Python, and even many other languages such as R or\n",
      "Octave. 42 | Chapter 2: End-to-End Machine Learning Project\n",
      "Figure 2-3. Your workspace in Jupyter\n",
      "A notebook contains a list of cells. Each cell can contain executable code or formatted\n",
      "text. Right now the notebook contains only one empty code cell, labeled “In [1]:”. Try\n",
      "typing print(\"Hello world!\") in the cell, and click on the play button (see\n",
      "Figure 2-4) or press Shift-Enter. This sends the current cell to this notebook’s Python\n",
      "kernel, which runs it and returns the output. The result is displayed below the cell,\n",
      "and since we reached the end of the notebook, a new cell is automatically created. Go\n",
      "through the User Interface Tour from Jupyter’s Help menu to learn the basics. Figure 2-4. Hello world Python notebook\n",
      "Download the Data\n",
      "In typical environments your data would be available in a relational database (or\n",
      "some other common datastore) and spread across multiple tables/documents/files. To\n",
      "Get the Data | 43\n",
      "access it, you would first need to get your credentials and access authorizations,11 and\n",
      "familiarize yourself with the data schema.\n",
      "\n",
      "Q: questions: 10 Note that Jupyter can handle multiple versions of Python, and even many\n",
      "A: \n",
      "Octave\n",
      "\n",
      "Q: 10 Note that Jupyter can handle multiple versions of Python, and even many other languages such\n",
      "A: R or\n",
      "Octave\n",
      "\n",
      "Q: Vous avez des questions: 42 | Chapter 2: End-to-End Machine Learning\n",
      "A: \n",
      "Get the Data |\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 93:\n",
      "In this project, however, things are much\n",
      "simpler: you will just download a single compressed file, housing.tgz, which contains a\n",
      "comma-separated value (CSV) file called housing.csv with all the data. You could use your web browser to download it, and run tar xzf housing.tgz to\n",
      "decompress the file and extract the CSV file, but it is preferable to create a small func‐\n",
      "tion to do that. It is useful in particular if data changes regularly, as it allows you to\n",
      "write a small script that you can run whenever you need to fetch the latest data (or\n",
      "you can set up a scheduled job to do that automatically at regular intervals). Auto‐\n",
      "mating the process of fetching the data is also useful if you need to install the dataset\n",
      "on multiple machines.\n",
      "\n",
      "Q: questions: In this project, however, things are much simpler: you will just download a\n",
      "A: single compressed file\n",
      "\n",
      "Q: questions: In this project, things are much simpler: you will just download a single compressed\n",
      "A: \n",
      "\n",
      "\n",
      "Q: questions: You could use your web browser to download it, and run tar xz\n",
      "A: \n",
      "decompress the file and extract the CSV file\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 94:\n",
      "Here is the function to fetch the data:12\n",
      "import os\n",
      "import tarfile\n",
      "from six.moves import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
      "HOUSING_PATH = \"datasets/housing\"\n",
      "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "if not os.path.isdir(housing_path):\n",
      "os.makedirs(housing_path)\n",
      "tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "housing_tgz = tarfile.open(tgz_path)\n",
      "housing_tgz.extractall(path=housing_path)\n",
      "housing_tgz.close()\n",
      "Now when you call fetch_housing_data(), it creates a datasets/housing directory in\n",
      "your workspace, downloads the housing.tgz file, and extracts the housing.csv from it in\n",
      "this directory. Now let’s load the data using Pandas. Once again you should write a small function to\n",
      "load the data:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "return pd.read_csv(csv_path)\n",
      "11 You might also need to check legal constraints, such as private fields that should never be copied to unsafe\n",
      "datastores. 12 In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\n",
      "notebook. 44 | Chapter 2: End-to-End Machine Learning Project\n",
      "This function returns a Pandas DataFrame object containing all the data. Take a Quick Look at the Data Structure\n",
      "Let’s take a look at the top five rows using the DataFrame’s head() method (see\n",
      "Figure 2-5). Figure 2-5.\n",
      "\n",
      "Q: pd def load_housing_data(housing_path=\n",
      "A: HOUSING_PATH\n",
      "\n",
      "Q: _data(): if not os.path.isdir(housing\n",
      "A: \n",
      "os.makedirs\n",
      "\n",
      "Q: s.csv) return pd.read_csv(c\n",
      "A: os\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 95:\n",
      "Top five rows in the dataset\n",
      "Each row represents one district. There are 10 attributes (you can see the first 6 in the\n",
      "screenshot): longitude, latitude, housing_median_age, total_rooms, total_bed\n",
      "rooms, population, households, median_income, median_house_value, and\n",
      "ocean_proximity. The info() method is useful to get a quick description of the data, in particular the\n",
      "total number of rows, and each attribute’s type and number of non-null values (see\n",
      "Figure 2-6). Figure 2-6. Housing info\n",
      "Get the Data | 45\n",
      "There are 20,640 instances in the dataset, which means that it is fairly small by\n",
      "Machine Learning standards, but it’s perfect to get started. Notice that the total_bed\n",
      "rooms attribute has only 20,433 non-null values, meaning that 207 districts are miss‐\n",
      "ing this feature. We will need to take care of this later. All attributes are numerical, except the ocean_proximity field. Its type is object, so it\n",
      "could hold any kind of Python object, but since you loaded this data from a CSV file\n",
      "you know that it must be a text attribute. When you looked at the top five rows, you\n",
      "probably noticed that the values in that column were repetitive, which means that it is\n",
      "probably a categorical attribute.\n",
      "\n",
      "Q: the data, and the number of non-null values, is a question:\n",
      "A: each attribute’s type\n",
      "\n",
      "Q: : Top five rows in the dataset Each row represents one district There are 10 attributes (you can\n",
      "A: you can see the first 6 in the\n",
      "screenshot\n",
      "\n",
      "Q: , and ocean_proximity. The info() method is useful to get a quick\n",
      "A: description of the data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 96:\n",
      "You can find out what categories exist and how many\n",
      "districts belong to each category by using the value_counts() method:\n",
      ">>> housing[\"ocean_proximity\"].value_counts()\n",
      "<1H OCEAN 9136\n",
      "INLAND 6551\n",
      "NEAR OCEAN 2658\n",
      "NEAR BAY 2290\n",
      "ISLAND 5\n",
      "Name: ocean_proximity, dtype: int64\n",
      "Let’s look at the other fields. The describe() method shows a summary of the\n",
      "numerical attributes (Figure 2-7). Figure 2-7. Summary of each numerical attribute\n",
      "The count, mean, min, and max rows are self-explanatory. Note that the null values are\n",
      "ignored (so, for example, count of total_bedrooms is 20,433, not 20,640). The std\n",
      "row shows the standard deviation (which measures how dispersed the values are). The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indi‐\n",
      "cates the value below which a given percentage of observations in a group of observa‐\n",
      "tions falls. For example, 25% of the districts have a housing_median_age lower than\n",
      "46 | Chapter 2: End-to-End Machine Learning Project\n",
      "18, while 50% are lower than 29 and 75% are lower than 37. These are often called the\n",
      "25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile).\n",
      "\n",
      "Q: OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_prox\n",
      "A: \n",
      "\n",
      "\n",
      "Q: () method shows a summary of the numerical attributes (Figure 2-7) Figure 2-7\n",
      "A: describe\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 97:\n",
      "Another quick way to get a feel of the type of data you are dealing with is to plot a\n",
      "histogram for each numerical attribute. A histogram shows the number of instances\n",
      "(on the vertical axis) that have a given value range (on the horizontal axis). You can\n",
      "either plot this one attribute at a time, or you can call the hist() method on the\n",
      "whole dataset, and it will plot a histogram for each numerical attribute (see\n",
      "Figure 2-8). For example, you can see that slightly over 800 districts have a\n",
      "median_house_value equal to about $500,000. %matplotlib inline # only in a Jupyter notebook\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=50, figsize=(20,15))\n",
      "plt.show()\n",
      "Figure 2-8. A histogram for each numerical attribute\n",
      "Get the Data | 47\n",
      "The hist() method relies on Matplotlib, which in turn relies on a\n",
      "user-specified graphical backend to draw on your screen. So before\n",
      "you can plot anything, you need to specify which backend Matplot‐\n",
      "lib should use. The simplest option is to use Jupyter’s magic com‐\n",
      "mand %matplotlib inline. This tells Jupyter to set up Matplotlib\n",
      "so it uses Jupyter’s own backend. Plots are then rendered within the\n",
      "notebook itself.\n",
      "\n",
      "Q: %matplotlib inline # only in a Jupyter notebook import\n",
      "A: matplotlib.pyplot\n",
      "\n",
      "Q: the number of instances (on the vertical axis) that have a given value range\n",
      "A: A histogram\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 98:\n",
      "Note that calling show() is optional in a Jupyter\n",
      "notebook, as Jupyter will automatically display plots when a cell is\n",
      "executed. Notice a few things in these histograms:\n",
      "1. First, the median income attribute does not look like it is expressed in US dollars\n",
      "(USD). After checking with the team that collected the data, you are told that the\n",
      "data has been scaled and capped at 15 (actually 15.0001) for higher median\n",
      "incomes, and at 0.5 (actually 0.4999) for lower median incomes. Working with\n",
      "preprocessed attributes is common in Machine Learning, and it is not necessarily\n",
      "a problem, but you should try to understand how the data was computed. 2. The housing median age and the median house value were also capped. The lat‐\n",
      "ter may be a serious problem since it is your target attribute (your labels). Your\n",
      "Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s out‐\n",
      "put) to see if this is a problem or not. If they tell you that they need precise pre‐\n",
      "dictions even beyond $500,000, then you have mainly two options:\n",
      "a.\n",
      "\n",
      "Q: the data. show() is optional in a Jupyter notebook, as J\n",
      "A: Jupyter will automatically display plots when a cell is\n",
      "executed\n",
      "\n",
      "Q: questions: Note that calling show() is optional in a Jupyter notebook, as\n",
      "A: Jupyter will automatically display plots when a cell is\n",
      "executed\n",
      "\n",
      "Q: a few things: 1 First, the median income attribute does not look like it is expressed\n",
      "A: US dollars\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 99:\n",
      "Collect proper labels for the districts whose labels were capped. b. Remove those districts from the training set (and also from the test set, since\n",
      "your system should not be evaluated poorly if it predicts values beyond\n",
      "$500,000). 3. These attributes have very different scales. We will discuss this later in this chap‐\n",
      "ter when we explore feature scaling. 4. Finally, many histograms are tail heavy: they extend much farther to the right of\n",
      "the median than to the left. This may make it a bit harder for some Machine\n",
      "Learning algorithms to detect patterns. We will try transforming these attributes\n",
      "later on to have more bell-shaped distributions. Hopefully you now have a better understanding of the kind of data you are dealing\n",
      "with. 48 | Chapter 2: End-to-End Machine Learning Project\n",
      "Wait! Before you look at the data any further, you need to create a\n",
      "test set, put it aside, and never look at it. Create a Test Set\n",
      "It may sound strange to voluntarily set aside part of the data at this stage.\n",
      "\n",
      "Q: . b. Remove districts from the training set (and also from the test set), since\n",
      "A: \n",
      "$500,000\n",
      "\n",
      "Q: b Remove those districts from the training set (and also from the test set, since your\n",
      "A: \n",
      "$500,000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 100:\n",
      "After all,\n",
      "you have only taken a quick glance at the data, and surely you should learn a whole\n",
      "lot more about it before you decide what algorithms to use, right? This is true, but\n",
      "your brain is an amazing pattern detection system, which means that it is highly\n",
      "prone to overfitting: if you look at the test set, you may stumble upon some seemingly\n",
      "interesting pattern in the test data that leads you to select a particular kind of\n",
      "Machine Learning model. When you estimate the generalization error using the test\n",
      "set, your estimate will be too optimistic and you will launch a system that will not\n",
      "perform as well as expected. This is called data snooping bias.\n",
      "\n",
      "Q: the pattern, and you should learn a lot more about it before you decide which algorithms to\n",
      "A: \n",
      "you have only taken a quick glance at the data\n",
      "\n",
      "Q: the generalization error using the test set, your estimate will be too optimistic and you will launch\n",
      "A: a system that will not\n",
      "perform as well as expected\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 101:\n",
      "Creating a test set is theoretically quite simple: just pick some instances randomly,\n",
      "typically 20% of the dataset, and set them aside:\n",
      "import numpy as np\n",
      "def split_train_test(data, test_ratio):\n",
      "shuffled_indices = np.random.permutation(len(data))\n",
      "test_set_size = int(len(data) * test_ratio)\n",
      "test_indices = shuffled_indices[:test_set_size]\n",
      "train_indices = shuffled_indices[test_set_size:]\n",
      "return data.iloc[train_indices], data.iloc[test_indices]\n",
      "You can then use this function like this:\n",
      ">>> train_set, test_set = split_train_test(housing, 0.2)\n",
      ">>> print(len(train_set), \"train +\", len(test_set), \"test\")\n",
      "16512 train + 4128 test\n",
      "Well, this works, but it is not perfect: if you run the program again, it will generate a\n",
      "different test set! Over time, you (or your Machine Learning algorithms) will get to\n",
      "see the whole dataset, which is what you want to avoid. One solution is to save the test set on the first run and then load it in subsequent\n",
      "runs. Another option is to set the random number generator’s seed (e.g., np.ran\n",
      "dom.seed(42))13 before calling np.random.permutation(), so that it always generates\n",
      "the same shuffled indices. 13 You will often see people set the random seed to 42. This number has no special property, other than to be\n",
      "The Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
      "\n",
      "Q: questions: Creating a test set is theoretically quite simple: just pick some instances randomly\n",
      "A: \n",
      "typically 20% of the dataset\n",
      "\n",
      "Q: questions: One solution is to save the test set on first run and then load it in subsequent\n",
      "A: runs\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 102:\n",
      "Get the Data | 49\n",
      "But both these solutions will break next time you fetch an updated dataset. A com‐\n",
      "mon solution is to use each instance’s identifier to decide whether or not it should go\n",
      "in the test set (assuming instances have a unique and immutable identifier). For\n",
      "example, you could compute a hash of each instance’s identifier, keep only the last\n",
      "byte of the hash, and put the instance in the test set if this value is lower or equal to\n",
      "51 (~20% of 256). This ensures that the test set will remain consistent across multiple\n",
      "runs, even if you refresh the dataset. The new test set will contain 20% of the new\n",
      "instances, but it will not contain any instance that was previously in the training set. Here is a possible implementation:\n",
      "import hashlib\n",
      "def test_set_check(identifier, test_ratio, hash):\n",
      "return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n",
      "def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n",
      "ids = data[id_column]\n",
      "in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n",
      "return data.loc[~in_test_set], data.loc[in_test_set]\n",
      "Unfortunately, the housing dataset does not have an identifier column.\n",
      "\n",
      "Q: : Get the Data | 49 But both these solutions will break next time you fetch an updated dataset\n",
      "A: \n",
      "mon\n",
      "\n",
      "Q: questions: Get the Data | 49 But both these solutions will break next time you fetch an updated\n",
      "A: \n",
      "ids\n",
      "\n",
      "Q: mon solution: com mon solution: Use each instance’s identifier to decide\n",
      "A: whether or not it should go\n",
      "in the test set\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 103:\n",
      "The simplest\n",
      "solution is to use the row index as the ID:\n",
      "housing_with_id = housing.reset_index() # adds an `index` column\n",
      "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
      "If you use the row index as a unique identifier, you need to make sure that new data\n",
      "gets appended to the end of the dataset, and no row ever gets deleted. If this is not\n",
      "possible, then you can try to use the most stable features to build a unique identifier. For example, a district’s latitude and longitude are guaranteed to be stable for a few\n",
      "million years, so you could combine them into an ID like so:14\n",
      "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
      "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
      "Scikit-Learn provides a few functions to split datasets into multiple subsets in various\n",
      "ways. The simplest function is train_test_split, which does pretty much the same\n",
      "thing as the function split_train_test defined earlier, with a couple of additional\n",
      "features.\n",
      "\n",
      "Q: , 0.2, \"id\") Scikit-Learn provides a few functions\n",
      "A: split datasets into multiple subsets in various\n",
      "ways\n",
      "\n",
      "Q: , 0.2, \"index\") If you use the row index as a unique identifie\n",
      "A: no row ever gets deleted\n",
      "\n",
      "Q: questions: If this is not possible, then you can try to use the most stable features to\n",
      "A: build a unique identifier\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 104:\n",
      "First there is a random_state parameter that allows you to set the random\n",
      "generator seed as explained previously, and second you can pass it multiple datasets\n",
      "with an identical number of rows, and it will split them on the same indices (this is\n",
      "very useful, for example, if you have a separate DataFrame for labels):\n",
      "14 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\n",
      "they will end up in the same set (test or train). This introduces some unfortunate sampling bias. 50 | Chapter 2: End-to-End Machine Learning Project\n",
      "from sklearn.model_selection import train_test_split\n",
      "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
      "So far we have considered purely random sampling methods. This is generally fine if\n",
      "your dataset is large enough (especially relative to the number of attributes), but if it\n",
      "is not, you run the risk of introducing a significant sampling bias. When a survey\n",
      "company decides to call 1,000 people to ask them a few questions, they don’t just pick\n",
      "1,000 people randomly in a phone booth. They try to ensure that these 1,000 people\n",
      "are representative of the whole population.\n",
      "\n",
      "Q: questions: First there is a random_state parameter that allows you to set the random generator\n",
      "A: \n",
      "\n",
      "\n",
      "Q: : This introduces some unfortunate sampling bias 50 | Chapter 2: End-to-End\n",
      "A: The location information is actually quite coarse\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 105:\n",
      "For example, the US population is com‐\n",
      "posed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\n",
      "try to maintain this ratio in the sample: 513 female and 487 male. This is called strati‐\n",
      "fied sampling: the population is divided into homogeneous subgroups called strata,\n",
      "and the right number of instances is sampled from each stratum to guarantee that the\n",
      "test set is representative of the overall population. If they used purely random sam‐\n",
      "pling, there would be about 12% chance of sampling a skewed test set with either less\n",
      "than 49% female or more than 54% female. Either way, the survey results would be\n",
      "significantly biased. Suppose you chatted with experts who told you that the median income is a very\n",
      "important attribute to predict median housing prices. You may want to ensure that\n",
      "the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create\n",
      "an income category attribute. Let’s look at the median income histogram more closely\n",
      "(see Figure 2-9):\n",
      "Figure 2-9.\n",
      "\n",
      "Q: a skewed test set with either less than 49% female or more than 5\n",
      "A: \n",
      "\n",
      "\n",
      "Q: com posed of 51.3% female and 48.7% male. Similarly,\n",
      "A: the US population\n",
      "\n",
      "Q: sampling: the population is divided into homogeneous subgroups called strata, and\n",
      "A: strati‐\n",
      "fied sampling\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 106:\n",
      "Histogram of income categories\n",
      "Most median income values are clustered around 2–5 (tens of thousands of dollars),\n",
      "but some median incomes go far beyond 6. It is important to have a sufficient num‐\n",
      "Get the Data | 51\n",
      "ber of instances in your dataset for each stratum, or else the estimate of the stratum’s\n",
      "importance may be biased. This means that you should not have too many strata, and\n",
      "each stratum should be large enough. The following code creates an income category\n",
      "attribute by dividing the median income by 1.5 (to limit the number of income cate‐\n",
      "gories), and rounding up using ceil (to have discrete categories), and then merging\n",
      "all the categories greater than 5 into category 5:\n",
      "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
      "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
      "Now you are ready to do stratified sampling based on the income category. For this\n",
      "you can use Scikit-Learn’s StratifiedShuffleSplit class:\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
      "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
      "strat_train_set = housing.loc[train_index]\n",
      "strat_test_set = housing.loc[test_index]\n",
      "Let’s see if this worked as expected.\n",
      "\n",
      "Q: based on the income category. based on the income category.,,\n",
      "A: stratified sampling\n",
      "\n",
      "Q: ssss questions: Histogram of income categories Most median income\n",
      "A: stratified sampling\n",
      "\n",
      "Q: Get the Data | 51 ber of instances in your dataset for each stratum, or else\n",
      "A: the estimate of the stratum’s\n",
      "importance may be biased\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 107:\n",
      "You can start by looking at the income category\n",
      "proportions in the full housing dataset:\n",
      ">>> housing[\"income_cat\"].value_counts() / len(housing)\n",
      "3.0 0.350581\n",
      "2.0 0.318847\n",
      "4.0 0.176308\n",
      "5.0 0.114438\n",
      "1.0 0.039826\n",
      "Name: income_cat, dtype: float64\n",
      "With similar code you can measure the income category proportions in the test set. Figure 2-10 compares the income category proportions in the overall dataset, in the\n",
      "test set generated with stratified sampling, and in a test set generated using purely\n",
      "random sampling. As you can see, the test set generated using stratified sampling has\n",
      "income category proportions almost identical to those in the full dataset, whereas the\n",
      "test set generated using purely random sampling is quite skewed. Figure 2-10. Sampling bias comparison of stratified versus purely random sampling\n",
      "52 | Chapter 2: End-to-End Machine Learning Project\n",
      "Now you should remove the income_cat attribute so the data is back to its original\n",
      "state:\n",
      "for set in (strat_train_set, strat_test_set):\n",
      "set.drop([\"income_cat\"], axis=1, inplace=True)\n",
      "We spent quite a bit of time on test set generation for a good reason: this is an often\n",
      "neglected but critical part of a Machine Learning project. Moreover, many of these\n",
      "ideas will be useful later when we discuss cross-validation.\n",
      "\n",
      "Q: questions: the income category proportions in the full housing dataset: >>> housing[\"\n",
      "A: \n",
      "3.0 0.350581\n",
      "\n",
      "\n",
      "Q: questions: You can start by looking at the income category proportions in the full housing dataset:\n",
      "A: \n",
      "3.0 0.350581\n",
      "\n",
      "\n",
      "Q: questions: Figure 2-10 compares the income category proportions in the overall dataset, in the\n",
      "A: the\n",
      "test set generated with stratified sampling\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 108:\n",
      "Now it’s time to move on\n",
      "to the next stage: exploring the data. Discover and Visualize the Data to Gain Insights\n",
      "So far you have only taken a quick glance at the data to get a general understanding of\n",
      "the kind of data you are manipulating. Now the goal is to go a little bit more in depth. First, make sure you have put the test set aside and you are only exploring the train‐\n",
      "ing set. Also, if the training set is very large, you may want to sample an exploration\n",
      "set, to make manipulations easy and fast. In our case, the set is quite small so you can\n",
      "just work directly on the full set. Let’s create a copy so you can play with it without\n",
      "harming the training set:\n",
      "housing = strat_train_set.copy()\n",
      "Visualizing Geographical Data\n",
      "Since there is geographical information (latitude and longitude), it is a good idea to\n",
      "create a scatterplot of all districts to visualize the data (Figure 2-11):\n",
      "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
      "Figure 2-11.\n",
      "\n",
      "Q: the data. Now it’s time to move on to the next stage: exploring the data\n",
      "A: \n",
      "\n",
      "\n",
      "Q: questions: Now it’s time to move on to another phase: exploring the data Discover and\n",
      "A: \n",
      "\n",
      "\n",
      "Q: questions: Discover and Visualize the Data to Gain Insights So far you have only\n",
      "A: taken a quick glance at the data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 109:\n",
      "A geographical scatterplot of the data\n",
      "Discover and Visualize the Data to Gain Insights | 53\n",
      "This looks like California all right, but other than that it is hard to see any particular\n",
      "pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places\n",
      "where there is a high density of data points (Figure 2-12):\n",
      "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
      "Figure 2-12. A better visualization highlighting high-density areas\n",
      "Now that’s much better: you can clearly see the high-density areas, namely the Bay\n",
      "Area and around Los Angeles and San Diego, plus a long line of fairly high density in\n",
      "the Central Valley, in particular around Sacramento and Fresno. More generally, our brains are very good at spotting patterns on pictures, but you\n",
      "may need to play around with visualization parameters to make the patterns stand\n",
      "out. Now let’s look at the housing prices (Figure 2-13). The radius of each circle represents\n",
      "the district’s population (option s), and the color represents the price (option c).\n",
      "\n",
      "Q: , but it’s hard to see any particular pattern. – patterns,\n",
      "A: California\n",
      "\n",
      "Q: a pattern? 53 This looks like California all right, but other than that it is hard\n",
      "A: \n",
      "pattern\n",
      "\n",
      "Q: , namely the Bay Area and around Los Angeles and San Diego, plus a long line\n",
      "A: \n",
      "housing\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 110:\n",
      "We\n",
      "will use a predefined color map (option cmap) called jet, which ranges from blue\n",
      "(low values) to red (high prices):15\n",
      "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
      "s=housing[\"population\"]/100, label=\"population\",\n",
      "c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
      ")\n",
      "plt.legend()\n",
      "15 If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\n",
      "down to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well. 54 | Chapter 2: End-to-End Machine Learning Project\n",
      "Figure 2-13. California housing prices\n",
      "This image tells you that the housing prices are very much related to the location\n",
      "(e.g., close to the ocean) and to the population density, as you probably knew already. It will probably be useful to use a clustering algorithm to detect the main clusters, and\n",
      "add new features that measure the proximity to the cluster centers. The ocean prox‐\n",
      "imity attribute may be useful as well, although in Northern California the housing\n",
      "prices in coastal districts are not too high, so it is not a simple rule.\n",
      "\n",
      "Q: , to the Bay Area down to San Diego (as you might expect). You can\n",
      "A: red pen\n",
      "\n",
      "Q: , a red pen and scribble over most of the coastline from the Bay\n",
      "A: \n",
      "plt.legend()\n",
      "\n",
      "\n",
      "Q: : You can add a patch of yellow around Sacramento as well 54 | Chapter 2: End\n",
      "A: \n",
      "housing.plot\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 111:\n",
      "Looking for Correlations\n",
      "Since the dataset is not too large, you can easily compute the standard correlation\n",
      "coefficient (also called Pearson’s r) between every pair of attributes using the corr()\n",
      "method:\n",
      "corr_matrix = housing.corr()\n",
      "Now let’s look at how much each attribute correlates with the median house value:\n",
      ">>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
      "median_house_value 1.000000\n",
      "median_income 0.687170\n",
      "total_rooms 0.135231\n",
      "housing_median_age 0.114220\n",
      "households 0.064702\n",
      "Discover and Visualize the Data to Gain Insights | 55\n",
      "total_bedrooms 0.047865\n",
      "population -0.026699\n",
      "longitude -0.047279\n",
      "latitude -0.142826\n",
      "Name: median_house_value, dtype: float64\n",
      "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
      "there is a strong positive correlation; for example, the median house value tends to go\n",
      "up when the median income goes up. When the coefficient is close to –1, it means\n",
      "that there is a strong negative correlation; you can see a small negative correlation\n",
      "between the latitude and the median house value (i.e., prices have a slight tendency to\n",
      "go down when you go north). Finally, coefficients close to zero mean that there is no\n",
      "linear correlation. Figure 2-14 shows various plots along with the correlation coeffi‐\n",
      "cient between their horizontal and vertical axes. Figure 2-14.\n",
      "\n",
      "Q: -1 to 1. The correlation coefficient ranges from –1 to 1. When it is\n",
      "A: close to 1, it means that\n",
      "there is a strong positive correlation\n",
      "\n",
      "Q: : –1 to 1 When it is close to 1, it means that there is a\n",
      "A: strong positive correlation\n",
      "\n",
      "Q: to –1 means that there is a strong negative correlation; for example, the median\n",
      "A: latitude\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 112:\n",
      "Standard correlation coefficient of various datasets (source: Wikipedia;\n",
      "public domain image)\n",
      "The correlation coefficient only measures linear correlations (“if x\n",
      "goes up, then y generally goes up/down”). It may completely miss\n",
      "out on nonlinear relationships (e.g., “if x is close to zero then y gen‐\n",
      "erally goes up”). Note how all the plots of the bottom row have a\n",
      "correlation coefficient equal to zero despite the fact that their axes\n",
      "are clearly not independent: these are examples of nonlinear rela‐\n",
      "tionships. Also, the second row shows examples where the correla‐\n",
      "tion coefficient is equal to 1 or –1; notice that this has nothing to\n",
      "do with the slope. For example, your height in inches has a correla‐\n",
      "tion coefficient of 1 with your height in feet or in nanometers. Another way to check for correlation between attributes is to use Pandas’\n",
      "scatter_matrix function, which plots every numerical attribute against every other\n",
      "numerical attribute.\n",
      "\n",
      "Q: x goes up, then y generally goes up/down). The correlation coefficient only measures\n",
      "A: linear correlations\n",
      "\n",
      "Q: questions: Standard correlation coefficient of various datasets (source: Wikipedia; public domain image) The\n",
      "A: The correlation coefficient only measures linear correlations\n",
      "\n",
      "Q: tionships.com.\n",
      "A: correla‐\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 113:\n",
      "Since there are now 11 numerical attributes, you would get 112 =\n",
      "56 | Chapter 2: End-to-End Machine Learning Project\n",
      "121 plots, which would not fit on a page, so let’s just focus on a few promising\n",
      "attributes that seem most correlated with the median housing value (Figure 2-15):\n",
      "from pandas.tools.plotting import scatter_matrix\n",
      "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
      "\"housing_median_age\"]\n",
      "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
      "Figure 2-15. Scatter matrix\n",
      "The main diagonal (top left to bottom right) would be full of straight lines if Pandas\n",
      "plotted each variable against itself, which would not be very useful. So instead Pandas\n",
      "displays a histogram of each attribute (other options are available; see Pandas’ docu‐\n",
      "mentation for more details). The most promising attribute to predict the median house value is the median\n",
      "income, so let’s zoom in on their correlation scatterplot (Figure 2-16):\n",
      "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
      "alpha=0.1)\n",
      "Discover and Visualize the Data to Gain Insights | 57\n",
      "Figure 2-16. Median income versus median house value\n",
      "This plot reveals a few things. First, the correlation is indeed very strong; you can\n",
      "clearly see the upward trend and the points are not too dispersed.\n",
      "\n",
      "Q: , so let’s focus on a few promising attributes that seem most correlated with the\n",
      "A: median housing value\n",
      "\n",
      "Q: , which would not fit on a page,, which would not fit on a\n",
      "A: 121 plots\n",
      "\n",
      "Q: questions: Scatter matrix The main diagonal (top left to bottom right) would be full of\n",
      "A: straight lines\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 114:\n",
      "Second, the price\n",
      "cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this\n",
      "plot reveals other less obvious straight lines: a horizontal line around $450,000,\n",
      "another around $350,000, perhaps one around $280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms\n",
      "from learning to reproduce these data quirks. Experimenting with Attribute Combinations\n",
      "Hopefully the previous sections gave you an idea of a few ways you can explore the\n",
      "data and gain insights. You identified a few data quirks that you may want to clean up\n",
      "before feeding the data to a Machine Learning algorithm, and you found interesting\n",
      "correlations between attributes, in particular with the target attribute. You also\n",
      "noticed that some attributes have a tail-heavy distribution, so you may want to trans‐\n",
      "form them (e.g., by computing their logarithm). Of course, your mileage will vary\n",
      "considerably with each project, but the general ideas are similar. One last thing you may want to do before actually preparing the data for Machine\n",
      "Learning algorithms is to try out various attribute combinations.\n",
      "\n",
      "Q: s Using Attribute Combinations Hopefully the previous sections gave you an idea of\n",
      "A: \n",
      "data and gain insights\n",
      "\n",
      "Q: the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000.\n",
      "A: \n",
      "plot\n",
      "\n",
      "Q: , around $280,000, and more. more questions: But this plot reveals other\n",
      "A: one around $280,000, and a few more below that\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 115:\n",
      "For example, the\n",
      "total number of rooms in a district is not very useful if you don’t know how many\n",
      "households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably\n",
      "want to compare it to the number of rooms. And the population per household also\n",
      "58 | Chapter 2: End-to-End Machine Learning Project\n",
      "seems like an interesting attribute combination to look at. Let’s create these new\n",
      "attributes:\n",
      "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
      "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
      "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
      "And now let’s look at the correlation matrix again:\n",
      ">>> corr_matrix = housing.corr()\n",
      ">>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
      "median_house_value 1.000000\n",
      "median_income 0.687170\n",
      "rooms_per_household 0.199343\n",
      "total_rooms 0.135231\n",
      "housing_median_age 0.114220\n",
      "households 0.064702\n",
      "total_bedrooms 0.047865\n",
      "population_per_household -0.021984\n",
      "population -0.026699\n",
      "longitude -0.047279\n",
      "latitude -0.142826\n",
      "bedrooms_per_room -0.260070\n",
      "Name: median_house_value, dtype: float64\n",
      "Hey, not bad! The new bedrooms_per_room attribute is much more correlated with\n",
      "the median house value than the total number of rooms or bedrooms. Apparently\n",
      "houses with a lower bedroom/room ratio tend to be more expensive. The number of\n",
      "rooms per household is also more informative than the total number of rooms in a\n",
      "district—obviously the larger the houses, the more expensive they are.\n",
      "\n",
      "Q: is more correlated with the median house value than the total number of rooms or bedrooms .\n",
      "A: The new bedrooms_per_room\n",
      "\n",
      "Q: Fragen: Exactly what you really want is the number of rooms per household.\n",
      "A: the\n",
      "total number of rooms in a district\n",
      "\n",
      "Q: Questions: What you really want is the number of rooms per household?\n",
      "A: the\n",
      "total number of rooms in a district\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 116:\n",
      "This round of exploration does not have to be absolutely thorough; the point is to\n",
      "start off on the right foot and quickly gain insights that will help you get a first rea‐\n",
      "sonably good prototype. But this is an iterative process: once you get a prototype up\n",
      "and running, you can analyze its output to gain more insights and come back to this\n",
      "exploration step. Prepare the Data for Machine Learning Algorithms\n",
      "It’s time to prepare the data for your Machine Learning algorithms. Instead of just\n",
      "doing this manually, you should write functions to do that, for several good reasons:\n",
      "• This will allow you to reproduce these transformations easily on any dataset (e.g.,\n",
      "the next time you get a fresh dataset). • You will gradually build a library of transformation functions that you can reuse\n",
      "in future projects. • You can use these functions in your live system to transform the new data before\n",
      "feeding it to your algorithms. Prepare the Data for Machine Learning Algorithms | 59\n",
      "• This will make it possible for you to easily try various transformations and see\n",
      "which combination of transformations works best.\n",
      "\n",
      "Q: to get a first rea sonably good prototype. But this is time to prepare\n",
      "A: ‐\n",
      "\n",
      "Q: to be a prototype, questions: This round of exploration does not have to be absolutely\n",
      "A: \n",
      "start off on the right foot and quickly gain insights\n",
      "\n",
      "Q: questions: But this is a iterative process: once you get a prototype up\n",
      "A: \n",
      "sonably good prototype\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 117:\n",
      "But first let’s revert to a clean training set (by copying strat_train_set once again),\n",
      "and let’s separate the predictors and the labels since we don’t necessarily want to apply\n",
      "the same transformations to the predictors and the target values (note that drop()\n",
      "creates a copy of the data and does not affect strat_train_set):\n",
      "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
      "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
      "Data Cleaning\n",
      "Most Machine Learning algorithms cannot work with missing features, so let’s create\n",
      "a few functions to take care of them. You noticed earlier that the total_bedrooms\n",
      "attribute has some missing values, so let’s fix this. You have three options:\n",
      "• Get rid of the corresponding districts. • Get rid of the whole attribute. • Set the values to some value (zero, the mean, the median, etc.). You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna()\n",
      "methods:\n",
      "housing.dropna(subset=[\"total_bedrooms\"]) # option 1\n",
      "housing.drop(\"total_bedrooms\", axis=1) # option 2\n",
      "median = housing[\"total_bedrooms\"].median()\n",
      "housing[\"total_bedrooms\"].fillna(median) # option 3\n",
      "If you choose option 3, you should compute the median value on the training set, and\n",
      "use it to fill the missing values in the training set, but also don’t forget to save the\n",
      "median value that you have computed.\n",
      "\n",
      "Q: the training set, and then revert to a clean training set (by copying\n",
      "A: strat_train_set\n",
      "\n",
      "Q: the data and does not affect strat_train_set., and a clean\n",
      "A: drop()\n",
      "\n",
      "Q: questions: You noticed earlier that the total_bedrooms attribute has some missing values, so let\n",
      "A: let’s fix this\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 118:\n",
      "You will need it later to replace missing values\n",
      "in the test set when you want to evaluate your system, and also once the system goes\n",
      "live to replace missing values in new data. Scikit-Learn provides a handy class to take care of missing values: Imputer. Here is\n",
      "how to use it. First, you need to create an Imputer instance, specifying that you want\n",
      "to replace each attribute’s missing values with the median of that attribute:\n",
      "from sklearn.preprocessing import Imputer\n",
      "imputer = Imputer(strategy=\"median\")\n",
      "Since the median can only be computed on numerical attributes, we need to create a\n",
      "copy of the data without the text attribute ocean_proximity:\n",
      "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
      "60 | Chapter 2: End-to-End Machine Learning Project\n",
      "Now you can fit the imputer instance to the training data using the fit() method:\n",
      "imputer.fit(housing_num)\n",
      "The imputer has simply computed the median of each attribute and stored the result\n",
      "in its statistics_ instance variable.\n",
      "\n",
      "Q: : Imputer. Here is how to use it. First, you need to create an Imp\n",
      "A: \n",
      "imputer = Imputer\n",
      "\n",
      "Q: the system goes live to replace missing values in new data Scikit-Learn provides handy\n",
      "A: Imputer\n",
      "\n",
      "Q: questions: Scikit-Learn provides handy class to take care of missing values: Imp\n",
      "A: \n",
      "imputer = Imputer\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 119:\n",
      "Only the total_bedrooms attribute had missing\n",
      "values, but we cannot be sure that there won’t be any missing values in new data after\n",
      "the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n",
      ">>> imputer.statistics_\n",
      "array([ -118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.5414])\n",
      ">>> housing_num.median().values\n",
      "array([ -118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.5414])\n",
      "Now you can use this “trained” imputer to transform the training set by replacing\n",
      "missing values by the learned medians:\n",
      "X = imputer.transform(housing_num)\n",
      "The result is a plain Numpy array containing the transformed features. If you want to\n",
      "put it back into a Pandas DataFrame, it’s simple:\n",
      "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
      "Scikit-Learn Design\n",
      "Scikit-Learn’s API is remarkably well designed. The main design principles are:16\n",
      "• Consistency. All objects share a consistent and simple interface:\n",
      "—Estimators. Any object that can estimate some parameters based on a dataset\n",
      "is called an estimator (e.g., an imputer is an estimator).\n",
      "\n",
      "Q: .. X = imputer.transform(housing_num)\n",
      "A: statistics_\n",
      "array\n",
      "\n",
      "Q: .statistics_ array([ -118.51 , 34.26\n",
      "A: imputer\n",
      "\n",
      "Q: , 1164 , 408\n",
      "A: imputer.statistics_\n",
      "array\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 120:\n",
      "The estimation itself is\n",
      "performed by the fit() method, and it takes only a dataset as a parameter (or\n",
      "two for supervised learning algorithms; the second dataset contains the\n",
      "labels). Any other parameter needed to guide the estimation process is con‐\n",
      "sidered a hyperparameter (such as an imputer’s strategy), and it must be set\n",
      "as an instance variable (generally via a constructor parameter). —Transformers. Some estimators (such as an imputer) can also transform a\n",
      "dataset; these are called transformers. Once again, the API is quite simple: the\n",
      "transformation is performed by the transform() method with the dataset to\n",
      "transform as a parameter. It returns the transformed dataset. This transforma‐\n",
      "tion generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called fit_transform()\n",
      "16 For more details on the design principles, see “API design for machine learning software: experiences from\n",
      "the scikit-learn project,” L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Müller, et al. (2013). Prepare the Data for Machine Learning Algorithms | 61\n",
      "that is equivalent to calling fit() and then transform() (but sometimes\n",
      "fit_transform() is optimized and runs much faster). —Predictors.\n",
      "\n",
      "Q: , and it returns the transformed dataset. The transformation is performed by the transform() method, and\n",
      "A: dataset to\n",
      "transform as a parameter\n",
      "\n",
      "Q: the estimation itself is performed by the fit() method, and it takes only a dataset as\n",
      "A: a parameter\n",
      "\n",
      "Q: questions: Any other parameter needed to guide the estimation process is con sidered a hyper\n",
      "A: —Transformers\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 121:\n",
      "Finally, some estimators are capable of making predictions given a\n",
      "dataset; they are called predictors. For example, the LinearRegression model\n",
      "in the previous chapter was a predictor: it predicted life satisfaction given a\n",
      "country’s GDP per capita. A predictor has a predict() method that takes a\n",
      "dataset of new instances and returns a dataset of corresponding predictions. It\n",
      "also has a score() method that measures the quality of the predictions given\n",
      "a test set (and the corresponding labels in the case of supervised learning\n",
      "algorithms).17\n",
      "• Inspection. All the estimator’s hyperparameters are accessible directly via public\n",
      "instance variables (e.g., imputer.strategy), and all the estimator’s learned\n",
      "parameters are also accessible via public instance variables with an underscore\n",
      "suffix (e.g., imputer.statistics_). • Nonproliferation of classes. Datasets are represented as NumPy arrays or SciPy\n",
      "sparse matrices, instead of homemade classes. Hyperparameters are just regular\n",
      "Python strings or numbers. • Composition. Existing building blocks are reused as much as possible. For\n",
      "example, it is easy to create a Pipeline estimator from an arbitrary sequence of\n",
      "transformers followed by a final estimator, as we will see. • Sensible defaults.\n",
      "\n",
      "Q: questions: Finally, some estimators are capable of making predictions given a dataset; they are\n",
      "A: predictors\n",
      "\n",
      "Q: questions: Lastly, some estimators are capable of making predictions given a dataset; they\n",
      "A: predictors\n",
      "\n",
      "Q: : it predicted life satisfaction given a country’s GDP per capita.\n",
      "A: LinearRegression model\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 122:\n",
      "Scikit-Learn provides reasonable default values for most\n",
      "parameters, making it easy to create a baseline working system quickly. Handling Text and Categorical Attributes\n",
      "Earlier we left out the categorical attribute ocean_proximity because it is a text\n",
      "attribute so we cannot compute its median. Most Machine Learning algorithms pre‐\n",
      "fer to work with numbers anyway, so let’s convert these text labels to numbers. Scikit-Learn provides a transformer for this task called LabelEncoder:\n",
      ">>> from sklearn.preprocessing import LabelEncoder\n",
      ">>> encoder = LabelEncoder()\n",
      ">>> housing_cat = housing[\"ocean_proximity\"]\n",
      ">>> housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
      ">>> housing_cat_encoded\n",
      "array([1, 1, 4, ..., 1, 0, 3])\n",
      "17 Some predictors also provide methods to measure the confidence of their predictions. 62 | Chapter 2: End-to-End Machine Learning Project\n",
      "This is better: now we can use this numerical data in any ML algorithm. You can look\n",
      "at the mapping that this encoder has learned using the classes_ attribute (“<1H\n",
      "OCEAN” is mapped to 0, “INLAND” is mapped to 1, etc. ):\n",
      ">>> print(encoder.classes_)\n",
      "['<1H OCEAN' 'INLAND' 'ISLAND' 'NEAR BAY' 'NEAR OCEAN']\n",
      "One issue with this representation is that ML algorithms will assume that two nearby\n",
      "values are more similar than two distant values.\n",
      "\n",
      "Q: .. '1H OCEAN' 'INLAND'\n",
      "A: ['<\n",
      "\n",
      "Q: questions: Scikit-Learn provides reasonable default values for most parameters, making it easy\n",
      "A: \n",
      "values are more similar than two distant values\n",
      "\n",
      "Q: fer to work with numbers anyway. Let’s convert text labels to numbers anyway.\n",
      "A: Most Machine Learning algorithms pre‐\n",
      "fer\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 123:\n",
      "Obviously this is not the case (for\n",
      "example, categories 0 and 4 are more similar than categories 0 and 1). To fix this\n",
      "issue, a common solution is to create one binary attribute per category: one attribute\n",
      "equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute\n",
      "equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is\n",
      "called one-hot encoding, because only one attribute will be equal to 1 (hot), while the\n",
      "others will be 0 (cold). Scikit-Learn provides a OneHotEncoder encoder to convert integer categorical values\n",
      "into one-hot vectors. Let’s encode the categories as one-hot vectors. Note that\n",
      "fit_transform() expects a 2D array, but housing_cat_encoded is a 1D array, so we\n",
      "need to reshape it:18\n",
      ">>> from sklearn.preprocessing import OneHotEncoder\n",
      ">>> encoder = OneHotEncoder()\n",
      ">>> housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
      ">>> housing_cat_1hot\n",
      "<16513x5 sparse matrix of type '<class 'numpy.float64'>'\n",
      "with 16513 stored elements in Compressed Sparse Row format>\n",
      "Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very\n",
      "useful when you have categorical attributes with thousands of categories.\n",
      "\n",
      "Q: a 2D array, but only one attribute will be equal to 1 (hot), while\n",
      "A: one-hot encoding\n",
      "\n",
      "Q: 0 and 1) To fix this issue, a common solution is to create one binary attribute\n",
      "A: one-hot encoding\n",
      "\n",
      "Q: questions: To fix this issue, a common solution is to create one binary attribute per category\n",
      "A: one-hot encoding\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 124:\n",
      "After one-\n",
      "hot encoding we get a matrix with thousands of columns, and the matrix is full of\n",
      "zeros except for one 1 per row. Using up tons of memory mostly to store zeros would\n",
      "be very wasteful, so instead a sparse matrix only stores the location of the nonzero\n",
      "elements. You can use it mostly like a normal 2D array,19 but if you really want to con‐\n",
      "vert it to a (dense) NumPy array, just call the toarray() method:\n",
      ">>> housing_cat_1hot.toarray()\n",
      "array([[ 0., 1., 0., 0., 0. ],\n",
      "[ 0., 1., 0., 0., 0. ],\n",
      "[ 0., 0., 0., 0., 1. ],\n",
      "...,\n",
      "[ 0., 1., 0., 0., 0. ],\n",
      "18 NumPy’s reshape() function allows one dimension to be –1, which means “unspecified”: the value is inferred\n",
      "from the length of the array and the remaining dimensions. 19 See SciPy’s documentation for more details. Prepare the Data for Machine Learning Algorithms | 63\n",
      "[ 1., 0., 0., 0., 0. ],\n",
      "[ 0., 0., 0., 1., 0.]])\n",
      "\n",
      "Q: ,,,,::,, and the matrix is\n",
      "A: full of\n",
      "zeros\n",
      "\n",
      "Q: encoding, so it only stores the location of the non-zero elements.\n",
      "A: sparse matrix\n",
      "\n",
      "Q: zeros. This is a question: How do you use it mostly like a normal\n",
      "A: You can use it mostly like a normal 2D array\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 125:\n",
      "We can apply both transformations (from text categories to integer categories, then\n",
      "from integer categories to one-hot vectors) in one shot using the LabelBinarizer\n",
      "class:\n",
      ">>> from sklearn.preprocessing import LabelBinarizer\n",
      ">>> encoder = LabelBinarizer()\n",
      ">>> housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
      ">>> housing_cat_1hot\n",
      "array([[0, 1, 0, 0, 0],\n",
      "[0, 1, 0, 0, 0],\n",
      "[0, 0, 0, 0, 1],\n",
      "...,\n",
      "[0, 1, 0, 0, 0],\n",
      "[1, 0, 0, 0, 0],\n",
      "[0, 0, 0, 1, 0]])\n",
      "Note that this returns a dense NumPy array by default. You can get a sparse matrix\n",
      "instead by passing sparse_output=True to the LabelBinarizer constructor. Custom Transformers\n",
      "Although Scikit-Learn provides many useful transformers, you will need to write\n",
      "your own for tasks such as custom cleanup operations or combining specific\n",
      "attributes. You will want your transformer to work seamlessly with Scikit-Learn func‐\n",
      "tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\n",
      "itance), all you need is to create a class and implement three methods: fit()\n",
      "(returning self), transform(), and fit_transform(). You can get the last one for\n",
      "free by simply adding TransformerMixin as a base class.\n",
      "\n",
      "Q: a sparse matrix instead by passing sparse_output=True to\n",
      "A: the LabelBinarizer constructor\n",
      "\n",
      "Q: questions: You can get a sparse matrix instead by passing sparse_out\n",
      "A: True\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 126:\n",
      "Also, if you add BaseEstima\n",
      "tor as a base class (and avoid *args and **kargs in your constructor) you will get\n",
      "two extra methods (get_params() and set_params()) that will be useful for auto‐\n",
      "matic hyperparameter tuning. For example, here is a small transformer class that adds\n",
      "the combined attributes we discussed earlier:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
      "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
      "def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
      "self.add_bedrooms_per_room = add_bedrooms_per_room\n",
      "def fit(self, X, y=None):\n",
      "return self # nothing else to do\n",
      "def transform(self, X, y=None):\n",
      "rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
      "population_per_household = X[:, population_ix] / X[:, household_ix]\n",
      "if self.add_bedrooms_per_room:\n",
      "bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
      "64 | Chapter 2: End-to-End Machine Learning Project\n",
      "return np.c_[X, rooms_per_household, population_per_household,\n",
      "bedrooms_per_room]\n",
      "else:\n",
      "return np.c_[X, rooms_per_household, population_per_household]\n",
      "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
      "housing_extra_attribs = attr_adder.transform(housing.values)\n",
      "In this example the transformer has one hyperparameter, add_bedrooms_per_room,\n",
      "set to True by default (it is often helpful to provide sensible defaults). This hyperpara‐\n",
      "meter will allow you to easily find out whether adding this attribute helps the\n",
      "Machine Learning algorithms or not.\n",
      "\n",
      "Q: ,,,,,, add_bedrooms_per_room\n",
      "A: \n",
      "64\n",
      "\n",
      "Q: :_bedrooms_per_room = True by default (it is often helpful to\n",
      "A: hyperparameter\n",
      "\n",
      "Q: ,,,,, a small transformer class that adds the combined\n",
      "A: BaseEstima\n",
      "tor\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 127:\n",
      "More generally, you can add a hyperparameter\n",
      "to gate any data preparation step that you are not 100% sure about. The more you\n",
      "automate these data preparation steps, the more combinations you can automatically\n",
      "try out, making it much more likely that you will find a great combination (and sav‐\n",
      "ing you a lot of time). Feature Scaling\n",
      "One of the most important transformations you need to apply to your data is feature\n",
      "scaling. With few exceptions, Machine Learning algorithms don’t perform well when\n",
      "the input numerical attributes have very different scales. This is the case for the hous‐\n",
      "ing data: the total number of rooms ranges from about 6 to 39,320, while the median\n",
      "incomes only range from 0 to 15. Note that scaling the target values is generally not\n",
      "required. There are two common ways to get all attributes to have the same scale: min-max\n",
      "scaling and standardization. Min-max scaling (many people call this normalization) is quite simple: values are\n",
      "shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\n",
      "ing the min value and dividing by the max minus the min.\n",
      "\n",
      "Q: the data, the more combinations you can automatically try out, making it much more likely that you\n",
      "A: The more you\n",
      "automate these data preparation steps\n",
      "\n",
      "Q: the more combinations you can automatically try out, making it much more likely that you will find\n",
      "A: The more you\n",
      "automate these data preparation steps\n",
      "\n",
      "Q: the more you automate these data preparation steps, the more combinations you can automatically try out,\n",
      "A: Feature Scaling\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 128:\n",
      "Scikit-Learn provides a\n",
      "transformer called MinMaxScaler for this. It has a feature_range hyperparameter\n",
      "that lets you change the range if you don’t want 0–1 for some reason. Standardization is quite different: first it subtracts the mean value (so standardized\n",
      "values always have a zero mean), and then it divides by the variance so that the result‐\n",
      "ing distribution has unit variance. Unlike min-max scaling, standardization does not\n",
      "bound values to a specific range, which may be a problem for some algorithms (e.g.,\n",
      "neural networks often expect an input value ranging from 0 to 1). However, standard‐\n",
      "ization is much less affected by outliers. For example, suppose a district had a median\n",
      "income equal to 100 (by mistake). Min-max scaling would then crush all the other\n",
      "values from 0–15 down to 0–0.15, whereas standardization would not be much affec‐\n",
      "ted. Scikit-Learn provides a transformer called StandardScaler for standardization. Prepare the Data for Machine Learning Algorithms | 65\n",
      "As with all the transformations, it is important to fit the scalers to\n",
      "the training data only, not to the full dataset (including the test set).\n",
      "\n",
      "Q: , but it is not much affec ted. Scikit-Lear\n",
      "A: standardization\n",
      "\n",
      "Q: questions: Scikit-Learn provides a transformer called MinMaxScaler for this\n",
      "A: Standardization\n",
      "\n",
      "Q: Frage: Does Standardization always have a zero mean? It has a feature_range hyper\n",
      "A: standardized\n",
      "values always have a zero mean\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 129:\n",
      "Only then can you use them to transform the training set and the\n",
      "test set (and new data). Transformation Pipelines\n",
      "As you can see, there are many data transformation steps that need to be executed in\n",
      "the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\n",
      "such sequences of transformations. Here is a small pipeline for the numerical\n",
      "attributes:\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "num_pipeline = Pipeline([\n",
      "('imputer', Imputer(strategy=\"median\")),\n",
      "('attribs_adder', CombinedAttributesAdder()),\n",
      "('std_scaler', StandardScaler()),\n",
      "])\n",
      "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
      "The Pipeline constructor takes a list of name/estimator pairs defining a sequence of\n",
      "steps. All but the last estimator must be transformers (i.e., they must have a\n",
      "fit_transform() method). The names can be anything you like. When you call the pipeline’s fit() method, it calls fit_transform() sequentially on\n",
      "all transformers, passing the output of each call as the parameter to the next call, until\n",
      "it reaches the final estimator, for which it just calls the fit() method. The pipeline exposes the same methods as the final estimator.\n",
      "\n",
      "Q: Pipeline from sklearn.preprocessing import Pipeline from skle\n",
      "A: Transformation Pipelines\n",
      "\n",
      "Q: the training set and the test set (and new data) Transformation Pipelines As you can see\n",
      "A: there are many data transformation steps\n",
      "\n",
      "Q: questions: Transformation Pipelines As you can see, there are many steps that need to be executed\n",
      "A: in\n",
      "the right order\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 130:\n",
      "In this example, the last\n",
      "estimator is a StandardScaler, which is a transformer, so the pipeline has a trans\n",
      "form() method that applies all the transforms to the data in sequence (it also has a\n",
      "fit_transform method that we could have used instead of calling fit() and then\n",
      "transform()). You now have a pipeline for numerical values, and you also need to apply the LabelBi\n",
      "narizer on the categorical values: how can you join these transformations into a sin‐\n",
      "gle pipeline? Scikit-Learn provides a FeatureUnion class for this. You give it a list of\n",
      "transformers (which can be entire transformer pipelines), and when its transform()\n",
      "method is called it runs each transformer’s transform() method in parallel, waits for\n",
      "their output, and then concatenates them and returns the result (and of course calling\n",
      "its fit() method calls all each transformer’s fit() method).\n",
      "\n",
      "Q: a standardScaler, which is a transformer, so the pipeline has a\n",
      "A: the last\n",
      "estimator\n",
      "\n",
      "Q: Questions: How can you join these transformations into a sin gle pipeline? Scikit\n",
      "A: ‐\n",
      "\n",
      "Q: : How can you join these transformations into a sin gle pipeline? Scikit\n",
      "A: ‐\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 131:\n",
      "A full pipeline handling\n",
      "both numerical and categorical attributes may look like this:\n",
      "66 | Chapter 2: End-to-End Machine Learning Project\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "num_attribs = list(housing_num)\n",
      "cat_attribs = [\"ocean_proximity\"]\n",
      "num_pipeline = Pipeline([\n",
      "('selector', DataFrameSelector(num_attribs)),\n",
      "('imputer', Imputer(strategy=\"median\")),\n",
      "('attribs_adder', CombinedAttributesAdder()),\n",
      "('std_scaler', StandardScaler()),\n",
      "])\n",
      "cat_pipeline = Pipeline([\n",
      "('selector', DataFrameSelector(cat_attribs)),\n",
      "('label_binarizer', LabelBinarizer()),\n",
      "])\n",
      "full_pipeline = FeatureUnion(transformer_list=[\n",
      "(\"num_pipeline\", num_pipeline),\n",
      "(\"cat_pipeline\", cat_pipeline),\n",
      "])\n",
      "And you can run the whole pipeline simply:\n",
      ">>> housing_prepared = full_pipeline.fit_transform(housing)\n",
      ">>> housing_prepared\n",
      "array([[ 0.73225807, -0.67331551, 0.58426443, ..., 0. ,\n",
      "0. , 0. ],\n",
      "[-0.99102923, 1.63234656, -0.92655887, ..., 0. ,\n",
      "0. , 0. ],\n",
      "[...]\n",
      ">>> housing_prepared.shape\n",
      "(16513, 17)\n",
      "Each subpipeline starts with a selector transformer: it simply transforms the data by\n",
      "selecting the desired attributes (numerical or categorical), dropping the rest, and con‐\n",
      "verting the resulting DataFrame to a NumPy array. There is nothing in Scikit-Learn\n",
      "to handle Pandas DataFrames,20 so we need to write a simple custom transformer for\n",
      "this task:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
      "def __init__(self, attribute_names):\n",
      "self.attribute_names = attribute_names\n",
      "def fit(self, X, y=None):\n",
      "return self\n",
      "20 But check out Pull Request #3886, which may introduce a ColumnTransformer class making attribute-specific\n",
      "transformations easy.\n",
      "\n",
      "Q: a complete pipeline handling both numerical and categorical attributes may look like this: >>>\n",
      "A: \n",
      "\n",
      "\n",
      "Q: : 66 | Chapter 2: End-to-End Machine Learning Project from s\n",
      "A: sklearn.pipeline\n",
      "\n",
      "Q: , 0 , 0\n",
      "A: \n",
      "0\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 132:\n",
      "You could also run pip3 install sklearn-pandas to get a DataFrameMapper class with\n",
      "a similar objective. Prepare the Data for Machine Learning Algorithms | 67\n",
      "def transform(self, X):\n",
      "return X[self.attribute_names].values\n",
      "Select and Train a Model\n",
      "At last! You framed the problem, you got the data and explored it, you sampled a\n",
      "training set and a test set, and you wrote transformation pipelines to clean up and\n",
      "prepare your data for Machine Learning algorithms automatically. You are now ready\n",
      "to select and train a Machine Learning model. Training and Evaluating on the Training Set\n",
      "The good news is that thanks to all these previous steps, things are now going to be\n",
      "much simpler than you might think. Let’s first train a Linear Regression model, like\n",
      "we did in the previous chapter:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lin_reg = LinearRegression()\n",
      "lin_reg.fit(housing_prepared, housing_labels)\n",
      "Done! You now have a working Linear Regression model. Let’s try it out on a few\n",
      "instances from the training set:\n",
      ">>> some_data = housing.iloc[:5]\n",
      ">>> some_labels = housing_labels.iloc[:5]\n",
      ">>> some_data_prepared = full_pipeline.transform(some_data)\n",
      ">>> print(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\n",
      "Predictions: [ 303104. 44800. 308928. 294208. 368704.]\n",
      "\n",
      "Q: , housing_labels) Done! You have a working Linear Regression\n",
      "A: \n",
      "lin_reg.fit(housing_prepared\n",
      "\n",
      "Q: a question: a problem, you got the data and explored it, you sample\n",
      "A: You framed\n",
      "\n",
      "Q: a model. a model a training set and test set, and you\n",
      "A: sampled\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 133:\n",
      ">>> print(\"Labels:\\t\\t\", list(some_labels))\n",
      "Labels: [359400.0, 69700.0, 302100.0, 301300.0, 351900.0]\n",
      "It works, although the predictions are not exactly accurate (e.g., the second prediction\n",
      "is off by more than 50%!). Let’s measure this regression model’s RMSE on the whole\n",
      "training set using Scikit-Learn’s mean_squared_error function:\n",
      ">>> from sklearn.metrics import mean_squared_error\n",
      ">>> housing_predictions = lin_reg.predict(housing_prepared)\n",
      ">>> lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
      ">>> lin_rmse = np.sqrt(lin_mse)\n",
      ">>> lin_rmse\n",
      "68628.413493824875\n",
      "Okay, this is better than nothing but clearly not a great score: most districts’\n",
      "median_housing_values range between $120,000 and $265,000, so a typical predic‐\n",
      "tion error of $68,628 is not very satisfying. This is an example of a model underfitting\n",
      "the training data. When this happens it can mean that the features do not provide\n",
      "enough information to make good predictions, or that the model is not powerful\n",
      "enough. As we saw in the previous chapter, the main ways to fix underfitting are to\n",
      "68 | Chapter 2: End-to-End Machine Learning Project\n",
      "select a more powerful model, to feed the training algorithm with better features, or\n",
      "to reduce the constraints on the model. This model is not regularized, so this rules\n",
      "out the last option.\n",
      "\n",
      "Q: , but it works, although the predictions are not exactly accurate (e.g., the\n",
      "A: the second prediction\n",
      "is off by more than 50%!).\n",
      "\n",
      "Q: RMSE on the whole training set using sklearn.metrics mean_\n",
      "A: mean_squared_error\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 134:\n",
      "You could try to add more features (e.g., the log of the popula‐\n",
      "tion), but first let’s try a more complex model to see how it does. Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding\n",
      "complex nonlinear relationships in the data (Decision Trees are presented in more\n",
      "detail in Chapter 6). The code should look familiar by now:\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "tree_reg = DecisionTreeRegressor()\n",
      "tree_reg.fit(housing_prepared, housing_labels)\n",
      "Now that the model is trained, let’s evaluate it on the training set:\n",
      ">>> housing_predictions = tree_reg.predict(housing_prepared)\n",
      ">>> tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
      ">>> tree_rmse = np.sqrt(tree_mse)\n",
      ">>> tree_rmse\n",
      "0.0\n",
      "Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\n",
      "it is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a\n",
      "model you are confident about, so you need to use part of the training set for train‐\n",
      "ing, and part for model validation.\n",
      "\n",
      "Q: , but it is not the only model that has overfit the data. How can you be\n",
      "A: the model has badly overfit the data. How can you be sure\n",
      "\n",
      "Q: tion tion tion tion tion tion could\n",
      "A: try to add more features\n",
      "\n",
      "Q: : How do we train a DecisionTreeRegressor This is a powerful\n",
      "A: capable of finding\n",
      "complex nonlinear relationships in the data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 135:\n",
      "Better Evaluation Using Cross-Validation\n",
      "One way to evaluate the Decision Tree model would be to use the train_test_split\n",
      "function to split the training set into a smaller training set and a validation set, then\n",
      "train your models against the smaller training set and evaluate them against the vali‐\n",
      "dation set. It’s a bit of work, but nothing too difficult and it would work fairly well. A great alternative is to use Scikit-Learn’s cross-validation feature. The following code\n",
      "performs K-fold cross-validation: it randomly splits the training set into 10 distinct\n",
      "subsets called folds, then it trains and evaluates the Decision Tree model 10 times,\n",
      "picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
      "scoring=\"neg_mean_squared_error\", cv=10)\n",
      "rmse_scores = np.sqrt(-scores)\n",
      "Select and Train a Model | 69\n",
      "Scikit-Learn cross-validation features expect a utility function\n",
      "(greater is better) rather than a cost function (lower is better), so\n",
      "the scoring function is actually the opposite of the MSE (i.e., a neg‐\n",
      "ative value), which is why the preceding code computes -scores\n",
      "before calculating the square root.\n",
      "\n",
      "Q: a value of -scores. The following code performs K-fold cross\n",
      "A: cross_val_score\n",
      "\n",
      "Q: : Better Evaluation Using Cross-Validation One way to evaluate the Decision Tree model\n",
      "A: train_test_split\n",
      "function\n",
      "\n",
      "Q: questions: It’s a bit of work, but nothing too difficult and it would work\n",
      "A: fairly well\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 136:\n",
      "Let’s look at the results:\n",
      ">>> def display_scores(scores):\n",
      "... print(\"Scores:\", scores)\n",
      "... print(\"Mean:\", scores.mean())\n",
      "... print(\"Standard deviation:\", scores.std())\n",
      "...\n",
      ">>> display_scores(tree_rmse_scores)\n",
      "Scores: [ 74678.4916885 64766.2398337 69632.86942005 69166.67693232\n",
      "71486.76507766 73321.65695983 71860.04741226 71086.32691692\n",
      "76934.2726093 69060.93319262]\n",
      "Mean: 71199.4280043\n",
      "Standard deviation: 3202.70522793\n",
      "Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to per‐\n",
      "form worse than the Linear Regression model! Notice that cross-validation allows\n",
      "you to get not only an estimate of the performance of your model, but also a measure\n",
      "of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\n",
      "score of approximately 71,200, generally ±3,200. You would not have this information\n",
      "if you just used one validation set. But cross-validation comes at the cost of training\n",
      "the model several times, so it is not always possible.\n",
      "\n",
      "Q: : [ 74678.4916885 64766.2398337 69632.\n",
      "A: >>> def display_scores\n",
      "\n",
      "Q: print(\"Scores:\", scores) .. print(\"Scores\n",
      "A: \n",
      "\n",
      "\n",
      "Q: print(\"Scores:\", scores) .. print(\"Mean:\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 137:\n",
      "Let’s compute the same scores for the Linear Regression model just to be sure:\n",
      ">>> lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
      "... scoring=\"neg_mean_squared_error\", cv=10)\n",
      "...\n",
      ">>> lin_rmse_scores = np.sqrt(-lin_scores)\n",
      ">>> display_scores(lin_rmse_scores)\n",
      "Scores: [ 70423.5893262 65804.84913139 66620.84314068 72510.11362141\n",
      "66414.74423281 71958.89083606 67624.90198297 67825.36117664\n",
      "72512.36533141 68028.11688067]\n",
      "Mean: 68972.377566\n",
      "Standard deviation: 2493.98819069\n",
      "That’s right: the Decision Tree model is overfitting so badly that it performs worse\n",
      "than the Linear Regression model. Let’s try one last model now: the RandomForestRegressor. As we will see in Chap‐\n",
      "ter 7, Random Forests work by training many Decision Trees on random subsets of\n",
      "the features, then averaging out their predictions. Building a model on top of many\n",
      "other models is called Ensemble Learning, and it is often a great way to push ML algo‐\n",
      "rithms even further.\n",
      "\n",
      "Q: : the Decision Tree model is overfitting so badly that it performs worse than the Line\n",
      "A: \n",
      "\n",
      "\n",
      "Q: : a .. score=\"neg_mean_squared_error\n",
      "A: cv=10\n",
      "\n",
      "Q: : np.sqrt(-lin_scores) >>\n",
      "A: lin_rmse_scores\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 138:\n",
      "We will skip most of the code since it is essentially the same as\n",
      "for the other models:\n",
      "70 | Chapter 2: End-to-End Machine Learning Project\n",
      ">>> from sklearn.ensemble import RandomForestRegressor\n",
      ">>> forest_reg = RandomForestRegressor()\n",
      ">>> forest_reg.fit(housing_prepared, housing_labels)\n",
      ">>> [...]\n",
      ">>> forest_rmse\n",
      "22542.396440343684\n",
      ">>> display_scores(forest_rmse_scores)\n",
      "Scores: [ 53789.2879722 50256.19806622 52521.55342602 53237.44937943\n",
      "52428.82176158 55854.61222549 52158.02291609 50093.66125649\n",
      "53240.80406125 52761.50852822]\n",
      "Mean: 52634.1919593\n",
      "Standard deviation: 1576.20472269\n",
      "Wow, this is much better: Random Forests look very promising. However, note that\n",
      "the score on the training set is still much lower than on the validation sets, meaning\n",
      "that the model is still overfitting the training set. Possible solutions for overfitting are\n",
      "to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. However, before you dive much deeper in Random Forests, you should try out many\n",
      "other models from various categories of Machine Learning algorithms (several Sup‐\n",
      "port Vector Machines with different kernels, possibly a neural network, etc. ), without\n",
      "spending too much time tweaking the hyperparameters. The goal is to shortlist a few\n",
      "(two to five) promising models. You should save every model you experiment with, so you can\n",
      "come back easily to any model you want.\n",
      "\n",
      "Q: : We will skip most of the code since it is essentially the same as for the other\n",
      "A: two to five\n",
      "\n",
      "Q: : We will skip most of the code since it is basically the same as for the other models\n",
      "A: two to five\n",
      "\n",
      "Q: Allerdings, note that the score on the training set is still much lower than on the validation sets\n",
      "A: two\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 139:\n",
      "Make sure you save both\n",
      "the hyperparameters and the trained parameters, as well as the\n",
      "cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types,\n",
      "and compare the types of errors they make. You can easily save\n",
      "Scikit-Learn models by using Python’s pickle module, or using\n",
      "sklearn.externals.joblib, which is more efficient at serializing\n",
      "large NumPy arrays:\n",
      "from sklearn.externals import joblib\n",
      "joblib.dump(my_model, \"my_model.pkl\")\n",
      "# and later...\n",
      "my_model_loaded = joblib.load(\"my_model.pkl\")\n",
      "Fine-Tune Your Model\n",
      "Let’s assume that you now have a shortlist of promising models. You now need to\n",
      "fine-tune them. Let’s look at a few ways you can do that. Fine-Tune Your Model | 71\n",
      "Grid Search\n",
      "One way to do that would be to fiddle with the hyperparameters manually, until you\n",
      "find a great combination of hyperparameter values. This would be very tedious work,\n",
      "and you may not have time to explore many combinations. Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to\n",
      "do is tell it which hyperparameters you want it to experiment with, and what values to\n",
      "try out, and it will evaluate all the possible combinations of hyperparameter values,\n",
      "using cross-validation.\n",
      "\n",
      "Q: , and then... my_model_loaded = joblib.load(\"my_model\n",
      "A: #\n",
      "\n",
      "Q: questions: Make sure you save both the hyperparameters and the trained parameters, as well\n",
      "A: \n",
      "cross-validation scores\n",
      "\n",
      "Q: , and compare the types of errors they make. you can easily compare scores across model types\n",
      "A: \n",
      "the hyperparameters and the trained parameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 140:\n",
      "For example, the following code searches for the best combi‐\n",
      "nation of hyperparameter values for the RandomForestRegressor:\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "param_grid = [\n",
      "{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
      "{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
      "]\n",
      "forest_reg = RandomForestRegressor()\n",
      "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
      "scoring='neg_mean_squared_error')\n",
      "grid_search.fit(housing_prepared, housing_labels)\n",
      "When you have no idea what value a hyperparameter should have,\n",
      "a simple approach is to try out consecutive powers of 10 (or a\n",
      "smaller number if you want a more fine-grained search, as shown\n",
      "in this example with the n_estimators hyperparameter). This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of\n",
      "n_estimators and max_features hyperparameter values specified in the first dict\n",
      "(don’t worry about what these hyperparameters mean for now; they will be explained\n",
      "in Chapter 7), then try all 2 × 3 = 6 combinations of hyperparameter values in the\n",
      "second dict, but this time with the bootstrap hyperparameter set to False instead of\n",
      "True (which is the default value for this hyperparameter).\n",
      "\n",
      "Q: n_estimators and max_features hyperparameter values specified in the first\n",
      "A: dict\n",
      "\n",
      "\n",
      "Q: : This param_grid tells Scikit-Learn to first evaluate all\n",
      "A: first dict\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 141:\n",
      "All in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRe\n",
      "gressor hyperparameter values, and it will train each model five times (since we are\n",
      "using five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90\n",
      "rounds of training! It may take quite a long time, but when it is done you can get the\n",
      "best combination of parameters like this:\n",
      ">>> grid_search.best_params_\n",
      "{'max_features': 6, 'n_estimators': 30}\n",
      "72 | Chapter 2: End-to-End Machine Learning Project\n",
      "Since 30 is the maximum value of n_estimators that was evalu‐\n",
      "ated, you should probably evaluate higher values as well, since the\n",
      "score may continue to improve. You can also get the best estimator directly:\n",
      ">>> grid_search.best_estimator_\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "max_features=6, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "n_estimators=30, n_jobs=1, oob_score=False, random_state=None,\n",
      "verbose=0, warm_start=False)\n",
      "If GridSearchCV is initialized with refit=True (which is the\n",
      "default), then once it finds the best estimator using cross-\n",
      "validation, it retrains it on the whole training set. This is usually a\n",
      "good idea since feeding it more data will likely improve its perfor‐\n",
      "mance.\n",
      "\n",
      "Q: n_estimators: 30 72 | Chapter 2: End-to-En\n",
      "A: >>> grid_search.best_params_\n",
      "\n",
      "\n",
      "Q: n_estimators that was evalu ated, you should probably evaluate\n",
      "A: higher values\n",
      "\n",
      "Q: n_estimators: 30 = 90 rounds of training! ! !\n",
      "A: \n",
      "72\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 142:\n",
      "And of course the evaluation scores are also available:\n",
      ">>> cvres = grid_search.cv_results_\n",
      "... for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
      "... print(np.sqrt(-mean_score), params)\n",
      "...\n",
      "64912.0351358 {'max_features': 2, 'n_estimators': 3}\n",
      "55535.2786524 {'max_features': 2, 'n_estimators': 10}\n",
      "52940.2696165 {'max_features': 2, 'n_estimators': 30}\n",
      "60384.0908354 {'max_features': 4, 'n_estimators': 3}\n",
      "52709.9199934 {'max_features': 4, 'n_estimators': 10}\n",
      "50503.5985321 {'max_features': 4, 'n_estimators': 30}\n",
      "59058.1153485 {'max_features': 6, 'n_estimators': 3}\n",
      "52172.0292957 {'max_features': 6, 'n_estimators': 10}\n",
      "49958.9555932 {'max_features': 6, 'n_estimators': 30}\n",
      "59122.260006 {'max_features': 8, 'n_estimators': 3}\n",
      "52441.5896087 {'max_features': 8, 'n_estimators': 10}\n",
      "50041.4899416 {'max_features': 8, 'n_estimators': 30}\n",
      "62371.1221202 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54572.2557534 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "59634.0533132 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52456.0883904 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "58825.665239 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "52012.9945396 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n",
      "In this example, we obtain the best solution by setting the max_features hyperpara‐\n",
      "meter to 6, and the n_estimators hyperparameter to 30. The RMSE score for this\n",
      "combination is 49,959, which is slightly better than the score you got earlier using the\n",
      "Fine-Tune Your Model | 73\n",
      "default hyperparameter values (which was 52,634). Congratulations, you have suc‐\n",
      "cessfully fine-tuned your best model!\n",
      "\n",
      "Q: : 2, 'n_estimators': 30 52940.2696\n",
      "A: \n",
      "62371.122\n",
      "\n",
      "Q: the scores are also available: >>> cvres = grid_search.cv\n",
      "A: \n",
      "62371.122\n",
      "\n",
      "Q: : 2, 'n_estimators': 3 52012.9945396\n",
      "A: \n",
      "62371\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 143:\n",
      "Don’t forget that you can treat some of the data preparation steps as\n",
      "hyperparameters. For example, the grid search will automatically\n",
      "find out whether or not to add a feature you were not sure about\n",
      "(e.g., using the add_bedrooms_per_room hyperparameter of your\n",
      "CombinedAttributesAdder transformer). It may similarly be used\n",
      "to automatically find the best way to handle outliers, missing fea‐\n",
      "tures, feature selection, and more. Randomized Search\n",
      "The grid search approach is fine when you are exploring relatively few combinations,\n",
      "like in the previous example, but when the hyperparameter search space is large, it is\n",
      "often preferable to use RandomizedSearchCV instead. This class can be used in much\n",
      "the same way as the GridSearchCV class, but instead of trying out all possible combi‐\n",
      "nations, it evaluates a given number of random combinations by selecting a random\n",
      "value for each hyperparameter at every iteration. This approach has two main bene‐\n",
      "fits:\n",
      "• If you let the randomized search run for, say, 1,000 iterations, this approach will\n",
      "explore 1,000 different values for each hyperparameter (instead of just a few val‐\n",
      "ues per hyperparameter with the grid search approach).\n",
      "\n",
      "Q: s, but when the hyperparameter search space is large, it is often preferable to\n",
      "A: RandomizedSearchCV\n",
      "\n",
      "Q: the hyperparameter of your CombinedAttributesAdder transformer) or not.\n",
      "A: add_bedrooms_per_room\n",
      "\n",
      "Q: the grid search will automatically find out whether or not to add a feature you were not sure\n",
      "A: \n",
      "hyperparameters\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 144:\n",
      "• You have more control over the computing budget you want to allocate to hyper‐\n",
      "parameter search, simply by setting the number of iterations. Ensemble Methods\n",
      "Another way to fine-tune your system is to try to combine the models that perform\n",
      "best. The group (or “ensemble”) will often perform better than the best individual\n",
      "model (just like Random Forests perform better than the individual Decision Trees\n",
      "they rely on), especially if the individual models make very different types of errors. We will cover this topic in more detail in Chapter 7. Analyze the Best Models and Their Errors\n",
      "You will often gain good insights on the problem by inspecting the best models.\n",
      "\n",
      "Q: You have more control over the computing budget you want to allocate to hyper parameter search .\n",
      "A: \n",
      "parameter search, simply by setting the number of iterations\n",
      "\n",
      "Q: • You have more control over the computing budget you want to allocate to hyper parameter search, simply\n",
      "A: setting the number of iterations\n",
      "\n",
      "Q: Methods.\n",
      "A: \n",
      "parameter search\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 145:\n",
      "For\n",
      "example, the RandomForestRegressor can indicate the relative importance of each\n",
      "attribute for making accurate predictions:\n",
      ">>> feature_importances = grid_search.best_estimator_.feature_importances_\n",
      ">>> feature_importances\n",
      "array([ 7.14156423e-02, 6.76139189e-02, 4.44260894e-02,\n",
      "74 | Chapter 2: End-to-End Machine Learning Project\n",
      "1.66308583e-02, 1.66076861e-02, 1.82402545e-02,\n",
      "1.63458761e-02, 3.26497987e-01, 6.04365775e-02,\n",
      "1.13055290e-01, 7.79324766e-02, 1.12166442e-02,\n",
      "1.53344918e-01, 8.41308969e-05, 2.68483884e-03,\n",
      "3.46681181e-03])\n",
      "Let’s display these importance scores next to their corresponding attribute names:\n",
      ">>> extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
      ">>> cat_one_hot_attribs = list(encoder.classes_)\n",
      ">>> attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
      ">>> sorted(zip(feature_importances, attributes), reverse=True)\n",
      "[(0.32649798665134971, 'median_income'),\n",
      "(0.15334491760305854, 'INLAND'),\n",
      "(0.11305529021187399, 'pop_per_hhold'),\n",
      "(0.07793247662544775, 'bedrooms_per_room'),\n",
      "(0.071415642259275158, 'longitude'),\n",
      "(0.067613918945568688, 'latitude'),\n",
      "(0.060436577499703222, 'rooms_per_hhold'),\n",
      "(0.04442608939578685, 'housing_median_age'),\n",
      "(0.018240254462909437, 'population'),\n",
      "(0.01663085833886218, 'total_rooms'),\n",
      "(0.016607686091288865, 'total_bedrooms'),\n",
      "(0.016345876147580776, 'households'),\n",
      "(0.011216644219017424, '<1H OCEAN'),\n",
      "(0.0034668118081117387, 'NEAR OCEAN'),\n",
      "(0.0026848388432755429, 'NEAR BAY'),\n",
      "(8.4130896890070617e-05, 'ISLAND')]\n",
      "With this information, you may want to try dropping some of the less useful features\n",
      "(e.g., apparently only one ocean_proximity category is really useful, so you could try\n",
      "dropping the others). You should also look at the specific errors that your system makes, then try to under‐\n",
      "stand why it makes them and what could fix the problem (adding extra features or, on\n",
      "the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\n",
      "\n",
      "Q: ,,, 'bedrooms_per_room'), (0.01660\n",
      "A: 016607686091288865\n",
      "\n",
      "Q: questions: You should also look at the specific errors that your system makes, then try to under\n",
      "A: under‐\n",
      "stand why it makes them and what could fix the problem\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 146:\n",
      "Evaluate Your System on the Test Set\n",
      "After tweaking your models for a while, you eventually have a system that performs\n",
      "sufficiently well. Now is the time to evaluate the final model on the test set. There is\n",
      "nothing special about this process; just get the predictors and the labels from your\n",
      "test set, run your full_pipeline to transform the data (call transform(), not\n",
      "fit_transform()! ), and evaluate the final model on the test set:\n",
      "final_model = grid_search.best_estimator_\n",
      "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
      "y_test = strat_test_set[\"median_house_value\"].copy()\n",
      "X_test_prepared = full_pipeline.transform(X_test)\n",
      "Fine-Tune Your Model | 75\n",
      "final_predictions = final_model.predict(X_test_prepared)\n",
      "final_mse = mean_squared_error(y_test, final_predictions)\n",
      "final_rmse = np.sqrt(final_mse) # => evaluates to 48,209.6\n",
      "The performance will usually be slightly worse than what you measured using cross-\n",
      "validation if you did a lot of hyperparameter tuning (because your system ends up\n",
      "fine-tuned to perform well on the validation data, and will likely not perform as well\n",
      "on unknown datasets). It is not the case in this example, but when this happens you\n",
      "must resist the temptation to tweak the hyperparameters to make the numbers look\n",
      "good on the test set; the improvements would be unlikely to generalize to new data.\n",
      "\n",
      "Q: the test set. to make the numbers look good on the test set..\n",
      "A: tweak the hyperparameters\n",
      "\n",
      "Q: the test set After tweaking your models for a while, you eventually have a system\n",
      "A: performs\n",
      "sufficiently well\n",
      "\n",
      "Q: if you did some hyperparameter tuning, your system ends up fine-tuned\n",
      "A: to perform well on the validation data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 147:\n",
      "Now comes the project prelaunch phase: you need to present your solution (high‐\n",
      "lighting what you have learned, what worked and what did not, what assumptions\n",
      "were made, and what your system’s limitations are), document everything, and create\n",
      "nice presentations with clear visualizations and easy-to-remember statements (e.g.,\n",
      "“the median income is the number one predictor of housing prices”). Launch, Monitor, and Maintain Your System\n",
      "Perfect, you got approval to launch! You need to get your solution ready for produc‐\n",
      "tion, in particular by plugging the production input data sources into your system\n",
      "and writing tests. You also need to write monitoring code to check your system’s live performance at\n",
      "regular intervals and trigger alerts when it drops. This is important to catch not only\n",
      "sudden breakage, but also performance degradation. This is quite common because\n",
      "models tend to “rot” as data evolves over time, unless the models are regularly trained\n",
      "on fresh data. Evaluating your system’s performance will require sampling the system’s predictions\n",
      "and evaluating them. This will generally require a human analysis. These analysts\n",
      "may be field experts, or workers on a crowdsourcing platform (such as Amazon\n",
      "Mechanical Turk or CrowdFlower).\n",
      "\n",
      "Q: the project prelaunch phase: the project prelaunch phase: you need to present your solution\n",
      "A: high‐\n",
      "\n",
      "Q: tion, you got approval to launch! Launch, Monitor, and Maintain Your System Perfect,\n",
      "A: Perfect\n",
      "\n",
      "Q: questions: Launch, Monitor, and Maintain Your System Perfect, you got approval to launch! You\n",
      "A: You need to get your solution ready for produc‐\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 148:\n",
      "Either way, you need to plug the human evalua‐\n",
      "tion pipeline into your system. You should also make sure you evaluate the system’s input data quality. Sometimes\n",
      "performance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\n",
      "tioning sensor sending random values, or another team’s output becoming stale), but\n",
      "it may take a while before your system’s performance degrades enough to trigger an\n",
      "alert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\n",
      "inputs is particularly important for online learning systems. Finally, you will generally want to train your models on a regular basis using fresh\n",
      "data. You should automate this process as much as possible. If you don’t, you are very\n",
      "76 | Chapter 2: End-to-End Machine Learning Project\n",
      "likely to refresh your model only every six months (at best), and your system’s perfor‐\n",
      "mance may fluctuate severely over time. If your system is an online learning system,\n",
      "you should make sure you save snapshots of its state at regular intervals so you can\n",
      "easily roll back to a previously working state. Try It Out!\n",
      "\n",
      "Q: tioning sensor sending random values, or another team’s output becoming stale\n",
      "A: malfunc‐\n",
      "\n",
      "Q: tion pipeline.tion pipelines tion. tion tion questions:\n",
      "A: \n",
      "alert\n",
      "\n",
      "Q: questions: What should you do to evaluate the system’s input data quality? - You\n",
      "A: You should also make sure you evaluate the system’s input data quality\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 149:\n",
      "Hopefully this chapter gave you a good idea of what a Machine Learning project\n",
      "looks like, and showed you some of the tools you can use to train a great system. As\n",
      "you can see, much of the work is in the data preparation step, building monitoring\n",
      "tools, setting up human evaluation pipelines, and automating regular model training. The Machine Learning algorithms are also important, of course, but it is probably\n",
      "preferable to be comfortable with the overall process and know three or four algo‐\n",
      "rithms well rather than to spend all your time exploring advanced algorithms and not\n",
      "enough time on the overall process. So, if you have not already done so, now is a good time to pick up a laptop, select a\n",
      "dataset that you are interested in, and try to go through the whole process from A to\n",
      "Z. A good place to start is on a competition website such as http://kaggle.com/: you\n",
      "will have a dataset to play with, a clear goal, and people to share the experience with. Exercises\n",
      "Using this chapter’s housing dataset:\n",
      "1.\n",
      "\n",
      "Q: , and a good idea of what a Machine Learning project looks like. Hopefully this\n",
      "A: chapter\n",
      "\n",
      "Q: questions: Hopefully this chapter gave you a good idea of what a Machine Learning project\n",
      "A: looks like\n",
      "\n",
      "Q: Frequently asked questions: As you can see, much of the work is in the data preparation step\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 150:\n",
      "Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyper‐\n",
      "parameters such as kernel=\"linear\" (with various values for the C hyperpara‐\n",
      "meter) or kernel=\"rbf\" (with various values for the C and gamma\n",
      "hyperparameters). Don’t worry about what these hyperparameters mean for now. How does the best SVR predictor perform? 2. Try replacing GridSearchCV with RandomizedSearchCV. 3. Try adding a transformer in the preparation pipeline to select only the most\n",
      "important attributes. 4. Try creating a single pipeline that does the full data preparation plus the final\n",
      "prediction. 5. Automatically explore some preparation options using GridSearchCV. Solutions to these exercises are available in the online Jupyter notebooks at https://\n",
      "github.com/ageron/handson-ml. Try It Out! | 77\n",
      "CHAPTER 3\n",
      "Classification\n",
      "In Chapter 1 we mentioned that the most common supervised learning tasks are\n",
      "regression (predicting values) and classification (predicting classes). In Chapter 2 we\n",
      "explored a regression task, predicting housing values, using various algorithms such\n",
      "as Linear Regression, Decision Trees, and Random Forests (which will be explained\n",
      "in further detail in later chapters). Now we will turn our attention to classification\n",
      "systems.\n",
      "\n",
      "Q: s).s a Support Vector Machine regressor (sklearn\n",
      "A: svm.SVR\n",
      "\n",
      "Q: meter) or kernel=\"rbf\" (with various values for the C hyperpara\n",
      "A: linear\n",
      "\n",
      "Q: Wie ergibt sich die Frage: Wie ergibt sich die beste SVR-V\n",
      "A: \n",
      "github\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 151:\n",
      "MNIST\n",
      "In this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\n",
      "images of digits handwritten by high school students and employees of the US Cen‐\n",
      "sus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\n",
      "ied so much that it is often called the “Hello World” of Machine Learning: whenever\n",
      "people come up with a new classification algorithm, they are curious to see how it\n",
      "will perform on MNIST. Whenever someone learns Machine Learning, sooner or\n",
      "later they tackle MNIST. Scikit-Learn provides many helper functions to download popular datasets. MNIST is\n",
      "one of them. The following code fetches the MNIST dataset:1\n",
      ">>> from sklearn.datasets import fetch_mldata\n",
      ">>> mnist = fetch_mldata('MNIST original')\n",
      ">>> mnist\n",
      "{'COL_NAMES': ['label', 'data'],\n",
      "'DESCR': 'mldata.org dataset: mnist-original',\n",
      "'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "[0, 0, 0, ..., 0, 0, 0],\n",
      "1 By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data.\n",
      "\n",
      "Q: : MNIST MNIST In this chapter, we will be using the MNIST\n",
      "A: mnist\n",
      "\n",
      "\n",
      "Q: MNIST In this chapter, we will be using the MNIST dataset, which is\n",
      "A: a set of 70,000 small\n",
      "images of digits\n",
      "\n",
      "Q: a new classification algorithm.\n",
      "A: they are curious to see how it\n",
      "will perform on MNIST\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 152:\n",
      "79\n",
      "[0, 0, 0, ..., 0, 0, 0],\n",
      "...,\n",
      "[0, 0, 0, ..., 0, 0, 0],\n",
      "[0, 0, 0, ..., 0, 0, 0],\n",
      "[0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
      "'target': array([ 0., 0., 0., ..., 9., 9., 9.])} Datasets loaded by Scikit-Learn generally have a similar dictionary structure includ‐\n",
      "ing:\n",
      "• A DESCR key describing the dataset\n",
      "• A data key containing an array with one row per instance and one column per\n",
      "feature\n",
      "• A target key containing an array with the labels\n",
      "Let’s look at these arrays:\n",
      ">>> X, y = mnist[\"data\"], mnist[\"target\"]\n",
      ">>> X.shape\n",
      "(70000, 784)\n",
      ">>> y.shape\n",
      "(70000,)\n",
      "There are 70,000 images, and each image has 784 features. This is because each image\n",
      "is 28×28 pixels, and each feature simply represents one pixel’s intensity, from 0\n",
      "(white) to 255 (black). Let’s take a peek at one digit from the dataset.\n",
      "\n",
      "Q: 'target': array([ 0., 0., 0., 0., ...,\n",
      "A: 9., 9., 9.])}\n",
      "\n",
      "Q: 'target': array([ 0., 0., 0., 0., 9., 9.\n",
      "A: 79\n",
      "\n",
      "\n",
      "Q: questions: This is because each image is 2828 pixels, and each feature simply represents one\n",
      "A: one pixel’s intensity\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 153:\n",
      "All you need to\n",
      "do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using\n",
      "Matplotlib’s imshow() function:\n",
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "some_digit = X[36000]\n",
      "some_digit_image = some_digit.reshape(28, 28)\n",
      "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
      "interpolation=\"nearest\")\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "This looks like a 5, and indeed that’s what the label tells us:\n",
      "80 | Chapter 3: Classification\n",
      ">>> y[36000]\n",
      "5.0\n",
      "Figure 3-1 shows a few more images from the MNIST dataset to give you a feel for\n",
      "the complexity of the classification task. Figure 3-1. A few digits from the MNIST dataset\n",
      "But wait! You should always create a test set and set it aside before inspecting the data\n",
      "closely. The MNIST dataset is actually already split into a training set (the first 60,000\n",
      "images) and a test set (the last 10,000 images):\n",
      "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
      "Let’s also shuffle the training set; this will guarantee that all cross-validation folds will\n",
      "be similar (you don’t want one fold to be missing some digits).\n",
      "\n",
      "Q: a 5 digits from the MNIST dataset. Figure 3-1 shows a\n",
      "A: \n",
      "plt.show()\n",
      "\n",
      "Q: a 5? The label says: 80 | Chapter 3: Classification >>> y\n",
      "A: \n",
      "5.0\n",
      "\n",
      "Q: questions: Figure 3-1 A few digits from the MNIST dataset But wait!\n",
      "A: \n",
      "plt.show()\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 154:\n",
      "Moreover, some learn‐\n",
      "ing algorithms are sensitive to the order of the training instances, and they perform\n",
      "poorly if they get many similar instances in a row. Shuffling the dataset ensures that\n",
      "this won’t happen:2\n",
      "2 Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\n",
      "stock market prices or weather conditions). We will explore this in the next chapters. MNIST | 81\n",
      "import numpy as np\n",
      "shuffle_index = np.random.permutation(60000)\n",
      "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
      "Training a Binary Classifier\n",
      "Let’s simplify the problem for now and only try to identify one digit—for example,\n",
      "the number 5. This “5-detector” will be an example of a binary classifier, capable of\n",
      "distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\n",
      "this classification task:\n",
      "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits. y_test_5 = (y_test == 5)\n",
      "Okay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\n",
      "Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This clas‐\n",
      "sifier has the advantage of being capable of handling very large datasets efficiently.\n",
      "\n",
      "Q: a binary classifier. a classifier. a classifier.\n",
      "A: Stochastic\n",
      "Gradient Descent (SGD) classifier\n",
      "\n",
      "Q: ing algorithms are sensitive to the order of the training instances, and they perform poorly if\n",
      "A: they get many similar instances in a row\n",
      "\n",
      "Q: : Shuffling the dataset ensures that this won’t happen:2 2 Shuff\n",
      "A: Stochastic\n",
      "Gradient Descent (SGD) classifier\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 155:\n",
      "This is in part because SGD deals with training instances independently, one at a time\n",
      "(which also makes SGD well suited for online learning), as we will see later. Let’s create\n",
      "an SGDClassifier and train it on the whole training set:\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "sgd_clf = SGDClassifier(random_state=42)\n",
      "sgd_clf.fit(X_train, y_train_5)\n",
      "The SGDClassifier relies on randomness during training (hence\n",
      "the name “stochastic”). If you want reproducible results, you\n",
      "should set the random_state parameter. Now you can use it to detect images of the number 5:\n",
      ">>> sgd_clf.predict([some_digit])\n",
      "array([ True], dtype=bool)\n",
      "The classifier guesses that this image represents a 5 (True). Looks like it guessed right\n",
      "in this particular case! Now, let’s evaluate this model’s performance. Performance Measures\n",
      "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we\n",
      "will spend a large part of this chapter on this topic. There are many performance\n",
      "measures available, so grab another coffee and get ready to learn many new concepts\n",
      "and acronyms! 82 | Chapter 3: Classification\n",
      "Measuring Accuracy Using Cross-Validation\n",
      "A good way to evaluate a model is to use cross-validation, just as you did in Chap‐\n",
      "ter 2.\n",
      "\n",
      "Q: a 5 (True). Looks like it guessed right in this particular case!\n",
      "A: sgd_clf\n",
      "\n",
      "Q: sgd_clf = SGDClassifier(random_state=\n",
      "A: 42\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 156:\n",
      "Implementing Cross-Validation\n",
      "Occasionally you will need more control over the cross-validation process than what\n",
      "cross_val_score() and similar functions provide. In these cases, you can implement\n",
      "cross-validation yourself; it is actually fairly straightforward. The following code does\n",
      "roughly the same thing as the preceding cross_val_score() code, and prints the\n",
      "same result:\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.base import clone\n",
      "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
      "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
      "clone_clf = clone(sgd_clf)\n",
      "X_train_folds = X_train[train_index]\n",
      "y_train_folds = (y_train_5[train_index])\n",
      "X_test_fold = X_train[test_index]\n",
      "y_test_fold = (y_train_5[test_index])\n",
      "clone_clf.fit(X_train_folds, y_train_folds)\n",
      "y_pred = clone_clf.predict(X_test_fold)\n",
      "n_correct = sum(y_pred == y_test_fold)\n",
      "print(n_correct / len(y_pred)) # prints 0.9502, 0.96565 and 0.96495\n",
      "The StratifiedKFold class performs stratified sampling (as explained in Chapter 2)\n",
      "to produce folds that contain a representative ratio of each class. At each iteration the\n",
      "code creates a clone of the classifier, trains that clone on the training folds, and makes\n",
      "predictions on the test fold. Then it counts the number of correct predictions and\n",
      "outputs the ratio of correct predictions. Let’s use the cross_val_score() function to evaluate your SGDClassifier model\n",
      "using K-fold cross-validation, with three folds.\n",
      "\n",
      "Q: StratifiedKFold from sklearn.base import clone\n",
      "A: \n",
      "skfolds\n",
      "\n",
      "Q: : Implementing Cross-Validation Occasionally you will need more control over the cross\n",
      "A: \n",
      "cross_val_score()\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 157:\n",
      "Remember that K-fold cross-\n",
      "validation means splitting the training set into K-folds (in this case, three), then mak‐\n",
      "ing predictions and evaluating them on each fold using a model trained on the\n",
      "remaining folds (see Chapter 2):\n",
      ">>> from sklearn.model_selection import cross_val_score\n",
      ">>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
      "array([ 0.9502 , 0.96565, 0.96495])\n",
      "Performance Measures | 83\n",
      "Wow! Above 95% accuracy (ratio of correct predictions) on all cross-validation folds? This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\n",
      "dumb classifier that just classifies every single image in the “not-5” class:\n",
      "from sklearn.base import BaseEstimator\n",
      "class Never5Classifier(BaseEstimator):\n",
      "def fit(self, X, y=None):\n",
      "pass\n",
      "def predict(self, X):\n",
      "return np.zeros((len(X), 1), dtype=bool)\n",
      "Can you guess this model’s accuracy? Let’s find out:\n",
      ">>> never_5_clf = Never5Classifier()\n",
      ">>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
      "array([ 0.909 , 0.90715, 0.9128 ])\n",
      "That’s right, it has over 90% accuracy! This is simply because only about 10% of the\n",
      "images are 5s, so if you always guess that an image is not a 5, you will be right about\n",
      "90% of the time. Beats Nostradamus.\n",
      "\n",
      "Q: a sklearn.base import BaseEstimator class Never5Classifier\n",
      "A: over 90% accuracy\n",
      "\n",
      "Q: Fragen: Beats Nostradamus.\n",
      "A: over 90%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 158:\n",
      "This demonstrates why accuracy is generally not the preferred performance measure\n",
      "for classifiers, especially when you are dealing with skewed datasets (i.e., when some\n",
      "classes are much more frequent than others). Confusion Matrix\n",
      "A much better way to evaluate the performance of a classifier is to look at the confu‐\n",
      "sion matrix. The general idea is to count the number of times instances of class A are\n",
      "classified as class B. For example, to know the number of times the classifier confused\n",
      "images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\n",
      "matrix. To compute the confusion matrix, you first need to have a set of predictions, so they\n",
      "can be compared to the actual targets. You could make predictions on the test set, but\n",
      "let’s keep it untouched for now (remember that you want to use the test set only at the\n",
      "very end of your project, once you have a classifier that you are ready to launch).\n",
      "\n",
      "Q: the classifier confused images of 5s with 3s. You would look in the 5th\n",
      "A: confusion\n",
      "matrix\n",
      "\n",
      "Q: a classifier. Confusion Matrix A much better way to evaluate the performance of\n",
      "A: confu‐\n",
      "sion matrix\n",
      "\n",
      "Q: sion Matrix A much better way to evaluate a classifier is to look at\n",
      "A: Confusion Matrix\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 159:\n",
      "Instead, you can use the cross_val_predict() function:\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
      "Just like the cross_val_score() function, cross_val_predict() performs K-fold\n",
      "cross-validation, but instead of returning the evaluation scores, it returns the predic‐\n",
      "tions made on each test fold. This means that you get a clean prediction for each\n",
      "instance in the training set (“clean” meaning that the prediction is made by a model\n",
      "that never saw the data during training). 84 | Chapter 3: Classification\n",
      "Now you are ready to get the confusion matrix using the confusion_matrix() func‐\n",
      "tion. Just pass it the target classes (y_train_5) and the predicted classes\n",
      "(y_train_pred):\n",
      ">>> from sklearn.metrics import confusion_matrix\n",
      ">>> confusion_matrix(y_train_5, y_train_pred)\n",
      "array([[53272, 1307],\n",
      "[ 1077, 4344]])\n",
      "Each row in a confusion matrix represents an actual class, while each column repre‐\n",
      "sents a predicted class. The first row of this matrix considers non-5 images (the nega‐\n",
      "tive class): 53,272 of them were correctly classified as non-5s (they are called true\n",
      "negatives), while the remaining 1,307 were wrongly classified as 5s (false positives).\n",
      "\n",
      "Q: y_train_pred = cross_val_predict(sgd_\n",
      "A: confusion_matrix\n",
      "\n",
      "Q: () performs K-fold cross-validation, but instead of returning the evaluation scores,\n",
      "A: cross_val_predict\n",
      "\n",
      "Q: : Classification Now you are ready to get the confusion matrix using the confusion_matrix\n",
      "A: Just pass it the target classes\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 160:\n",
      "The second row considers the images of 5s (the positive class): 1,077 were wrongly\n",
      "classified as non-5s (false negatives), while the remaining 4,344 were correctly classi‐\n",
      "fied as 5s (true positives). A perfect classifier would have only true positives and true\n",
      "negatives, so its confusion matrix would have nonzero values only on its main diago‐\n",
      "nal (top left to bottom right):\n",
      ">>> confusion_matrix(y_train_5, y_train_perfect_predictions)\n",
      "array([[54579, 0],\n",
      "[ 0, 5421]])\n",
      "The confusion matrix gives you a lot of information, but sometimes you may prefer a\n",
      "more concise metric. An interesting one to look at is the accuracy of the positive pre‐\n",
      "dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision\n",
      "TP\n",
      "precision=\n",
      "TP+FP\n",
      "TP is the number of true positives, and FP is the number of false positives. A trivial way to have perfect precision is to make one single positive prediction and\n",
      "ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the\n",
      "classifier would ignore all but one positive instance.\n",
      "\n",
      "Q: TP is the number of true positives, and FP is the number of false positive\n",
      "A: \n",
      "TP+FP\n",
      "\n",
      "Q: (false negatives): 1,077 wrongly classified as non-5s (\n",
      "A: \n",
      "TP+FP\n",
      "\n",
      "Q: questions: A perfect classifier would have only true positives and true negatives, so its\n",
      "A: confusion matrix\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 161:\n",
      "So precision is typically used\n",
      "along with another metric named recall, also called sensitivity or true positive rate\n",
      "(TPR): this is the ratio of positive instances that are correctly detected by the classifier\n",
      "(Equation 3-2). Equation 3-2. Recall\n",
      "TP\n",
      "recall=\n",
      "TP+FN\n",
      "FN is of course the number of false negatives. Performance Measures | 85\n",
      "If you are confused about the confusion matrix, Figure 3-2 may help. Figure 3-2. An illustrated confusion matrix\n",
      "Precision and Recall\n",
      "Scikit-Learn provides several functions to compute classifier metrics, including preci‐\n",
      "sion and recall:\n",
      ">>> from sklearn.metrics import precision_score, recall_score\n",
      ">>> precision_score(y_train_5, y_pred) # == 4344 / (4344 + 1307)\n",
      "0.76871350203503808\n",
      ">>> recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077)\n",
      "0.79136690647482011\n",
      "Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only 77% of the time. Moreover,\n",
      "it only detects 79% of the 5s. It is often convenient to combine precision and recall into a single metric called the F\n",
      "1\n",
      "score, in particular if you need a simple way to compare two classifiers.\n",
      "\n",
      "Q: a 5 and it only detects 79% of the 5s. This is the ratio\n",
      "A: 5-detector\n",
      "\n",
      "Q: : this is the ratio of positive instances that are correctly detected by the classifier (Equ\n",
      "A: true positive rate\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 162:\n",
      "The F score is\n",
      "1\n",
      "the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean\n",
      "treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F score if both recall and precision are\n",
      "1\n",
      "high. Equation 3-3. F score\n",
      "1\n",
      "2 precision×recall TP\n",
      "F = =2× =\n",
      "1 1 1 precision+recall FN+FP\n",
      "+ TP+\n",
      "precision recall 2\n",
      "86 | Chapter 3: Classification\n",
      "To compute the F score, simply call the f1_score() function:\n",
      "1\n",
      ">>> from sklearn.metrics import f1_score\n",
      ">>> f1_score(y_train_5, y_pred)\n",
      "0.78468208092485547\n",
      "The F score favors classifiers that have similar precision and recall. This is not always\n",
      "1\n",
      "what you want: in some contexts you mostly care about precision, and in other con‐\n",
      "texts you really care about recall.\n",
      "\n",
      "Q: f1_score >>> f1_score(y_train_\n",
      "A: 0.78468208092485547\n",
      "\n",
      "Q: Fragen: F score is 1 the harmonic mean of precision and recall (Equation 3-3)\n",
      "A: 1\n",
      "\n",
      "Q: F score.\n",
      "A: \n",
      "1\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 163:\n",
      "For example, if you trained a classifier to detect vid‐\n",
      "eos that are safe for kids, you would probably prefer a classifier that rejects many\n",
      "good videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\n",
      "sifier that has a much higher recall but lets a few really bad videos show up in your\n",
      "product (in such cases, you may even want to add a human pipeline to check the clas‐\n",
      "sifier’s video selection). On the other hand, suppose you train a classifier to detect\n",
      "shoplifters on surveillance images: it is probably fine if your classifier has only 30%\n",
      "precision as long as it has 99% recall (sure, the security guards will get a few false\n",
      "alerts, but almost all shoplifters will get caught). Unfortunately, you can’t have it both ways: increasing precision reduces recall, and\n",
      "vice versa. This is called the precision/recall tradeoff. Precision/Recall Tradeoff\n",
      "To understand this tradeoff, let’s look at how the SGDClassifier makes its classifica‐\n",
      "tion decisions.\n",
      "\n",
      "Q: vid eos that are safe for kids, you would probably prefer a\n",
      "A: classifier\n",
      "\n",
      "Q: : increasing precision reduces recall, and vice versa.\n",
      "A: precision/recall tradeoff\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 164:\n",
      "For each instance, it computes a score based on a decision function,\n",
      "and if that score is greater than a threshold, it assigns the instance to the positive\n",
      "class, or else it assigns it to the negative class. Figure 3-3 shows a few digits positioned\n",
      "from the lowest score on the left to the highest score on the right. Suppose the deci‐\n",
      "sion threshold is positioned at the central arrow (between the two 5s): you will find 4\n",
      "true positives (actual 5s) on the right of that threshold, and one false positive (actually\n",
      "a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\n",
      "actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\n",
      "raise the threshold (move it to the arrow on the right), the false positive (the 6)\n",
      "becomes a true negative, thereby increasing precision (up to 100% in this case), but\n",
      "one true positive becomes a false negative, decreasing recall down to 50%. Conversely,\n",
      "lowering the threshold increases recall and reduces precision. Performance Measures | 87\n",
      "Figure 3-3.\n",
      "\n",
      "Q: the deci sion threshold is positioned at the central arrow (between the two 5\n",
      "A: ‐\n",
      "\n",
      "Q: a score based on a decision function, and if that score is greater than\n",
      "A: a threshold\n",
      "\n",
      "Q: Suppose the deci sion threshold is positioned at the central arrow (between the two\n",
      "A: 5s\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 165:\n",
      "Decision threshold and precision/recall tradeoff\n",
      "Scikit-Learn does not let you set the threshold directly, but it does give you access to\n",
      "the decision scores that it uses to make predictions. Instead of calling the classifier’s\n",
      "predict() method, you can call its decision_function() method, which returns a\n",
      "score for each instance, and then make predictions based on those scores using any\n",
      "threshold you want:\n",
      ">>> y_scores = sgd_clf.decision_function([some_digit])\n",
      ">>> y_scores\n",
      "array([ 161855.74572176])\n",
      ">>> threshold = 0\n",
      ">>> y_some_digit_pred = (y_scores > threshold)\n",
      "array([ True], dtype=bool)\n",
      "The SGDClassifier uses a threshold equal to 0, so the previous code returns the same\n",
      "result as the predict() method (i.e., True). Let’s raise the threshold:\n",
      ">>> threshold = 200000\n",
      ">>> y_some_digit_pred = (y_scores > threshold)\n",
      ">>> y_some_digit_pred\n",
      "array([False], dtype=bool)\n",
      "This confirms that raising the threshold decreases recall. The image actually repre‐\n",
      "sents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\n",
      "threshold is increased to 200,000. So how can you decide which threshold to use?\n",
      "\n",
      "Q: 0 >>> threshold = 200000 >>> y_some_digit_pred\n",
      "A: any\n",
      "threshold\n",
      "\n",
      "Q: threshold, but it gives you access to the decision scores that it uses to make predictions using any\n",
      "A: any\n",
      "threshold\n",
      "\n",
      "Q: the classifier’s predict() method, and then make predictions based on those scores using\n",
      "A: any\n",
      "threshold\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 166:\n",
      "For this you will first need to get the\n",
      "scores of all instances in the training set using the cross_val_predict() function\n",
      "again, but this time specifying that you want it to return decision scores instead of\n",
      "predictions:\n",
      "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
      "method=\"decision_function\")\n",
      "Now with these scores you can compute precision and recall for all possible thresh‐\n",
      "olds using the precision_recall_curve() function:\n",
      "88 | Chapter 3: Classification\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
      "Finally, you can plot precision and recall as functions of the threshold value using\n",
      "Matplotlib (Figure 3-4):\n",
      "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
      "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
      "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
      "plt.xlabel(\"Threshold\")\n",
      "plt.legend(loc=\"upper left\")\n",
      "plt.ylim([0, 1])\n",
      "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
      "plt.show()\n",
      "Figure 3-4. Precision and recall versus the decision threshold\n",
      "You may wonder why the precision curve is bumpier than the recall\n",
      "curve in Figure 3-4. The reason is that precision may sometimes go\n",
      "down when you raise the threshold (although in general it will go\n",
      "up). To understand why, look back at Figure 3-3 and notice what\n",
      "happens when you start from the central threshold and move it just\n",
      "one digit to the right: precision goes from 4/5 (80%) down to 3/4\n",
      "(75%).\n",
      "\n",
      "Q: , recalls, thresholds = precision_recall_curve(y_train_\n",
      "A: precision_recall_curve\n",
      "precisions\n",
      "\n",
      "Q: Precision and recall versus the decision threshold You may wonder why the precision curve is bumpier than the\n",
      "A: \n",
      "Matplotlib\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 167:\n",
      "On the other hand, recall can only go down when the thres‐\n",
      "hold is increased, which explains why its curve looks smooth. Now you can simply select the threshold value that gives you the best precision/recall\n",
      "tradeoff for your task. Another way to select a good precision/recall tradeoff is to plot\n",
      "precision directly against recall, as shown in Figure 3-5. Performance Measures | 89\n",
      "Figure 3-5. Precision versus recall\n",
      "You can see that precision really starts to fall sharply around 80% recall. You will\n",
      "probably want to select a precision/recall tradeoff just before that drop—for example,\n",
      "at around 60% recall. But of course the choice depends on your project. So let’s suppose you decide to aim for 90% precision. You look up the first plot\n",
      "(zooming in a bit) and find that you need to use a threshold of about 70,000. To make\n",
      "predictions (on the training set for now), instead of calling the classifier’s predict()\n",
      "method, you can just run this code:\n",
      "y_train_pred_90 = (y_scores > 70000)\n",
      "Let’s check these predictions’ precision and recall:\n",
      ">>> precision_score(y_train_5, y_train_pred_90)\n",
      "0.8998702983138781\n",
      ">>> recall_score(y_train_5, y_train_pred_90)\n",
      "0.63991883416343853\n",
      "Great, you have a 90% precision classifier (or close enough)!\n",
      "\n",
      "Q: a threshold of about 70,000. You can also plot precision directly against recall . You\n",
      "A: \n",
      "0.8998702983138781\n",
      "\n",
      "\n",
      "Q: the best the best. The question is, recall can only go down when the\n",
      "A: thres‐\n",
      "hold is increased\n",
      "\n",
      "Q: Questions: Now you can simply select the threshold value that gives you the best precision/recall trade\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 168:\n",
      "As you can see, it is\n",
      "fairly easy to create a classifier with virtually any precision you want: just set a high\n",
      "enough threshold, and you’re done. Hmm, not so fast. A high-precision classifier is\n",
      "not very useful if its recall is too low! If someone says “let’s reach 99% precision,” you should ask, “at\n",
      "what recall?”\n",
      "90 | Chapter 3: Classification\n",
      "The ROC Curve\n",
      "The receiver operating characteristic (ROC) curve is another common tool used with\n",
      "binary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\n",
      "ting precision versus recall, the ROC curve plots the true positive rate (another name\n",
      "for recall) against the false positive rate. The FPR is the ratio of negative instances that\n",
      "are incorrectly classified as positive. It is equal to one minus the true negative rate,\n",
      "which is the ratio of negative instances that are correctly classified as negative. The\n",
      "TNR is also called specificity. Hence the ROC curve plots sensitivity (recall) versus\n",
      "1 – specificity.\n",
      "\n",
      "Q: a high-precision classifier is not very useful if its recall is too low\n",
      "A: 99% precision\n",
      "\n",
      "Q: questions: As you can see, it is fairly easy to create a classifier with virtually\n",
      "A: any precision you want\n",
      "\n",
      "Q: The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers.\n",
      "A: precision/recall curve\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 169:\n",
      "To plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\n",
      "hold values, using the roc_curve() function:\n",
      "from sklearn.metrics import roc_curve\n",
      "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
      "Then you can plot the FPR against the TPR using Matplotlib. This code produces the\n",
      "plot in Figure 3-6:\n",
      "def plot_roc_curve(fpr, tpr, label=None):\n",
      "plt.plot(fpr, tpr, linewidth=2, label=label)\n",
      "plt.plot([0, 1], [0, 1], 'k--')\n",
      "plt.axis([0, 1, 0, 1])\n",
      "plt.xlabel('False Positive Rate')\n",
      "plt.ylabel('True Positive Rate')\n",
      "plot_roc_curve(fpr, tpr)\n",
      "plt.show()\n",
      "Figure 3-6. ROC curve\n",
      "Performance Measures | 91\n",
      "Once again there is a tradeoff: the higher the recall (TPR), the more false positives\n",
      "(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\n",
      "random classifier; a good classifier stays as far away from that line as possible (toward\n",
      "the top-left corner). One way to compare classifiers is to measure the area under the curve (AUC). A per‐\n",
      "fect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will\n",
      "have a ROC AUC equal to 0.5.\n",
      "\n",
      "Q: ROC curve Performance Measures | 91 Once again there is a tradeoff: the\n",
      "A: more false positives\n",
      "\n",
      "\n",
      "Q: ROC curve, you first need to compute the TPR and FPR for various th\n",
      "A: \n",
      "hold values\n",
      "\n",
      "Q: : This code produces the plot in Figure 3-6: def plot_roc_curve\n",
      "A: Matplotlib\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 170:\n",
      "Scikit-Learn provides a function to compute the ROC\n",
      "AUC:\n",
      ">>> from sklearn.metrics import roc_auc_score\n",
      ">>> roc_auc_score(y_train_5, y_scores)\n",
      "0.97061072797174941\n",
      "Since the ROC curve is so similar to the precision/recall (or PR)\n",
      "curve, you may wonder how to decide which one to use. As a rule\n",
      "of thumb, you should prefer the PR curve whenever the positive\n",
      "class is rare or when you care more about the false positives than\n",
      "the false negatives, and the ROC curve otherwise. For example,\n",
      "looking at the previous ROC curve (and the ROC AUC score), you\n",
      "may think that the classifier is really good. But this is mostly\n",
      "because there are few positives (5s) compared to the negatives\n",
      "(non-5s). In contrast, the PR curve makes it clear that the classifier\n",
      "has room for improvement (the curve could be closer to the top-\n",
      "right corner). Let’s train a RandomForestClassifier and compare its ROC curve and ROC AUC\n",
      "score to the SGDClassifier. First, you need to get scores for each instance in the\n",
      "training set. But due to the way it works (see Chapter 7), the RandomForestClassi\n",
      "fier class does not have a decision_function() method. Instead it has a pre\n",
      "dict_proba() method.\n",
      "\n",
      "Q: ROC AUC score. Here are some examples of the ROC curve that generate questions.\n",
      "A: 0.97061072797174941\n",
      "\n",
      "Q: roc_auc_score >>> roc_auc_score >>\n",
      "A: 0.97061072797174941\n",
      "\n",
      "Q: the PR curve whenever the positive class is rare or when you care more about the false positives\n",
      "A: roc_auc_score\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 171:\n",
      "Scikit-Learn classifiers generally have one or the other. The\n",
      "predict_proba() method returns an array containing a row per instance and a col‐\n",
      "umn per class, each containing the probability that the given instance belongs to the\n",
      "given class (e.g., 70% chance that the image represents a 5):\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "forest_clf = RandomForestClassifier(random_state=42)\n",
      "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
      "method=\"predict_proba\")\n",
      "But to plot a ROC curve, you need scores, not probabilities. A simple solution is to\n",
      "use the positive class’s probability as the score:\n",
      "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
      "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\n",
      "92 | Chapter 3: Classification\n",
      "Now you are ready to plot the ROC curve. It is useful to plot the first ROC curve as\n",
      "well to see how they compare (Figure 3-7):\n",
      "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
      "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
      "plt.legend(loc=\"bottom right\")\n",
      "plt.show()\n",
      "Figure 3-7. Comparing ROC curves\n",
      "As you can see in Figure 3-7, the RandomForestClassifier’s ROC curve looks much\n",
      "better than the SGDClassifier’s: it comes much closer to the top-left corner.\n",
      "\n",
      "Q: a ROC curve. a ROC curve. a ROC curve\n",
      "A: \n",
      "plt.show()\n",
      "\n",
      "Q: , a 5):, not probabilities, that the given instance belongs to the\n",
      "A: scores\n",
      "\n",
      "Q: , you need scores, not probabilities. a ROC curve, you need scores\n",
      "A: \n",
      "plt.show()\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 172:\n",
      "As a\n",
      "result, its ROC AUC score is also significantly better:\n",
      ">>> roc_auc_score(y_train_5, y_scores_forest)\n",
      "0.99312433660038291\n",
      "Try measuring the precision and recall scores: you should find 98.5% precision and\n",
      "82.8% recall. Not too bad! Hopefully you now know how to train binary classifiers, choose the appropriate met‐\n",
      "ric for your task, evaluate your classifiers using cross-validation, select the precision/\n",
      "recall tradeoff that fits your needs, and compare various models using ROC curves\n",
      "and ROC AUC scores. Now let’s try to detect more than just the 5s. Multiclass Classification\n",
      "Whereas binary classifiers distinguish between two classes, multiclass classifiers (also\n",
      "called multinomial classifiers) can distinguish between more than two classes. Multiclass Classification | 93\n",
      "Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\n",
      "capable of handling multiple classes directly. Others (such as Support Vector Machine\n",
      "classifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\n",
      "ous strategies that you can use to perform multiclass classification using multiple\n",
      "binary classifiers.\n",
      "\n",
      "Q: ROC AUC scores 98.5% precision and recall scores. Now let’s\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ROC AUC score is significantly better: >>> roc_auc_score\n",
      "A: 0.99312433660038291\n",
      "\n",
      "Q: rics, how to train binary classifiers, choose the appropriate met\n",
      "A: \n",
      "ric for your task\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 173:\n",
      "For example, one way to create a system that can classify the digit images into 10\n",
      "classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\n",
      "1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\n",
      "the decision score from each classifier for that image and you select the class whose\n",
      "classifier outputs the highest score. This is called the one-versus-all (OvA) strategy\n",
      "(also called one-versus-the-rest). Another strategy is to train a binary classifier for every pair of digits: one to distin‐\n",
      "guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to\n",
      "train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\n",
      "binary classifiers! When you want to classify an image, you have to run the image\n",
      "through all 45 classifiers and see which class wins the most duels.\n",
      "\n",
      "Q: 0 to 9) is to train 10 binary classifiers, one for each digit\n",
      "A: 10\n",
      "classes\n",
      "\n",
      "Q: ector, a 1-detector, a 2-detector,\n",
      "A: 0-detector\n",
      "\n",
      "Q: the classifier. Then you select the class whose classifier outputs the highest score\n",
      "A: one-versus-all\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 174:\n",
      "The main advan‐\n",
      "tage of OvO is that each classifier only needs to be trained on the part of the training\n",
      "set for the two classes that it must distinguish. Some algorithms (such as Support Vector Machine classifiers) scale poorly with the\n",
      "size of the training set, so for these algorithms OvO is preferred since it is faster to\n",
      "train many classifiers on small training sets than training few classifiers on large\n",
      "training sets. For most binary classification algorithms, however, OvA is preferred. Scikit-Learn detects when you try to use a binary classification algorithm for a multi‐\n",
      "class classification task, and it automatically runs OvA (except for SVM classifiers for\n",
      "which it uses OvO). Let’s try this with the SGDClassifier:\n",
      ">>> sgd_clf.fit(X_train, y_train) # y_train, not y_train_5\n",
      ">>> sgd_clf.predict([some_digit])\n",
      "array([ 5.]) That was easy! This code trains the SGDClassifier on the training set using the origi‐\n",
      "nal target classes from 0 to 9 (y_train), instead of the 5-versus-all target classes\n",
      "(y_train_5). Then it makes a prediction (a correct one in this case). Under the hood,\n",
      "Scikit-Learn actually trained 10 binary classifiers, got their decision scores for the\n",
      "image, and selected the class with the highest score.\n",
      "\n",
      "Q: a binary classification algorithm, but it is preferred. Scikit-Learn trains 10\n",
      "A: OvA\n",
      "\n",
      "Q: tage of OvO is that each classifier only needs to be trained on part of\n",
      "A: the training\n",
      "set\n",
      "\n",
      "Q: (such as Support Vector Machine classifiers) scale poorly with the size of the training set\n",
      "A: algorithms\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 175:\n",
      "To see that this is indeed the case, you can call the decision_function() method. Instead of returning just one score per instance, it now returns 10 scores, one per\n",
      "class:\n",
      ">>> some_digit_scores = sgd_clf.decision_function([some_digit])\n",
      ">>> some_digit_scores\n",
      "94 | Chapter 3: Classification\n",
      "array([[-311402.62954431, -363517.28355739, -446449.5306454 ,\n",
      "-183226.61023518, -414337.15339485, 161855.74572176,\n",
      "-452576.39616343, -471957.14962573, -518542.33997148,\n",
      "-536774.63961222]])\n",
      "The highest score is indeed the one corresponding to class 5:\n",
      ">>> np.argmax(some_digit_scores)\n",
      "5\n",
      ">>> sgd_clf.classes_\n",
      "array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) >>> sgd_clf.classes[5]\n",
      "5.0\n",
      "When a classifier is trained, it stores the list of target classes in its\n",
      "classes_ attribute, ordered by value. In this case, the index of each\n",
      "class in the classes_ array conveniently matches the class itself\n",
      "(e.g., the class at index 5 happens to be class 5), but in general you\n",
      "won’t be so lucky. If you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use\n",
      "the OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance\n",
      "and pass a binary classifier to its constructor.\n",
      "\n",
      "Q: , you can call the decision_function() method. Instead of returning just one score per instance\n",
      "A: OneVsOne\n",
      "\n",
      "Q: () method returns 10 scores, one per class: >>> some_digit_scores\n",
      "A: decision_function()\n",
      "\n",
      "Q: , it returns 10 scores, one per class: >>> some_digit_scores\n",
      "A: OneVsOneClassifier\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 176:\n",
      "For example, this code creates a multi‐\n",
      "class classifier using the OvO strategy, based on a SGDClassifier:\n",
      ">>> from sklearn.multiclass import OneVsOneClassifier\n",
      ">>> ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\n",
      ">>> ovo_clf.fit(X_train, y_train)\n",
      ">>> ovo_clf.predict([some_digit])\n",
      "array([ 5.]) >>> len(ovo_clf.estimators_)\n",
      "45\n",
      "Training a RandomForestClassifier is just as easy:\n",
      ">>> forest_clf.fit(X_train, y_train)\n",
      ">>> forest_clf.predict([some_digit])\n",
      "array([ 5.]) This time Scikit-Learn did not have to run OvA or OvO because Random Forest\n",
      "classifiers can directly classify instances into multiple classes. You can call\n",
      "predict_proba() to get the list of probabilities that the classifier assigned to each\n",
      "instance for each class:\n",
      ">>> forest_clf.predict_proba([some_digit])\n",
      "array([[ 0.1, 0. , 0. , 0.1, 0. , 0.8, 0. , 0. , 0. , 0. ]]) You can see that the classifier is fairly confident about its prediction: the 0.8 at the 5th\n",
      "index in the array means that the model estimates an 80% probability that the image\n",
      "Multiclass Classification | 95\n",
      "represents a 5. It also thinks that the image could instead be a 0 or a 3 (10% chance\n",
      "each). Now of course you want to evaluate these classifiers. As usual, you want to use cross-\n",
      "validation.\n",
      "\n",
      "Q: a multi classifier using the OvO strategy. You can call predict_pro\n",
      "A: \n",
      "predict_proba()\n",
      "\n",
      "Q: creates a multi classifier using the OvO strategy, based on\n",
      "A: SGDClassifier\n",
      "\n",
      "Q: questions: You can call predict_proba() to get list of probabilities that the class\n",
      "A: each\n",
      "instance for each class\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 177:\n",
      "Let’s evaluate the SGDClassifier’s accuracy using the cross_val_score()\n",
      "function:\n",
      ">>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
      "array([ 0.84063187, 0.84899245, 0.86652998])\n",
      "It gets over 84% on all test folds. If you used a random classifier, you would get 10%\n",
      "accuracy, so this is not such a bad score, but you can still do much better. For exam‐\n",
      "ple, simply scaling the inputs (as discussed in Chapter 2) increases accuracy above\n",
      "90%:\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      ">>> scaler = StandardScaler()\n",
      ">>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
      ">>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n",
      "array([ 0.91011798, 0.90874544, 0.906636 ])\n",
      "Error Analysis\n",
      "Of course, if this were a real project, you would follow the steps in your Machine\n",
      "Learning project checklist (see Appendix B): exploring data preparation options, try‐\n",
      "ing out multiple models, shortlisting the best ones and fine-tuning their hyperpara‐\n",
      "meters using GridSearchCV, and automating as much as possible, as you did in the\n",
      "previous chapter. Here, we will assume that you have found a promising model and\n",
      "you want to find ways to improve it. One way to do this is to analyze the types of\n",
      "errors it makes. First, you can look at the confusion matrix.\n",
      "\n",
      "Q: , y_train, cv=3, scoring=\"accuracy\") array(\n",
      "A: cross_val_score\n",
      "\n",
      "Q: =3 a random classifier. It gets over 84% accuracy on all test fold\n",
      "A: cross_val_score()\n",
      "\n",
      "Q: ,, you would get 10% accuracy, so this is not a bad score, but\n",
      "A: If you used a random classifier\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 178:\n",
      "You need to make predictions using the\n",
      "cross_val_predict() function, then call the confusion_matrix() function, just like\n",
      "you did earlier:\n",
      ">>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
      ">>> conf_mx = confusion_matrix(y_train, y_train_pred)\n",
      ">>> conf_mx\n",
      "array([[5725, 3, 24, 9, 10, 49, 50, 10, 39, 4],\n",
      "[ 2, 6493, 43, 25, 7, 40, 5, 10, 109, 8],\n",
      "[ 51, 41, 5321, 104, 89, 26, 87, 60, 166, 13],\n",
      "[ 47, 46, 141, 5342, 1, 231, 40, 50, 141, 92],\n",
      "[ 19, 29, 41, 10, 5366, 9, 56, 37, 86, 189],\n",
      "[ 73, 45, 36, 193, 64, 4582, 111, 30, 193, 94],\n",
      "[ 29, 34, 44, 2, 42, 85, 5627, 10, 45, 0],\n",
      "[ 25, 24, 74, 32, 54, 12, 6, 5787, 15, 236],\n",
      "[ 52, 161, 73, 156, 10, 163, 61, 25, 5027, 123],\n",
      "[ 43, 35, 26, 92, 178, 28, 2, 223, 82, 5240]])\n",
      "96 | Chapter 3: Classification\n",
      "That’s a lot of numbers. It’s often more convenient to look at an image representation\n",
      "of the confusion matrix, using Matplotlib’s matshow() function:\n",
      "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
      "plt.show()\n",
      "This confusion matrix looks fairly good, since most images are on the main diagonal,\n",
      "which means that they were classified correctly.\n",
      "\n",
      "Q: ,, and,, 96 | Chapter 3: Classification That’\n",
      "A: a lot of numbers\n",
      "\n",
      "Q: ,, then call the confusion_matrix() function, just like you did earlier\n",
      "A: \n",
      "cross_val_predict() function\n",
      "\n",
      "Q: looks fairly good, since most images are on the main diagonal, which means that they were classified\n",
      "A: confusion matrix\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 179:\n",
      "The 5s look slightly darker than the\n",
      "other digits, which could mean that there are fewer images of 5s in the dataset or that\n",
      "the classifier does not perform as well on 5s as on other digits. In fact, you can verify\n",
      "that both are the case. Let’s focus the plot on the errors. First, you need to divide each value in the confusion\n",
      "matrix by the number of images in the corresponding class, so you can compare error\n",
      "rates instead of absolute number of errors (which would make abundant classes look\n",
      "unfairly bad):\n",
      "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
      "norm_conf_mx = conf_mx / row_sums\n",
      "Now let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:\n",
      "np.fill_diagonal(norm_conf_mx, 0)\n",
      "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
      "plt.show()\n",
      "Error Analysis | 97\n",
      "Now you can clearly see the kinds of errors the classifier makes. Remember that rows\n",
      "represent actual classes, while columns represent predicted classes. The columns for\n",
      "classes 8 and 9 are quite bright, which tells you that many images get misclassified as\n",
      "8s or 9s. Similarly, the rows for classes 8 and 9 are also quite bright, telling you that 8s\n",
      "and 9s are often confused with other digits.\n",
      "\n",
      "Q: , and let’s focus the plot on the errors. First, you need to divide each\n",
      "A: confusion\n",
      "matrix\n",
      "\n",
      "Q: s. The 5s look slightly darker than the other digits, which could mean that\n",
      "A: there are fewer images of 5s in the dataset\n",
      "\n",
      "Q: questions: In fact, you can verify that both are case Let’s focus the plot on\n",
      "A: errors\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 180:\n",
      "Conversely, some rows are pretty dark,\n",
      "such as row 1: this means that most 1s are classified correctly (a few are confused\n",
      "with 8s, but that’s about it). Notice that the errors are not perfectly symmetrical; for\n",
      "example, there are more 5s misclassified as 8s than the reverse. Analyzing the confusion matrix can often give you insights on ways to improve your\n",
      "classifier. Looking at this plot, it seems that your efforts should be spent on improving\n",
      "classification of 8s and 9s, as well as fixing the specific 3/5 confusion. For example,\n",
      "you could try to gather more training data for these digits. Or you could engineer\n",
      "new features that would help the classifier—for example, writing an algorithm to\n",
      "count the number of closed loops (e.g., 8 has two, 6 has one, 5 has none). Or you\n",
      "could preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make\n",
      "some patterns stand out more, such as closed loops. Analyzing individual errors can also be a good way to gain insights on what your\n",
      "classifier is doing and why it is failing, but it is more difficult and time-consuming.\n",
      "\n",
      "Q: the confusion matrix can often give you insights on ways to improve your classifier . Analy\n",
      "A: Analyzing\n",
      "\n",
      "Q: : this means that most 1s are classified correctly (a few are confused with 8s,\n",
      "A: row 1\n",
      "\n",
      "Q: the confusion matrix.\n",
      "A: \n",
      "classifier\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 181:\n",
      "For example, let’s plot examples of 3s and 5s:\n",
      "cl_a, cl_b = 3, 5\n",
      "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
      "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
      "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
      "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
      "plt.figure(figsize=(8,8))\n",
      "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
      "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
      "98 | Chapter 3: Classification\n",
      "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
      "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
      "plt.show()\n",
      "The two 5×5 blocks on the left show digits classified as 3s, and the two 5×5 blocks on\n",
      "the right show images classified as 5s. Some of the digits that the classifier gets wrong\n",
      "(i.e., in the bottom-left and top-right blocks) are so badly written that even a human\n",
      "would have trouble classifying them (e.g., the 5 on the 8th row and 1st column truly\n",
      "looks like a 3). However, most misclassified images seem like obvious errors to us,\n",
      "and it’s hard to understand why the classifier made the mistakes it did.3 The reason is\n",
      "that we used a simple SGDClassifier, which is a linear model.\n",
      "\n",
      "Q: digits classified as 3s, and the two 55 blocks on the right show\n",
      "A: human\n",
      "\n",
      "Q: digits classified as 3s, and the two 55 blocks on the left show\n",
      "A: human\n",
      "\n",
      "Q: classifier made the mistakes it did.3 The reason is that we used a simple S\n",
      "A: a human\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 182:\n",
      "All it does is assign a\n",
      "weight per class to each pixel, and when it sees a new image it just sums up the weigh‐\n",
      "ted pixel intensities to get a score for each class. So since 3s and 5s differ only by a few\n",
      "pixels, this model will easily confuse them. The main difference between 3s and 5s is the position of the small line that joins the\n",
      "top line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\n",
      "the classifier might classify it as a 5, and vice versa. In other words, this classifier is\n",
      "quite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\n",
      "would be to preprocess the images to ensure that they are well centered and not too\n",
      "rotated. This will probably help reduce other errors as well. 3 But remember that our brain is a fantastic pattern recognition system, and our visual system does a lot of\n",
      "complex preprocessing before any information reaches our consciousness, so the fact that it feels simple does\n",
      "not mean that it is.\n",
      "\n",
      "Q: pixel intensity to get a score for each class. 3 But remember that our brain is\n",
      "A: a fantastic pattern recognition system\n",
      "\n",
      "Q: pixel intensities to get a score for each class. This model will easily confuse them\n",
      "A: 3s and 5s\n",
      "\n",
      "Q: is the position of the small line that joins the top line to the bottom arc.\n",
      "A: 3s and 5s\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 183:\n",
      "Error Analysis | 99\n",
      "Multilabel Classification\n",
      "Until now each instance has always been assigned to just one class. In some cases you\n",
      "may want your classifier to output multiple classes for each instance. For example,\n",
      "consider a face-recognition classifier: what should it do if it recognizes several people\n",
      "on the same picture? Of course it should attach one label per person it recognizes. Say\n",
      "the classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\n",
      "when it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\n",
      "“Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\n",
      "binary labels is called a multilabel classification system. We won’t go into face recognition just yet, but let’s look at a simpler example, just for\n",
      "illustration purposes:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "y_train_large = (y_train >= 7)\n",
      "y_train_odd = (y_train % 2 == 1)\n",
      "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
      "knn_clf = KNeighborsClassifier()\n",
      "knn_clf.fit(X_train, y_multilabel)\n",
      "This code creates a y_multilabel array containing two target labels for each digit\n",
      "image: the first indicates whether or not the digit is large (7, 8, or 9) and the second\n",
      "indicates whether or not it is odd.\n",
      "\n",
      "Q: y_multilabel array containing two target labels for each digit image .\n",
      "A: the first indicates whether or not the digit is large\n",
      "\n",
      "Q: Fragen: Error Analysis | 99 Multilabel Classification Jusqu'à maintenant chaque instance\n",
      "A: attach one label per person it recognizes\n",
      "\n",
      "Q: multiple people on the same picture?\n",
      "A: attach one label per person it recognizes\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 184:\n",
      "The next lines create a KNeighborsClassifier\n",
      "instance (which supports multilabel classification, but not all classifiers do) and we\n",
      "train it using the multiple targets array. Now you can make a prediction, and notice\n",
      "that it outputs two labels:\n",
      ">>> knn_clf.predict([some_digit])\n",
      "array([[False, True]], dtype=bool)\n",
      "And it gets it right! The digit 5 is indeed not large (False) and odd (True). There are many ways to evaluate a multilabel classifier, and selecting the right metric\n",
      "really depends on your project. For example, one approach is to measure the F score\n",
      "1\n",
      "for each individual label (or any other binary classifier metric discussed earlier), then\n",
      "simply compute the average score. This code computes the average F score across all\n",
      "1\n",
      "labels:\n",
      ">>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)\n",
      ">>> f1_score(y_train, y_train_knn_pred, average=\"macro\")\n",
      "0.96845540180280221\n",
      "This assumes that all labels are equally important, which may not be the case. In par‐\n",
      "ticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\n",
      "to give more weight to the classifier’s score on pictures of Alice.\n",
      "\n",
      "Q: the classifier’s score. The next lines create a KNeighbors\n",
      "A: \n",
      "0.96845540180280221\n",
      "\n",
      "Q: questions: The next lines create a KNeighborsClassifier instance (which\n",
      "A: supports multilabel classification\n",
      "\n",
      "Q: questions: Now you can make a prediction, and notice that it outputs two labels:\n",
      "A: KNeighborsClassifier\n",
      "instance\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 185:\n",
      "One simple option is\n",
      "100 | Chapter 3: Classification\n",
      "to give each label a weight equal to its support (i.e., the number of instances with that\n",
      "target label). To do this, simply set average=\"weighted\" in the preceding code.4\n",
      "Multioutput Classification\n",
      "The last type of classification task we are going to discuss here is called multioutput-\n",
      "multiclass classification (or simply multioutput classification). It is simply a generaliza‐\n",
      "tion of multilabel classification where each label can be multiclass (i.e., it can have\n",
      "more than two possible values). To illustrate this, let’s build a system that removes noise from images. It will take as\n",
      "input a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\n",
      "sented as an array of pixel intensities, just like the MNIST images. Notice that the\n",
      "classifier’s output is multilabel (one label per pixel) and each label can have multiple\n",
      "values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\n",
      "classification system. The line between classification and regression is sometimes blurry,\n",
      "such as in this example. Arguably, predicting pixel intensity is more\n",
      "akin to regression than to classification.\n",
      "\n",
      "Q: multilabel classification. The line between classification and regression is sometimes blurry . predicting\n",
      "A: pixel intensity\n",
      "\n",
      "Q: Multioutput Classification The last type of classification task we are going to discuss here is called multi\n",
      "A: multiclass classification\n",
      "\n",
      "Q: tion of multilabel classification where each label can be multiclass (i.e.\n",
      "A: it can have\n",
      "more than two possible values\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 186:\n",
      "Moreover, multioutput\n",
      "systems are not limited to classification tasks; you could even have\n",
      "a system that outputs multiple labels per instance, including both\n",
      "class labels and value labels. Let’s start by creating the training and test sets by taking the MNIST images and\n",
      "adding noise to their pixel intensities using NumPy’s randint() function. The target\n",
      "images will be the original images:\n",
      "noise = rnd.randint(0, 100, (len(X_train), 784))\n",
      "noise = rnd.randint(0, 100, (len(X_test), 784))\n",
      "X_train_mod = X_train + noise\n",
      "X_test_mod = X_test + noise\n",
      "y_train_mod = X_train\n",
      "y_test_mod = X_test\n",
      "Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\n",
      "you should be frowning right now):\n",
      "4 Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\n",
      "more details. Multioutput Classification | 101\n",
      "On the left is the noisy input image, and on the right is the clean target image. Now\n",
      "let’s train the classifier and make it clean this image:\n",
      "knn_clf.fit(X_train_mod, y_train_mod)\n",
      "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
      "plot_digit(clean_digit)\n",
      "Looks close enough to the target! This concludes our tour of classification.\n",
      "\n",
      "Q: , so you should be frowning right now!:: knn\n",
      "A: snooping on the test data\n",
      "\n",
      "Q: questions:,,,,, multioutput systems are not limited\n",
      "A: classification tasks\n",
      "\n",
      "Q: questions: Let’s start by creating the training and test sets by taking the MNIST\n",
      "A: \n",
      "plot\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 187:\n",
      "Hopefully\n",
      "you should now know how to select good metrics for classification tasks, pick the\n",
      "appropriate precision/recall tradeoff, compare classifiers, and more generally build\n",
      "good classification systems for a variety of tasks. Exercises\n",
      "1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\n",
      "on the test set. Hint: the KNeighborsClassifier works quite well for this task;\n",
      "you just need to find good hyperparameter values (try a grid search on the\n",
      "weights and n_neighbors hyperparameters). 2. Write a function that can shift an MNIST image in any direction (left, right, up,\n",
      "or down) by one pixel.5 Then, for each image in the training set, create four shif‐\n",
      "ted copies (one per direction) and add them to the training set. Finally, train your\n",
      "best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of\n",
      "5 You can use the shift() function from the scipy.ndimage.interpolation module. For example,\n",
      "shift(image, [2, 1], cval=0) shifts the image 2 pixels down and 1 pixel to the right.\n",
      "\n",
      "Q: the MNIST dataset achieves 97% accuracy on the test set . Exercises 1.\n",
      "A: Try to build a classifier\n",
      "\n",
      "Q: the correct precision/recall tradeoff, compare classifiers, and build good classification systems\n",
      "A: \n",
      "appropriate\n",
      "\n",
      "Q: Questions: Exercises 1 Try to build a classifier for the MNIST dataset that\n",
      "A: over 97% accuracy\n",
      "on the test set\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 188:\n",
      "102 | Chapter 3: Classification\n",
      "artificially growing the training set is called data augmentation or training set\n",
      "expansion. 3. Tackle the Titanic dataset. A great place to start is on Kaggle. 4. Build a spam classifier (a more challenging exercise):\n",
      "• Download examples of spam and ham from Apache SpamAssassin’s public\n",
      "datasets. • Unzip the datasets and familiarize yourself with the data format. • Split the datasets into a training set and a test set. • Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector\n",
      "indicating the presence or absence of each possible word. For example, if all\n",
      "emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email\n",
      "“Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1]\n",
      "(meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is\n",
      "present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\n",
      "each word.\n",
      "\n",
      "Q: questions: 102 | Chapter 3: Classification artificially growing the training set is called data\n",
      "A: data augmentation\n",
      "\n",
      "Q: : Classification artificially growing the training set is called data augmentation or training set expansion 3\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 189:\n",
      "• You may want to add hyperparameters to your preparation pipeline to control\n",
      "whether or not to strip off email headers, convert each email to lowercase,\n",
      "remove punctuation, replace all URLs with “URL,” replace all numbers with\n",
      "“NUMBER,” or even perform stemming (i.e., trim off word endings; there are\n",
      "Python libraries available to do this). • Then try out several classifiers and see if you can build a great spam classifier,\n",
      "with both high recall and high precision. Solutions to these exercises are available in the online Jupyter notebooks at https://\n",
      "github.com/ageron/handson-ml. Exercises | 103\n",
      "CHAPTER 4\n",
      "Training Models\n",
      "So far we have treated Machine Learning models and their training algorithms mostly\n",
      "like black boxes. If you went through some of the exercises in the previous chapters,\n",
      "you may have been surprised by how much you can get done without knowing any‐\n",
      "thing about what’s under the hood: you optimized a regression system, you improved\n",
      "a digit image classifier, and you even built a spam classifier from scratch—all this\n",
      "without knowing how they actually work. Indeed, in many situations you don’t really\n",
      "need to know the implementation details.\n",
      "\n",
      "Q: the exercises in the previous chapters. • You may want to add hyperparameters to your\n",
      "A: preparation pipeline\n",
      "\n",
      "Q: Try out several classifiers and see if you can build a great spam classifier\n",
      "A: without knowing how they actually work\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 190:\n",
      "However, having a good understanding of how things work can help you quickly\n",
      "home in on the appropriate model, the right training algorithm to use, and a good set\n",
      "of hyperparameters for your task. Understanding what’s under the hood will also help\n",
      "you debug issues and perform error analysis more efficiently. Lastly, most of the top‐\n",
      "ics discussed in this chapter will be essential in understanding, building, and training\n",
      "neural networks (discussed in Part II of this book). In this chapter, we will start by looking at the Linear Regression model, one of the\n",
      "simplest models there is. We will discuss two very different ways to train it:\n",
      "• Using a direct “closed-form” equation that directly computes the model parame‐\n",
      "ters that best fit the model to the training set (i.e., the model parameters that\n",
      "minimize the cost function over the training set). • Using an iterative optimization approach, called Gradient Descent (GD), that\n",
      "gradually tweaks the model parameters to minimize the cost function over the\n",
      "training set, eventually converging to the same set of parameters as the first\n",
      "method.\n",
      "\n",
      "Q: how things work can help you quickly home in on the appropriate model, the right training algorithm to\n",
      "A: having a good understanding\n",
      "\n",
      "Q: :: What’s under the hood?: Understanding what’s under the\n",
      "A: debug issues and perform error analysis more efficiently\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 191:\n",
      "We will look at a few variants of Gradient Descent that we will use again\n",
      "and again when we study neural networks in Part II: Batch GD, Mini-batch GD,\n",
      "and Stochastic GD. 105\n",
      "Next we will look at Polynomial Regression, a more complex model that can fit non‐\n",
      "linear datasets. Since this model has more parameters than Linear Regression, it is\n",
      "more prone to overfitting the training data, so we will look at how to detect whether\n",
      "or not this is the case, using learning curves, and then we will look at several regulari‐\n",
      "zation techniques that can reduce the risk of overfitting the training set. Finally, we will look at two more models that are commonly used for classification\n",
      "tasks: Logistic Regression and Softmax Regression. There will be quite a few math equations in this chapter, using basic\n",
      "notions of linear algebra and calculus. To understand these equa‐\n",
      "tions, you will need to know what vectors and matrices are, how to\n",
      "transpose them, what the dot product is, what matrix inverse is,\n",
      "and what partial derivatives are.\n",
      "\n",
      "Q: a few mathematical equations that are commonly used for classification tasks: Logistic Regression and\n",
      "A: Softmax Regression\n",
      "\n",
      "Q: GD, Mini-batch GD, and Stochastic GD 105 Next\n",
      "A: \n",
      "\n",
      "\n",
      "Q: 105 Next we will look at Polynomial Regression, a more complex model\n",
      "A: \n",
      "linear datasets\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 192:\n",
      "If you are unfamiliar with these\n",
      "concepts, please go through the linear algebra and calculus intro‐\n",
      "ductory tutorials available as Jupyter notebooks in the online sup‐\n",
      "plemental material. For those who are truly allergic to\n",
      "mathematics, you should still go through this chapter and simply\n",
      "skip the equations; hopefully, the text will be sufficient to help you\n",
      "understand most of the concepts. Linear Regression\n",
      "In Chapter 1, we looked at a simple regression model of life satisfaction: life_satisfac‐\n",
      "tion = θ + θ × GDP_per_capita. 0 1\n",
      "This model is just a linear function of the input feature GDP_per_capita. θ and θ are\n",
      "0 1\n",
      "the model’s parameters. More generally, a linear model makes a prediction by simply computing a weighted\n",
      "sum of the input features, plus a constant called the bias term (also called the intercept\n",
      "term), as shown in Equation 4-1. Equation 4-1. Linear Regression model prediction\n",
      "y =θ +θ x +θ x +⋯+θ x\n",
      "0 1 1 2 2 n n\n",
      "• ŷ is the predicted value. • n is the number of features. • x is the ith feature value.\n",
      "\n",
      "Q: a question: If you are truly allergic to mathematics, please go through the linear algebra and\n",
      "A: \n",
      "skip the equations\n",
      "\n",
      "Q: plemental material: the concepts you need to understand.: If you are\n",
      "A: unfamiliar\n",
      "\n",
      "Q: tion =  +     GDP_per_cap\n",
      "A: θ + θ\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 193:\n",
      "i\n",
      "• θ is the jth model parameter (including the bias term θ and the feature weights\n",
      "j 0\n",
      "θ , θ , ⋯, θ ). 1 2 n\n",
      "106 | Chapter 4: Training Models\n",
      "This can be written much more concisely using a vectorized form, as shown in Equa‐\n",
      "tion 4-2. Equation 4-2. Linear Regression model prediction (vectorized form)\n",
      "y =h  =θT · \n",
      "θ\n",
      "• θ is the model’s parameter vector, containing the bias term θ and the feature\n",
      "0\n",
      "weights θ to θ . 1 n\n",
      "• θT is the transpose of θ (a row vector instead of a column vector). • x is the instance’s feature vector, containing x to x , with x always equal to 1. 0 n 0\n",
      "• θT · x is the dot product of θT and x. • h is the hypothesis function, using the model parameters θ.\n",
      "θ\n",
      "Okay, that’s the Linear Regression model, so now how do we train it? Well, recall that\n",
      "training a model means setting its parameters so that the model best fits the training\n",
      "set. For this purpose, we first need a measure of how well (or poorly) the model fits\n",
      "the training data.\n",
      "\n",
      "Q: •  is the jth model parameter (including the bias term\n",
      "A: θ\n",
      "\n",
      "Q: 106 | Chapter 4: Training Models This can be written much more concisely using\n",
      "A: vectorized form\n",
      "\n",
      "Q: tion 4-2 Equation 4-2 Equation 4-2 Equation 4-2 Equation 4\n",
      "A: \n",
      "y =h  =θT · \n",
      "θ\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 194:\n",
      "In Chapter 2 we saw that the most common performance measure\n",
      "of a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There‐\n",
      "fore, to train a Linear Regression model, you need to find the value of θ that minimi‐\n",
      "zes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE)\n",
      "than the RMSE, and it leads to the same result (because the value that minimizes a\n",
      "function also minimizes its square root).1\n",
      "The MSE of a Linear Regression hypothesis h on a training set X is calculated using\n",
      "θ\n",
      "Equation 4-3. Equation 4-3. MSE cost function for a Linear Regression model\n",
      "m\n",
      "MSE  ,h = 1 ∑ θT ·  i −yi 2\n",
      "θ m\n",
      "i=1\n",
      "Most of these notations were presented in Chapter 2 (see “Notations” on page 38). The only difference is that we write h instead of just h in order to make it clear that\n",
      "θ\n",
      "the model is parametrized by the vector θ. To simplify notations, we will just write\n",
      "MSE(θ) instead of MSE(X, h).\n",
      "\n",
      "Q: Equation 4-3. Equation 4-3. Equation 4-3. MSE cost\n",
      "A: m\n",
      "MSE\n",
      "\n",
      "Q: questions: In Chapter 2 we saw that the most common performance measure of a regression model is the\n",
      "A: Root Mean Square Error\n",
      "\n",
      "Q: Equation 4-3 zes the RMSE.\n",
      "A: \n",
      "i=1\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 195:\n",
      "θ\n",
      "1 It is often the case that a learning algorithm will try to optimize a different function than the performance\n",
      "measure used to evaluate the final model. This is generally because that function is easier to compute, because\n",
      "it has useful differentiation properties that the performance measure lacks, or because we want to constrain\n",
      "the model during training, as we will see when we discuss regularization. Linear Regression | 107\n",
      "The Normal Equation\n",
      "To find the value of θ that minimizes the cost function, there is a closed-form solution\n",
      "—in other words, a mathematical equation that gives the result directly. This is called\n",
      "the Normal Equation (Equation 4-4).2\n",
      "Equation 4-4. Normal Equation\n",
      "θ =  T ·  −1 ·  T · \n",
      "• θ is the value of θ that minimizes the cost function. • y is the vector of target values containing y(1) to y(m). Let’s generate some linear-looking data to test this equation on (Figure 4-1):\n",
      "import numpy as np\n",
      "X = 2 * np.random.rand(100, 1)\n",
      "y = 4 + 3 * X + np.random.randn(100, 1)\n",
      "Figure 4-1.\n",
      "\n",
      "Q: the cost function.  1 It is often a learning algorithm will try to optimize\n",
      "A: Normal Equation\n",
      "\n",
      "Q: 1 It is often the case that a learning algorithm will try to optimize a\n",
      "A: the Normal Equation\n",
      "\n",
      "Q: the cost function, or because we want to constrain the model during training, as we will see\n",
      "A: the Normal Equation (Equation 4-4).\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 196:\n",
      "Randomly generated linear dataset\n",
      "2 The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\n",
      "book. 108 | Chapter 4: Training Models\n",
      "Now let’s compute θ using the Normal Equation. We will use the inv() function from\n",
      "NumPy’s Linear Algebra module (np.linalg) to compute the inverse of a matrix, and\n",
      "the dot() method for matrix multiplication:\n",
      "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
      "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
      "The actual function that we used to generate the data is y = 4 + 3x + Gaussian noise. 0\n",
      "Let’s see what the equation found:\n",
      ">>> theta_best\n",
      "array([[ 4.21509616],\n",
      "[ 2.77011339]])\n",
      "We would have hoped for θ = 4 and θ = 3 instead of θ = 3.865 and θ = 3.139. Close\n",
      "0 1 0 1\n",
      "enough, but the noise made it impossible to recover the exact parameters of the origi‐\n",
      "nal function.\n",
      "\n",
      "Q: questions: Randomly generated linear dataset 2 The demonstration that this returns the value of  that\n",
      "A: θ\n",
      "\n",
      "Q: questions: 108 | Chapter 4: Training Models Now let’s compute  using\n",
      "A: Normal Equation\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 197:\n",
      "Now you can make predictions using θ:\n",
      ">>> X_new = np.array([[0], [2]])\n",
      ">>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
      ">>> y_predict = X_new_b.dot(theta_best)\n",
      ">>> y_predict\n",
      "array([[ 4.21509616],\n",
      "[ 9.75532293]])\n",
      "Let’s plot this model’s predictions (Figure 4-2):\n",
      "plt.plot(X_new, y_predict, \"r-\")\n",
      "plt.plot(X, y, \"b.\") plt.axis([0, 2, 0, 15])\n",
      "plt.show()\n",
      "Figure 4-2. Linear Regression model predictions\n",
      "Linear Regression | 109\n",
      "The equivalent code using Scikit-Learn looks like this:3\n",
      ">>> from sklearn.linear_model import LinearRegression\n",
      ">>> lin_reg = LinearRegression()\n",
      ">>> lin_reg.fit(X, y)\n",
      ">>> lin_reg.intercept_, lin_reg.coef_\n",
      "(array([ 4.21509616]), array([[ 2.77011339]]))\n",
      ">>> lin_reg.predict(X_new)\n",
      "array([[ 4.21509616],\n",
      "[ 9.75532293]])\n",
      "Computational Complexity\n",
      "The Normal Equation computes the inverse of XT · X, which is an n × n matrix\n",
      "(where n is the number of features). The computational complexity of inverting such a\n",
      "matrix is typically about O(n2.4) to O(n3) (depending on the implementation). In\n",
      "other words, if you double the number of features, you multiply the computation\n",
      "time by roughly 22.4 = 5.3 to 23 = 8. The Normal Equation gets very slow when the number of features\n",
      "grows large (e.g., 100,000).\n",
      "\n",
      "Q: X_new, y_predict, \"r-\") plt.plo\n",
      "A: \n",
      "plt.plot\n",
      "\n",
      "Q: (X, y) >>> lin_reg = LinearRegression\n",
      "A: n × n\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 198:\n",
      "On the positive side, this equation is linear with regards to the number of instances in\n",
      "the training set (it is O(m)), so it handles large training sets efficiently, provided they\n",
      "can fit in memory. Also, once you have trained your Linear Regression model (using the Normal Equa‐\n",
      "tion or any other algorithm), predictions are very fast: the computational complexity\n",
      "is linear with regards to both the number of instances you want to make predictions\n",
      "on and the number of features. In other words, making predictions on twice as many\n",
      "instances (or twice as many features) will just take roughly twice as much time. Now we will look at very different ways to train a Linear Regression model, better\n",
      "suited for cases where there are a large number of features, or too many training\n",
      "instances to fit in memory. 3 Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_). 110 | Chapter 4: Training Models\n",
      "Gradient Descent\n",
      "Gradient Descent is a very generic optimization algorithm capable of finding optimal\n",
      "solutions to a wide range of problems. The general idea of Gradient Descent is to\n",
      "tweak parameters iteratively in order to minimize a cost function.\n",
      "\n",
      "Q: of features, or too many training instances to fit in memory. Using the Normal Equa\n",
      "A: a large number\n",
      "\n",
      "Q: the number of instances you want to make predictions on and the number of features you want to make\n",
      "A: computational complexity\n",
      "is linear\n",
      "\n",
      "Q: tion:\n",
      "A: Normal Equa‐\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 199:\n",
      "Suppose you are lost in the mountains in a dense fog; you can only feel the slope of\n",
      "the ground below your feet. A good strategy to get to the bottom of the valley quickly\n",
      "is to go downhill in the direction of the steepest slope. This is exactly what Gradient\n",
      "Descent does: it measures the local gradient of the error function with regards to the\n",
      "parameter vector θ, and it goes in the direction of descending gradient. Once the gra‐\n",
      "dient is zero, you have reached a minimum! Concretely, you start by filling θ with random values (this is called random initializa‐\n",
      "tion), and then you improve it gradually, taking one baby step at a time, each step\n",
      "attempting to decrease the cost function (e.g., the MSE), until the algorithm converges\n",
      "to a minimum (see Figure 4-3). Figure 4-3. Gradient Descent\n",
      "An important parameter in Gradient Descent is the size of the steps, determined by\n",
      "the learning rate hyperparameter. If the learning rate is too small, then the algorithm\n",
      "will have to go through many iterations to converge, which will take a long time (see\n",
      "Figure 4-4). Gradient Descent | 111\n",
      "Figure 4-4.\n",
      "\n",
      "Q: , and it goes in the direction of the steepest slope. Gradient Descent\n",
      "A: descending gradient\n",
      "\n",
      "Q: the valley.\n",
      "A: Gradient\n",
      "Descent\n",
      "\n",
      "Q: the slopes. This is exactly what Gradient Descent does: it measures the local gradient\n",
      "A: \n",
      "parameter vector θ\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 200:\n",
      "Learning rate too small\n",
      "On the other hand, if the learning rate is too high, you might jump across the valley\n",
      "and end up on the other side, possibly even higher up than you were before. This\n",
      "might make the algorithm diverge, with larger and larger values, failing to find a good\n",
      "solution (see Figure 4-5). Figure 4-5. Learning rate too large\n",
      "Finally, not all cost functions look like nice regular bowls. There may be holes, ridges,\n",
      "plateaus, and all sorts of irregular terrains, making convergence to the minimum very\n",
      "difficult. Figure 4-6 shows the two main challenges with Gradient Descent: if the ran‐\n",
      "dom initialization starts the algorithm on the left, then it will converge to a local mini‐\n",
      "mum, which is not as good as the global minimum. If it starts on the right, then it will\n",
      "take a very long time to cross the plateau, and if you stop too early you will never\n",
      "reach the global minimum. 112 | Chapter 4: Training Models\n",
      "Figure 4-6.\n",
      "\n",
      "Q: ,, a 4-6. Learning rate too small converge to\n",
      "A: \n",
      "mum, which is not as good as the global minimum\n",
      "\n",
      "Q: the learning rate is too low., the learning rate is too low.,\n",
      "A: \n",
      "\n",
      "\n",
      "Q: : This might make the algorithm diverge, with larger values, failing to find a good\n",
      "A: solution\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 201:\n",
      "Gradient Descent pitfalls\n",
      "Fortunately, the MSE cost function for a Linear Regression model happens to be a\n",
      "convex function, which means that if you pick any two points on the curve, the line\n",
      "segment joining them never crosses the curve. This implies that there are no local\n",
      "minima, just one global minimum. It is also a continuous function with a slope that\n",
      "never changes abruptly.4 These two facts have a great consequence: Gradient Descent\n",
      "is guaranteed to approach arbitrarily close the global minimum (if you wait long\n",
      "enough and if the learning rate is not too high). In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if\n",
      "the features have very different scales. Figure 4-7 shows Gradient Descent on a train‐\n",
      "ing set where features 1 and 2 have the same scale (on the left), and on a training set\n",
      "where feature 1 has much smaller values than feature 2 (on the right).5\n",
      "Figure 4-7. Gradient Descent with and without feature scaling\n",
      "4 Technically speaking, its derivative is Lipschitz continuous.\n",
      "\n",
      "Q: the MSE cost function for a Linear Regression model happens to be a con\n",
      "A: \n",
      "convex function\n",
      "\n",
      "Q: the line segment joining them never crosses the curve. This implies that there are no local minima\n",
      "A: \n",
      "convex function\n",
      "\n",
      "Q: . This implies that there are no local minima, just one global minimum. This implies that\n",
      "A: \n",
      "convex function\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 202:\n",
      "5 Since feature 1 is smaller, it takes a larger change in θ1 to affect the cost function, which is why the bowl is\n",
      "elongated along the θ1 axis. Gradient Descent | 113\n",
      "As you can see, on the left the Gradient Descent algorithm goes straight toward the\n",
      "minimum, thereby reaching it quickly, whereas on the right it first goes in a direction\n",
      "almost orthogonal to the direction of the global minimum, and it ends with a long\n",
      "march down an almost flat valley. It will eventually reach the minimum, but it will\n",
      "take a long time. When using Gradient Descent, you should ensure that all features\n",
      "have a similar scale (e.g., using Scikit-Learn’s StandardScaler\n",
      "class), or else it will take much longer to converge. This diagram also illustrates the fact that training a model means searching for a\n",
      "combination of model parameters that minimizes a cost function (over the training\n",
      "set). It is a search in the model’s parameter space: the more parameters a model has,\n",
      "the more dimensions this space has, and the harder the search is: searching for a nee‐\n",
      "dle in a 300-dimensional haystack is much trickier than in three dimensions.\n",
      "\n",
      "Q: the minimum, thereby reaching it quickly, whereas on the right it goes straight toward the\n",
      "A: \n",
      "\n",
      "\n",
      "Q: 113 Questions: 5 Since feature 1 is smaller, it takes a larger change in\n",
      "A: θ1\n",
      "\n",
      "Q: 113 Questions: Gradient Descent | 113\n",
      "A: \n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 203:\n",
      "Fortu‐\n",
      "nately, since the cost function is convex in the case of Linear Regression, the needle is\n",
      "simply at the bottom of the bowl. Batch Gradient Descent\n",
      "To implement Gradient Descent, you need to compute the gradient of the cost func‐\n",
      "tion with regards to each model parameter θ. In other words, you need to calculate\n",
      "j\n",
      "how much the cost function will change if you change θ just a little bit. This is called\n",
      "j\n",
      "a partial derivative. It is like asking “what is the slope of the mountain under my feet\n",
      "if I face east?” and then asking the same question facing north (and so on for all other\n",
      "dimensions, if you can imagine a universe with more than three dimensions). Equa‐\n",
      "tion 4-5 computes the partial derivative of the cost function with regards to parame‐\n",
      "∂\n",
      "ter θ, noted MSE θ . j ∂θ\n",
      "j\n",
      "Equation 4-5. Partial derivatives of the cost function\n",
      "m\n",
      "∂ MSE θ = 2 ∑ θT ·  i −yi xi\n",
      "∂θ m j\n",
      "j i=1\n",
      "Instead of computing these gradients individually, you can use Equation 4-6 to com‐\n",
      "pute them all in one go.\n",
      "\n",
      "Q: i yi xi  m j\n",
      "A: Batch Gradient Descent\n",
      "\n",
      "Q: Questions: Fortu nately, since the cost function is convex in case\n",
      "A: Linear Regression\n",
      "\n",
      "Q: Questions: Batch Gradient Descent.\n",
      "A: \n",
      "j\n",
      "a partial derivative\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 204:\n",
      "The gradient vector, noted ∇ MSE(θ), contains all the partial\n",
      "θ\n",
      "derivatives of the cost function (one for each model parameter). 114 | Chapter 4: Training Models\n",
      "Equation 4-6. Gradient vector of the cost function\n",
      "∂\n",
      "MSE θ\n",
      "∂θ\n",
      "0\n",
      "∂\n",
      "∇ θMSE θ = ∂θ 1MSE θ = m2  T ·  ·θ− \n",
      "⋮\n",
      "∂\n",
      "MSE θ\n",
      "∂θ\n",
      "n\n",
      "Notice that this formula involves calculations over the full training\n",
      "set X, at each Gradient Descent step! This is why the algorithm is\n",
      "called Batch Gradient Descent: it uses the whole batch of training\n",
      "data at every step. As a result it is terribly slow on very large train‐\n",
      "ing sets (but we will see much faster Gradient Descent algorithms\n",
      "shortly). However, Gradient Descent scales well with the number of\n",
      "features; training a Linear Regression model when there are hun‐\n",
      "dreds of thousands of features is much faster using Gradient\n",
      "Descent than using the Normal Equation. Once you have the gradient vector, which points uphill, just go in the opposite direc‐\n",
      "tion to go downhill. This means subtracting ∇ MSE(θ) from θ.\n",
      "\n",
      "Q: MSE() contains all the partial  derivatives of the cost function\n",
      "A: gradient vector\n",
      "\n",
      "Q: 114 | Chapter 4: Training Models Equation 4-6: Training Models Equation\n",
      "A: ∂\n",
      "MSE θ\n",
      "∂θ\n",
      "0\n",
      "\n",
      "\n",
      "Q: X, at each Gradient Descent step!\n",
      "A: training\n",
      "set\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 205:\n",
      "This is where the\n",
      "θ\n",
      "learning rate η comes into play:6 multiply the gradient vector by η to determine the\n",
      "size of the downhill step (Equation 4-7). Equation 4-7. Gradient Descent step\n",
      "θnext step =θ−η∇ MSE θ\n",
      "θ\n",
      "Let’s look at a quick implementation of this algorithm:\n",
      "eta = 0.1 # learning rate\n",
      "n_iterations = 1000\n",
      "m = 100\n",
      "theta = np.random.randn(2,1) # random initialization\n",
      "for iteration in range(n_iterations):\n",
      "gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
      "theta = theta - eta * gradients\n",
      "6 Eta (η) is the 7th letter of the Greek alphabet. Gradient Descent | 115\n",
      "That wasn’t too hard! Let’s look at the resulting theta:\n",
      ">>> theta\n",
      "array([[ 4.21509616],\n",
      "[ 2.77011339]])\n",
      "Hey, that’s exactly what the Normal Equation found! Gradient Descent worked per‐\n",
      "fectly. But what if you had used a different learning rate eta? Figure 4-8 shows the\n",
      "first 10 steps of Gradient Descent using three different learning rates (the dashed line\n",
      "represents the starting point). Figure 4-8. Gradient Descent with various learning rates\n",
      "On the left, the learning rate is too low: the algorithm will eventually reach the solu‐\n",
      "tion, but it will take a long time.\n",
      "\n",
      "Q: MSE   Let’s look at this algorithm: Equation 4\n",
      "A: \n",
      "θ\n",
      "\n",
      "Q: theta array([4.21509616], [ 2.77011339]])\n",
      "A: \n",
      ">>>\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 206:\n",
      "In the middle, the learning rate looks pretty good: in\n",
      "just a few iterations, it has already converged to the solution. On the right, the learn‐\n",
      "ing rate is too high: the algorithm diverges, jumping all over the place and actually\n",
      "getting further and further away from the solution at every step. To find a good learning rate, you can use grid search (see Chapter 2). However, you\n",
      "may want to limit the number of iterations so that grid search can eliminate models\n",
      "that take too long to converge. You may wonder how to set the number of iterations. If it is too low, you will still be\n",
      "far away from the optimal solution when the algorithm stops, but if it is too high, you\n",
      "will waste time while the model parameters do not change anymore. A simple solu‐\n",
      "tion is to set a very large number of iterations but to interrupt the algorithm when the\n",
      "gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny\n",
      "number ϵ (called the tolerance)—because this happens when Gradient Descent has\n",
      "(almost) reached the minimum.\n",
      "\n",
      "Q: ing rate looks pretty good: in just a few iterations, it has already\n",
      "A: converged to the solution\n",
      "\n",
      "Q: ing rate is too high: the algorithm diverges, jumping all over the place and actually\n",
      "A: learn‐\n",
      "ing rate\n",
      "\n",
      "Q: ing rate is too high: the algorithm diverges, jumping all over the place and actually getting\n",
      "A: further and further away from the solution\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 207:\n",
      "116 | Chapter 4: Training Models\n",
      "Convergence Rate\n",
      "When the cost function is convex and its slope does not change abruptly (as is the\n",
      "case for the MSE cost function), it can be shown that Batch Gradient Descent with a\n",
      "1\n",
      "fixed learning rate has a convergence rate of O . In other words, if you divide\n",
      "iterations\n",
      "the tolerance ϵ by 10 (to have a more precise solution), then the algorithm will have\n",
      "to run about 10 times more iterations. Stochastic Gradient Descent\n",
      "The main problem with Batch Gradient Descent is the fact that it uses the whole\n",
      "training set to compute the gradients at every step, which makes it very slow when\n",
      "the training set is large. At the opposite extreme, Stochastic Gradient Descent just\n",
      "picks a random instance in the training set at every step and computes the gradients\n",
      "based only on that single instance. Obviously this makes the algorithm much faster\n",
      "since it has very little data to manipulate at every iteration.\n",
      "\n",
      "Q: the the a 1 fixed learning rate. This makes the algorithm much faster since it\n",
      "A: it has very little data to manipulate at every iteration\n",
      "\n",
      "Q: the tolerance  by 10 (to have a more precise solution), then the algorithm will\n",
      "A: \n",
      "iterations\n",
      "the tolerance ϵ\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 208:\n",
      "It also makes it possible to\n",
      "train on huge training sets, since only one instance needs to be in memory at each\n",
      "iteration (SGD can be implemented as an out-of-core algorithm.7)\n",
      "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\n",
      "less regular than Batch Gradient Descent: instead of gently decreasing until it reaches\n",
      "the minimum, the cost function will bounce up and down, decreasing only on aver‐\n",
      "age. Over time it will end up very close to the minimum, but once it gets there it will\n",
      "continue to bounce around, never settling down (see Figure 4-9). So once the algo‐\n",
      "rithm stops, the final parameter values are good, but not optimal. Figure 4-9. Stochastic Gradient Descent\n",
      "7 Out-of-core algorithms are discussed in Chapter 1. Gradient Descent | 117\n",
      "When the cost function is very irregular (as in Figure 4-6), this can actually help the\n",
      "algorithm jump out of local minima, so Stochastic Gradient Descent has a better\n",
      "chance of finding the global minimum than Batch Gradient Descent does. Therefore randomness is good to escape from local optima, but bad because it means\n",
      "that the algorithm can never settle at the minimum.\n",
      "\n",
      "Q: , but it will bounce around, never settling down (see Figure 4-9). Gradient\n",
      "A: Over time it will end up very close to the minimum\n",
      "\n",
      "Q: to train on huge training sets, since only one instance needs to be in memory at each it\n",
      "A: \n",
      "iteration\n",
      "\n",
      "Q: : a few questions: Often it will end up close to the minimum, but once\n",
      "A: it will\n",
      "continue to bounce around\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 209:\n",
      "One solution to this dilemma is\n",
      "to gradually reduce the learning rate. The steps start out large (which helps make\n",
      "quick progress and escape local minima), then get smaller and smaller, allowing the\n",
      "algorithm to settle at the global minimum. This process is called simulated annealing,\n",
      "because it resembles the process of annealing in metallurgy where molten metal is\n",
      "slowly cooled down. The function that determines the learning rate at each iteration\n",
      "is called the learning schedule. If the learning rate is reduced too quickly, you may get\n",
      "stuck in a local minimum, or even end up frozen halfway to the minimum. If the\n",
      "learning rate is reduced too slowly, you may jump around the minimum for a long\n",
      "time and end up with a suboptimal solution if you halt training too early.\n",
      "\n",
      "Q: the learning rate. The steps start out large (which helps make quick progress and escape local minim\n",
      "A: minima\n",
      "\n",
      "Q: Fragen: Eine Lösung à cette dilemma: a solution is to gradually reduce the learning rate\n",
      "A: \n",
      "algorithm\n",
      "\n",
      "Q: large (which helps make quick progress and escape local minima), then get smaller and smaller,\n",
      "A: The steps start out large\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 210:\n",
      "This code implements Stochastic Gradient Descent using a simple learning schedule:\n",
      "n_epochs = 50\n",
      "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
      "def learning_schedule(t):\n",
      "return t0 / (t + t1)\n",
      "theta = np.random.randn(2,1) # random initialization\n",
      "for epoch in range(n_epochs):\n",
      "for i in range(m):\n",
      "random_index = np.random.randint(m)\n",
      "xi = X_b[random_index:random_index+1]\n",
      "yi = y[random_index:random_index+1]\n",
      "gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
      "eta = learning_schedule(epoch * m + i)\n",
      "theta = theta - eta * gradients\n",
      "By convention we iterate by rounds of m iterations; each round is called an epoch. While the Batch Gradient Descent code iterated 1,000 times through the whole train‐\n",
      "ing set, this code goes through the training set only 50 times and reaches a fairly good\n",
      "solution:\n",
      ">>> theta\n",
      "array([[ 4.21076011],\n",
      "[ 2.74856079]])\n",
      "Figure 4-10 shows the first 10 steps of training (notice how irregular the steps are). 118 | Chapter 4: Training Models\n",
      "Figure 4-10. Stochastic Gradient Descent first 10 steps\n",
      "Note that since instances are picked randomly, some instances may be picked several\n",
      "times per epoch while others may not be picked at all.\n",
      "\n",
      "Q: - yi = 5, 50 # learning schedule hyperparameters def learning_\n",
      "A: \n",
      "yi\n",
      "\n",
      "Q: epochs = 50 t0, t1 = 5, 50 # learning\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ) 118 | Chapter 4: Training Models Figure 4-10 shows the first 10 steps of\n",
      "A: training\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 211:\n",
      "If you want to be sure that the\n",
      "algorithm goes through every instance at each epoch, another approach is to shuffle\n",
      "the training set, then go through it instance by instance, then shuffle it again, and so\n",
      "on. However, this generally converges more slowly. To perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe\n",
      "gressor class, which defaults to optimizing the squared error cost function. The fol‐\n",
      "lowing code runs 50 epochs, starting with a learning rate of 0.1 (eta0=0.1), using the\n",
      "default learning schedule (different from the preceding one), and it does not use any\n",
      "regularization (penalty=None; more details on this shortly):\n",
      "from sklearn.linear_model import SGDRegressor\n",
      "sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)\n",
      "sgd_reg.fit(X, y.ravel())\n",
      "Once again, you find a solution very close to the one returned by the Normal Equa‐\n",
      "tion:\n",
      ">>> sgd_reg.intercept_, sgd_reg.coef_\n",
      "(array([ 4.18380366]), array([ 2.74205299]))\n",
      "Mini-batch Gradient Descent\n",
      "The last Gradient Descent algorithm we will look at is called Mini-batch Gradient\n",
      "Descent.\n",
      "\n",
      "Q: sgd_reg = SGDRegressor(n_iter\n",
      "A: \n",
      "sgd_reg.fit(X, y.ravel())\n",
      "\n",
      "Q: the algorithm, then shuffle it again, and so on.\n",
      "A: the training set\n",
      "\n",
      "Q: converges more slowly. SGDRe gressor class, which defaults\n",
      "A: \n",
      "gressor class, which defaults to optimizing the squared error cost function\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 212:\n",
      "It is quite simple to understand once you know Batch and Stochastic Gradi‐\n",
      "ent Descent: at each step, instead of computing the gradients based on the full train‐\n",
      "ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\n",
      "Gradient Descent | 119\n",
      "batch GD computes the gradients on small random sets of instances called mini-\n",
      "batches. The main advantage of Mini-batch GD over Stochastic GD is that you can\n",
      "get a performance boost from hardware optimization of matrix operations, especially\n",
      "when using GPUs. The algorithm’s progress in parameter space is less erratic than with SGD, especially\n",
      "with fairly large mini-batches. As a result, Mini-batch GD will end up walking\n",
      "around a bit closer to the minimum than SGD. But, on the other hand, it may be\n",
      "harder for it to escape from local minima (in the case of problems that suffer from\n",
      "local minima, unlike Linear Regression as we saw earlier). Figure 4-11 shows the\n",
      "paths taken by the three Gradient Descent algorithms in parameter space during\n",
      "training.\n",
      "\n",
      "Q: ent Descent | 119 batch GD computes the gradients on small random sets\n",
      "A: Mini-\n",
      "Gradient\n",
      "\n",
      "Q: questions: What’s more, Mini-batch GD than Stochastic GD\n",
      "A: you can\n",
      "get a performance boost from hardware optimization of matrix operations\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 213:\n",
      "They all end up near the minimum, but Batch GD’s path actually stops at the\n",
      "minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\n",
      "tic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\n",
      "ing schedule. Figure 4-11. Gradient Descent paths in parameter space\n",
      "Let’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\n",
      "m is the number of training instances and n is the number of features); see Table 4-1. Table 4-1. Comparison of algorithms for Linear Regression\n",
      "Algorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\n",
      "Normal Equation Fast No Slow 0 No LinearRegression\n",
      "Batch GD Slow No Fast 2 Yes n/a\n",
      "8 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\n",
      "used to train many other models, as we will see.\n",
      "\n",
      "Q: GD 0 No LinearRegression Batch GD Slow No Fast 2 Yes\n",
      "A: n/a\n",
      "\n",
      "\n",
      "Q: GD and Mini-batch GD continue to walk around. Don’t forget that\n",
      "A: Batch GD takes a lot of time to take each step\n",
      "\n",
      "Q: ing schedule Figure 4-11 ing schedule Figure 4-11 ing schedule Figure 4\n",
      "A: near the minimum\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 214:\n",
      "120 | Chapter 4: Training Models\n",
      "Algorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\n",
      "Stochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\n",
      "Mini-batch GD Fast Yes Fast ≥2 Yes n/a\n",
      "There is almost no difference after training: all these algorithms\n",
      "end up with very similar models and make predictions in exactly\n",
      "the same way. Polynomial Regression\n",
      "What if your data is actually more complex than a simple straight line? Surprisingly,\n",
      "you can actually use a linear model to fit nonlinear data. A simple way to do this is to\n",
      "add powers of each feature as new features, then train a linear model on this extended\n",
      "set of features. This technique is called Polynomial Regression. Let’s look at an example. First, let’s generate some nonlinear data, based on a simple\n",
      "quadratic equation9 (plus some noise; see Figure 4-12):\n",
      "m = 100\n",
      "X = 6 * np.random.rand(m, 1) - 3\n",
      "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
      "Figure 4-12. Generated nonlinear and noisy dataset\n",
      "9 A quadratic equation is of the form y = ax2 + bx + c.\n",
      "Polynomial Regression | 121\n",
      "Clearly, a straight line will never fit this data properly.\n",
      "\n",
      "Q: a linear model to fit nonlinear data. a quadratic equation9\n",
      "A: Polynomial Regression\n",
      "\n",
      "Q: GD Fast Yes Fast 2 Yes SGDRegressor Mini-batch\n",
      "A: \n",
      "Stochastic\n",
      "\n",
      "Q: a linear model to fit nonlinear data.\n",
      "A: Polynomial Regression\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 215:\n",
      "So let’s use Scikit-Learn’s Poly\n",
      "nomialFeatures class to transform our training data, adding the square (2nd-degree\n",
      "polynomial) of each feature in the training set as new features (in this case there is\n",
      "just one feature):\n",
      ">>> from sklearn.preprocessing import PolynomialFeatures\n",
      ">>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
      ">>> X_poly = poly_features.fit_transform(X)\n",
      ">>> X[0]\n",
      "array([-0.75275929])\n",
      ">>> X_poly[0]\n",
      "array([-0.75275929, 0.56664654])\n",
      "X_poly now contains the original feature of X plus the square of this feature. Now you\n",
      "can fit a LinearRegression model to this extended training data (Figure 4-13):\n",
      ">>> lin_reg = LinearRegression()\n",
      ">>> lin_reg.fit(X_poly, y)\n",
      ">>> lin_reg.intercept_, lin_reg.coef_\n",
      "(array([ 1.78134581]), array([[ 0.93366893, 0.56456263]]))\n",
      "Figure 4-13. Polynomial Regression model predictions\n",
      "Not bad: the model estimates y =0.56x2+0.93x +1.78 when in fact the original\n",
      "1 1\n",
      "function was y=0.5x2+1.0x +2.0+Gaussian noise. 1 1\n",
      "Note that when there are multiple features, Polynomial Regression is capable of find‐\n",
      "ing relationships between features (which is something a plain Linear Regression\n",
      "model cannot do). This is made possible by the fact that PolynomialFeatures also\n",
      "adds all combinations of features up to the given degree.\n",
      "\n",
      "Q: X_poly, y) >>> lin_reg = Line\n",
      "A: find‐\n",
      "ing relationships between features\n",
      "\n",
      "Q: (X_poly, y) >>> lin_reg = Line\n",
      "A: 0.56x2+0.93x +1.78\n",
      "\n",
      "Q: , y) >>> lin_reg = LinearRegression() >>\n",
      "A: 0.56x2+0.93x +1.78\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 216:\n",
      "For example, if there were\n",
      "122 | Chapter 4: Training Models\n",
      "two features a and b, PolynomialFeatures with degree=3 would not only add the\n",
      "features a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2. PolynomialFeatures(degree=d) transforms an array containing n\n",
      "n+d ! features into an array containing features, where n! is the\n",
      "d!n! factorial of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\n",
      "rial explosion of the number of features! Learning Curves\n",
      "If you perform high-degree Polynomial Regression, you will likely fit the training\n",
      "data much better than with plain Linear Regression. For example, Figure 4-14 applies\n",
      "a 300-degree polynomial model to the preceding training data, and compares the\n",
      "result with a pure linear model and a quadratic model (2nd-degree polynomial). Notice how the 300-degree polynomial model wiggles around to get as close as possi‐\n",
      "ble to the training instances. Figure 4-14. High-degree Polynomial Regression\n",
      "Of course, this high-degree Polynomial Regression model is severely overfitting the\n",
      "training data, while the linear model is underfitting it. The model that will generalize\n",
      "best in this case is the quadratic model.\n",
      "\n",
      "Q: Polynomial Regression Models (degree=3) transforms an array containing features\n",
      "A: \n",
      "122\n",
      "\n",
      "Q: questions arise: if there were 122 | Chapter 4: Training Models two features a\n",
      "A: \n",
      "122\n",
      "\n",
      "Q: n! features into an array containing features, where n! is the factorial\n",
      "A: n\n",
      "n+d\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 217:\n",
      "It makes sense since the data was generated\n",
      "using a quadratic model, but in general you won’t know what function generated the\n",
      "data, so how can you decide how complex your model should be? How can you tell\n",
      "that your model is overfitting or underfitting the data? Learning Curves | 123\n",
      "In Chapter 2 you used cross-validation to get an estimate of a model’s generalization\n",
      "performance. If a model performs well on the training data but generalizes poorly\n",
      "according to the cross-validation metrics, then your model is overfitting. If it per‐\n",
      "forms poorly on both, then it is underfitting. This is one way to tell when a model is\n",
      "too simple or too complex. Another way is to look at the learning curves: these are plots of the model’s perfor‐\n",
      "mance on the training set and the validation set as a function of the training set size. To generate the plots, simply train the model several times on different sized subsets\n",
      "of the training set.\n",
      "\n",
      "Q: the data, but it is underfitting. If it per forms poorly on both,\n",
      "A: overfitting\n",
      "\n",
      "Q: using quadratic model, but you won’t know what function generated the data?\n",
      "A: \n",
      "data\n",
      "\n",
      "Q: a model performs well on the training data but generalizes poorly according to the cross\n",
      "A: your model is overfitting\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 218:\n",
      "The following code defines a function that plots the learning\n",
      "curves of a model given some training data:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.model_selection import train_test_split\n",
      "def plot_learning_curves(model, X, y):\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
      "train_errors, val_errors = [], []\n",
      "for m in range(1, len(X_train)):\n",
      "model.fit(X_train[:m], y_train[:m])\n",
      "y_train_predict = model.predict(X_train[:m])\n",
      "y_val_predict = model.predict(X_val)\n",
      "train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
      "val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
      "plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
      "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
      "Let’s look at the learning curves of the plain Linear Regression model (a straight line;\n",
      "Figure 4-15):\n",
      "lin_reg = LinearRegression()\n",
      "plot_learning_curves(lin_reg, X, y)\n",
      "Figure 4-15. Learning curves\n",
      "124 | Chapter 4: Training Models\n",
      "This deserves a bit of explanation. First, let’s look at the performance on the training\n",
      "data: when there are just one or two instances in the training set, the model can fit\n",
      "them perfectly, which is why the curve starts at zero. But as new instances are added\n",
      "to the training set, it becomes impossible for the model to fit the training data per‐\n",
      "fectly, both because the data is noisy and because it is not linear at all.\n",
      "\n",
      "Q: : lin_reg = LinearRegression() plot_learning_curve\n",
      "A: \n",
      "fectly\n",
      "\n",
      "Q: a function that plots the learning curves of a model given some training data:\n",
      "A: plt.plot\n",
      "\n",
      "Q: 124 | Chapter 4: Training Models This is a question: What are the learning\n",
      "A: learning\n",
      "curves\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 219:\n",
      "So the error on\n",
      "the training data goes up until it reaches a plateau, at which point adding new instan‐\n",
      "ces to the training set doesn’t make the average error much better or worse. Now let’s\n",
      "look at the performance of the model on the validation data. When the model is\n",
      "trained on very few training instances, it is incapable of generalizing properly, which\n",
      "is why the validation error is initially quite big. Then as the model is shown more\n",
      "training examples, it learns and thus the validation error slowly goes down. However,\n",
      "once again a straight line cannot do a good job modeling the data, so the error ends\n",
      "up at a plateau, very close to the other curve. These learning curves are typical of an underfitting model. Both curves have reached\n",
      "a plateau; they are close and fairly high. If your model is underfitting the training data, adding more train‐\n",
      "ing examples will not help. You need to use a more complex model\n",
      "or come up with better features.\n",
      "\n",
      "Q: the model on the validation data. The error goes up until it reaches a plateau,\n",
      "A: \n",
      "trained on very few training instances\n",
      "\n",
      "Q: data. So the error on the training data goes up until it reaches a plateau,\n",
      "A: \n",
      "ces\n",
      "\n",
      "Q: . Let’s look at the performance of the model on the validation data.\n",
      "A: When the model is\n",
      "trained on very few training instances\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 220:\n",
      "Now let’s look at the learning curves of a 10th-degree polynomial model on the same\n",
      "data (Figure 4-16):\n",
      "from sklearn.pipeline import Pipeline\n",
      "polynomial_regression = Pipeline((\n",
      "(\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
      "(\"sgd_reg\", LinearRegression()),\n",
      "))\n",
      "plot_learning_curves(polynomial_regression, X, y)\n",
      "These learning curves look a bit like the previous ones, but there are two very impor‐\n",
      "tant differences:\n",
      "• The error on the training data is much lower than with the Linear Regression\n",
      "model. • There is a gap between the curves. This means that the model performs signifi‐\n",
      "cantly better on the training data than on the validation data, which is the hall‐\n",
      "mark of an overfitting model. However, if you used a much larger training set,\n",
      "the two curves would continue to get closer. Learning Curves | 125\n",
      "Figure 4-16. Learning curves for the polynomial model\n",
      "One way to improve an overfitting model is to feed it more training\n",
      "data until the validation error reaches the training error.\n",
      "\n",
      "Q: polynomial model. Learning curves for the polynomial model One way to\n",
      "A: to feed it more training\n",
      "data until the validation error reaches the training error\n",
      "\n",
      "Q: of a 10th-degree polynomial model on the same data (Figure 4\n",
      "A: 125\n",
      "\n",
      "Q: • There is a gap between the curves This means that the model performs signifi\n",
      "A: training data\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 221:\n",
      "The Bias/Variance Tradeoff\n",
      "An important theoretical result of statistics and Machine Learning is the fact that a\n",
      "model’s generalization error can be expressed as the sum of three very different\n",
      "errors:\n",
      "Bias\n",
      "This part of the generalization error is due to wrong assumptions, such as assum‐\n",
      "ing that the data is linear when it is actually quadratic. A high-bias model is most\n",
      "likely to underfit the training data.10\n",
      "Variance\n",
      "This part is due to the model’s excessive sensitivity to small variations in the\n",
      "training data. A model with many degrees of freedom (such as a high-degree pol‐\n",
      "ynomial model) is likely to have high variance, and thus to overfit the training\n",
      "data. 10 This notion of bias is not to be confused with the bias term of linear models. 126 | Chapter 4: Training Models\n",
      "Irreducible error\n",
      "This part is due to the noisiness of the data itself. The only way to reduce this\n",
      "part of the error is to clean up the data (e.g., fix the data sources, such as broken\n",
      "sensors, or detect and remove outliers). Increasing a model’s complexity will typically increase its variance and reduce its bias.\n",
      "\n",
      "Q: : Bias/Variance Tradeoff An important theoretical result of statistics and Machine Learning\n",
      "A: \n",
      "Bias\n",
      "\n",
      "\n",
      "Q: Bias This part of the generalization error is due to wrong assumptions, such as assum\n",
      "A: that the data is linear when it is actually quadratic\n",
      "\n",
      "Q: is most likely to underfit the training data.10 Variance This part is due to the\n",
      "A: high-bias model\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 222:\n",
      "Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a tradeoff. Regularized Linear Models\n",
      "As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\n",
      "model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\n",
      "for it to overfit the data. For example, a simple way to regularize a polynomial model\n",
      "is to reduce the number of polynomial degrees. For a linear model, regularization is typically achieved by constraining the weights of\n",
      "the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\n",
      "which implement three different ways to constrain the weights. Ridge Regression\n",
      "Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin‐\n",
      "ear Regression: a regularization term equal to α∑n θ2 is added to the cost function. i=1 i\n",
      "This forces the learning algorithm to not only fit the data but also keep the model\n",
      "weights as small as possible. Note that the regularization term should only be added\n",
      "to the cost function during training.\n",
      "\n",
      "Q: , the harder it will be for it to overfit the data. For a linear model\n",
      "A: fewer degrees of freedom it has\n",
      "\n",
      "Q: ’s complexity increases its bias and reduces its variance.\n",
      "A: reducing a model\n",
      "\n",
      "Q: Linear Models. This is why it is called a tradeoff Regularized Linear\n",
      "A: reducing a model’s complexity\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 223:\n",
      "Once the model is trained, you want to evaluate\n",
      "the model’s performance using the unregularized performance measure. It is quite common for the cost function used during training to be\n",
      "different from the performance measure used for testing. Apart\n",
      "from regularization, another reason why they might be different is\n",
      "that a good training cost function should have optimization-\n",
      "friendly derivatives, while the performance measure used for test‐\n",
      "ing should be as close as possible to the final objective. A good\n",
      "example of this is a classifier trained using a cost function such as\n",
      "the log loss (discussed in a moment) but evaluated using precision/\n",
      "recall. The hyperparameter α controls how much you want to regularize the model. If α = 0\n",
      "then Ridge Regression is just Linear Regression. If α is very large, then all weights end\n",
      "Regularized Linear Models | 127\n",
      "up very close to zero and the result is a flat line going through the data’s mean. Equa‐\n",
      "tion 4-8 presents the Ridge Regression cost function.11\n",
      "Equation 4-8.\n",
      "\n",
      "Q: , you want to evaluate the model’s performance using the unregularized performance measure\n",
      "A: Once the model is trained\n",
      "\n",
      "Q: .\n",
      "A: unregularized performance measure\n",
      "\n",
      "Q: a good training cost function should have optimization-friendly derivatives, while the performance measure used for\n",
      "A: test‐\n",
      "ing\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 224:\n",
      "Ridge Regression cost function\n",
      "n\n",
      "J θ =MSE θ +α1 ∑ θ2\n",
      "2 i\n",
      "i=1\n",
      "Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). If we\n",
      "0\n",
      "define w as the vector of feature weights (θ to θ ), then the regularization term is\n",
      "1 n\n",
      "simply equal to ½(∥ w ∥ )2, where ∥ · ∥ represents the ℓ norm of the weight vector.12\n",
      "2 2 2\n",
      "For Gradient Descent, just add αw to the MSE gradient vector (Equation 4-6). It is important to scale the data (e.g., using a StandardScaler)\n",
      "before performing Ridge Regression, as it is sensitive to the scale of\n",
      "the input features. This is true of most regularized models. Figure 4-17 shows several Ridge models trained on some linear data using different α\n",
      "value. On the left, plain Ridge models are used, leading to linear predictions. On the\n",
      "right, the data is first expanded using PolynomialFeatures(degree=10), then it is\n",
      "scaled using a StandardScaler, and finally the Ridge models are applied to the result‐\n",
      "ing features: this is Polynomial Regression with Ridge regularization.\n",
      "\n",
      "Q: , and the data is scaled using a StandardScaler . This is true\n",
      "A: On the\n",
      "right\n",
      "\n",
      "Q: 2 2 i i=1 Note that the bias term\n",
      "A: \n",
      "n\n",
      "\n",
      "\n",
      "Q: w as the vector of feature weights ( to  ), then the regular\n",
      "A: \n",
      "0\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 225:\n",
      "Note how\n",
      "increasing α leads to flatter (i.e., less extreme, more reasonable) predictions; this\n",
      "reduces the model’s variance but increases its bias. As with Linear Regression, we can perform Ridge Regression either by computing a\n",
      "closed-form equation or by performing Gradient Descent. The pros and cons are the\n",
      "same. Equation 4-9 shows the closed-form solution (where A is the n × n identity\n",
      "matrix13 except with a 0 in the top-left cell, corresponding to the bias term). 11 It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\n",
      "notation throughout the rest of this book. The context will make it clear which cost function is being dis‐\n",
      "cussed. 12 Norms are discussed in Chapter 2. 13 A square matrix full of 0s except for 1s on the main diagonal (top-left to bottom-right). 128 | Chapter 4: Training Models\n",
      "Figure 4-17. Ridge Regression\n",
      "Equation 4-9.\n",
      "\n",
      "Q: , , ,. Note how increasing  leads\n",
      "A: α\n",
      "\n",
      "Q: leads to flatter (i.e., less extreme, more reasonable) predictions;\n",
      "A: \n",
      "increasing α\n",
      "\n",
      "Q: Questions: As with Linear Regression, we can perform Ridge Regression either by computing a\n",
      "A: \n",
      "closed-form equation\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 226:\n",
      "Ridge Regression closed-form solution\n",
      "θ =  T ·  +α  −1 ·  T · \n",
      "Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐\n",
      "tion (a variant of Equation 4-9 using a matrix factorization technique by André-Louis\n",
      "Cholesky):\n",
      ">>> from sklearn.linear_model import Ridge\n",
      ">>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
      ">>> ridge_reg.fit(X, y)\n",
      ">>> ridge_reg.predict([[1.5]])\n",
      "array([[ 1.55071465]])\n",
      "And using Stochastic Gradient Descent:14\n",
      ">>> sgd_reg = SGDRegressor(penalty=\"l2\")\n",
      ">>> sgd_reg.fit(X, y.ravel())\n",
      ">>> sgd_reg.predict([[1.5]])\n",
      "array([[ 1.13500145]])\n",
      "The penalty hyperparameter sets the type of regularization term to use. Specifying\n",
      "\"l2\" indicates that you want SGD to add a regularization term to the cost function\n",
      "equal to half the square of the ℓ norm of the weight vector: this is simply Ridge\n",
      "2\n",
      "Regression. 14 Alternatively you can use the Ridge class with the \"sag\" solver. Stochastic Average GD is a variant of SGD. For more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\n",
      "rithm” by Mark Schmidt et al. from the University of British Columbia.\n",
      "\n",
      "Q: ([1.5]]) array([[1.5]]) and using a closed\n",
      "A: ridge_reg.predict\n",
      "\n",
      "Q: the l norm of the weight vector: this is simply Ridge 2 Regression 14 Alternatively\n",
      "A: Ridge\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 227:\n",
      "Regularized Linear Models | 129\n",
      "Lasso Regression\n",
      "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso\n",
      "Regression) is another regularized version of Linear Regression: just like Ridge\n",
      "Regression, it adds a regularization term to the cost function, but it uses the ℓ norm\n",
      "1\n",
      "of the weight vector instead of half the square of the ℓ norm (see Equation 4-10). 2\n",
      "Equation 4-10. Lasso Regression cost function\n",
      "n\n",
      "J θ =MSE θ +α ∑ θ\n",
      "i\n",
      "i=1\n",
      "Figure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models with\n",
      "Lasso models and uses smaller α values. Figure 4-18. Lasso Regression\n",
      "An important characteristic of Lasso Regression is that it tends to completely elimi‐\n",
      "nate the weights of the least important features (i.e., set them to zero). For example,\n",
      "the dashed line in the right plot on Figure 4-18 (with α = 10-7) looks quadratic, almost\n",
      "linear: all the weights for the high-degree polynomial features are equal to zero. In\n",
      "other words, Lasso Regression automatically performs feature selection and outputs a\n",
      "sparse model (i.e., with few nonzero feature weights).\n",
      "\n",
      "Q: =MSE  +   i i=1\n",
      "A: α ∑ θ\n",
      "\n",
      "Q: Fragen: 129 Lasso Regression Least Absolute Shrinkage and Selection Operator Re\n",
      "A: Lasso\n",
      "Regression\n",
      "\n",
      "Q: 4-17 but replaces Ridge models with Lasso models and uses smaller  values and\n",
      "A: Least Absolute Shrinkage and Selection Operator Regression\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 228:\n",
      "You can get a sense of why this is the case by looking at Figure 4-19: on the top-left\n",
      "plot, the background contours (ellipses) represent an unregularized MSE cost func‐\n",
      "tion (α = 0), and the white circles show the Batch Gradient Descent path with that\n",
      "cost function. The foreground contours (diamonds) represent the ℓ penalty, and the\n",
      "1\n",
      "triangles show the BGD path for this penalty only (α → ∞). Notice how the path first\n",
      "130 | Chapter 4: Training Models\n",
      "reaches θ = 0, then rolls down a gutter until it reaches θ = 0. On the top-right plot,\n",
      "1 2\n",
      "the contours represent the same cost function plus an ℓ penalty with α = 0.5. The\n",
      "1\n",
      "global minimum is on the θ = 0 axis. BGD first reaches θ = 0, then rolls down the\n",
      "2 2\n",
      "gutter until it reaches the global minimum. The two bottom plots show the same\n",
      "thing but uses an ℓ penalty instead. The regularized minimum is closer to θ = 0 than\n",
      "2\n",
      "the unregularized minimum, but the weights do not get fully eliminated. Figure 4-19.\n",
      "\n",
      "Q: = 0, then rolls down the 2 2 gutter until it reaches the global minimum.\n",
      "A: BGD\n",
      "\n",
      "Q: : the l penalty, and the 1 triangles show the BGD path for this\n",
      "A: \n",
      "1\n",
      "\n",
      "Q: = 0, then rolls down a gutter until it reaches  = 0\n",
      "A: the path first\n",
      "130 | Chapter 4: Training Models\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 229:\n",
      "Lasso versus Ridge regularization\n",
      "On the Lasso cost function, the BGD path tends to bounce across\n",
      "the gutter toward the end. This is because the slope changes\n",
      "abruptly at θ = 0. You need to gradually reduce the learning rate in\n",
      "2\n",
      "order to actually converge to the global minimum. The Lasso cost function is not differentiable at θ = 0 (for i = 1, 2, ⋯, n), but Gradient\n",
      "i\n",
      "Descent still works fine if you use a subgradient vector g15 instead when any θ = 0.\n",
      "i\n",
      "Equation 4-11 shows a subgradient vector equation you can use for Gradient Descent\n",
      "with the Lasso cost function. 15 You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra‐\n",
      "dient vectors around that point. Regularized Linear Models | 131\n",
      "Equation 4-11. Lasso Regression subgradient vector\n",
      "sign θ\n",
      "1 −1 if θ <0\n",
      "i\n",
      "sign θ\n",
      "g θ,J =∇ MSE θ +α 2 where sign θ = 0 if θ =0\n",
      "θ i i\n",
      "⋮\n",
      "+1 if θ >0\n",
      "i\n",
      "sign θ\n",
      "n\n",
      "Here is a small Scikit-Learn example using the Lasso class. Note that you could\n",
      "instead use an SGDRegressor(penalty=\"l1\").\n",
      "\n",
      "Q: g15 instead when any  = 0. i Equation 4-11 shows a\n",
      "A: Gradient\n",
      "i\n",
      "Descent\n",
      "\n",
      "Q: Lasso versus Ridge regularization On the Lasso cost function, the BGD path\n",
      "A: \n",
      "abruptly at θ = 0\n",
      "\n",
      "Q: Fragen: This is because the slope changes abruptly at  = 0 You need to gradually\n",
      "A: reduce the learning rate\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 230:\n",
      ">>> from sklearn.linear_model import Lasso\n",
      ">>> lasso_reg = Lasso(alpha=0.1)\n",
      ">>> lasso_reg.fit(X, y)\n",
      ">>> lasso_reg.predict([[1.5]])\n",
      "array([ 1.53788174])\n",
      "Elastic Net\n",
      "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The\n",
      "regularization term is a simple mix of both Ridge and Lasso’s regularization terms,\n",
      "and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
      "Regression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12). Equation 4-12. Elastic Net cost function\n",
      "n n\n",
      "J θ =MSE θ +rα ∑ θ + 1−r α ∑ θ2\n",
      "i 2 i\n",
      "i=1 i=1\n",
      "So when should you use Linear Regression, Ridge, Lasso, or Elastic Net? It is almost\n",
      "always preferable to have at least a little bit of regularization, so generally you should\n",
      "avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a\n",
      "few features are actually useful, you should prefer Lasso or Elastic Net since they tend\n",
      "to reduce the useless features’ weights down to zero as we have discussed.\n",
      "\n",
      "Q: ([1.5]]) array([ 1.53788174]) Elastic Net El\n",
      "A: lasso_reg.predict\n",
      "\n",
      "Q: Elastic Net Elastic Net Elastic Net Elastic Net is a middle ground between Ridge\n",
      "A: Lasso Regression\n",
      "\n",
      "Q: ization term is a simple mix of both Ridge and Lasso’s regularization terms\n",
      "A: \n",
      "regularization\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 231:\n",
      "In general,\n",
      "Elastic Net is preferred over Lasso since Lasso may behave erratically when the num‐\n",
      "ber of features is greater than the number of training instances or when several fea‐\n",
      "tures are strongly correlated. Here is a short example using Scikit-Learn’s ElasticNet (l1_ratio corresponds to\n",
      "the mix ratio r):\n",
      ">>> from sklearn.linear_model import ElasticNet\n",
      ">>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
      ">>> elastic_net.fit(X, y)\n",
      ">>> elastic_net.predict([[1.5]])\n",
      "array([ 1.54333232])\n",
      "132 | Chapter 4: Training Models\n",
      "Early Stopping\n",
      "A very different way to regularize iterative learning algorithms such as Gradient\n",
      "Descent is to stop training as soon as the validation error reaches a minimum. This is\n",
      "called early stopping. Figure 4-20 shows a complex model (in this case a high-degree\n",
      "Polynomial Regression model) being trained using Batch Gradient Descent. As the\n",
      "epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\n",
      "naturally goes down, and so does its prediction error on the validation set. However,\n",
      "after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data.\n",
      "\n",
      "Q: r) 132 | Chapter 4: Training Models Early Stopping . Figure 4\n",
      "A: \n",
      "\n",
      "\n",
      "Q: r = r) >>> elastic_net = ElasticNet(alpha=\n",
      "A: 0.1\n",
      "\n",
      "Q: = 0.01, l1_ratio=0.5) >>> elastic_net =\n",
      "A: ElasticNet\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 232:\n",
      "With early stop‐\n",
      "ping you just stop training as soon as the validation error reaches the minimum. It is\n",
      "such a simple and efficient regularization technique that Geoffrey Hinton called it a\n",
      "“beautiful free lunch.”\n",
      "Figure 4-20. Early stopping regularization\n",
      "With Stochastic and Mini-batch Gradient Descent, the curves are\n",
      "not so smooth, and it may be hard to know whether you have\n",
      "reached the minimum or not. One solution is to stop only after the\n",
      "validation error has been above the minimum for some time (when\n",
      "you are confident that the model will not do any better), then roll\n",
      "back the model parameters to the point where the validation error\n",
      "was at a minimum.\n",
      "\n",
      "Q: ping you just stop training as soon as the validation error reaches the minimum .\n",
      "A: early stop‐\n",
      "\n",
      "Q: pingpingping ping Stop ping ping stop training\n",
      "A: \n",
      "ping you just stop training as soon as the validation error reaches the minimum\n",
      "\n",
      "Q: regularization With Stochastic and Mini-batch Gradient Descent, the curves\n",
      "A: not so smooth\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 233:\n",
      "Here is a basic implementation of early stopping:\n",
      "from sklearn.base import clone\n",
      "Regularized Linear Models | 133\n",
      "sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,\n",
      "learning_rate=\"constant\", eta0=0.0005)\n",
      "minimum_val_error = float(\"inf\")\n",
      "best_epoch = None\n",
      "best_model = None\n",
      "for epoch in range(1000):\n",
      "sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off\n",
      "y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
      "val_error = mean_squared_error(y_val_predict, y_val)\n",
      "if val_error < minimum_val_error:\n",
      "minimum_val_error = val_error\n",
      "best_epoch = epoch\n",
      "best_model = clone(sgd_reg)\n",
      "Note that with warm_start=True, when the fit() method is called, it just continues\n",
      "training where it left off instead of restarting from scratch. Logistic Regression\n",
      "As we discussed in Chapter 1, some regression algorithms can be used for classifica‐\n",
      "tion as well (and vice versa). Logistic Regression (also called Logit Regression) is com‐\n",
      "monly used to estimate the probability that an instance belongs to a particular class\n",
      "(e.g., what is the probability that this email is spam?). If the estimated probability is\n",
      "greater than 50%, then the model predicts that the instance belongs to that class\n",
      "(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\n",
      "belongs to the negative class, labeled “0”). This makes it a binary classifier. Estimating Probabilities\n",
      "So how does it work?\n",
      "\n",
      "Q: clone Regularized Linear Models | 133 sgd_re\n",
      "A: positive class\n",
      "\n",
      "Q: : sklearn.base import clone Regularized Linear Model\n",
      "A: positive class\n",
      "\n",
      "Q: tion. Logit Regression (also called Logit Regression) is com\n",
      "A: used to estimate the probability that an instance belongs to a particular class\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 234:\n",
      "Just like a Linear Regression model, a Logistic Regression\n",
      "model computes a weighted sum of the input features (plus a bias term), but instead\n",
      "of outputting the result directly like the Linear Regression model does, it outputs the\n",
      "logistic of this result (see Equation 4-13). Equation 4-13. Logistic Regression model estimated probability (vectorized form)\n",
      "p=h  =σ θT · \n",
      "θ\n",
      "The logistic—also called the logit, noted σ(·)—is a sigmoid function (i.e., S-shaped)\n",
      "that outputs a number between 0 and 1. It is defined as shown in Equation 4-14 and\n",
      "Figure 4-21. 134 | Chapter 4: Training Models\n",
      "Equation 4-14. Logistic function\n",
      "1\n",
      "σ t =\n",
      "1+ exp −t\n",
      "Figure 4-21. Logistic function\n",
      "Once the Logistic Regression model has estimated the probability p = h (x) that an\n",
      "θ\n",
      "instance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\n",
      "tion 4-15). Equation 4-15. Logistic Regression model prediction\n",
      "0 if p<0.5,\n",
      "y =\n",
      "1 if p≥0.5. Notice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\n",
      "model predicts 1 if θT · x is positive, and 0 if it is negative.\n",
      "\n",
      "Q: a Linear Regression model computes a weighted sum of the input features\n",
      "A: ŷ\n",
      "\n",
      "Q: Linear Regression model, a Logistic Regression model computes a weighte\n",
      "A: \n",
      "logistic of this result\n",
      "\n",
      "Q: questions: Equation 4-13 Logistic Regression model estimated probability (vectorized form)\n",
      "A: p=h  =σ θT · \n",
      "θ\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 235:\n",
      "Training and Cost Function\n",
      "Good, now you know how a Logistic Regression model estimates probabilities and\n",
      "makes predictions. But how is it trained? The objective of training is to set the param‐\n",
      "eter vector θ so that the model estimates high probabilities for positive instances (y =\n",
      "1) and low probabilities for negative instances (y = 0). This idea is captured by the\n",
      "cost function shown in Equation 4-16 for a single training instance x. Equation 4-16. Cost function of a single training instance\n",
      "− log p if y=1,\n",
      "c θ =\n",
      "− log 1−p if y=0. This cost function makes sense because – log(t) grows very large when t approaches\n",
      "0, so the cost will be large if the model estimates a probability close to 0 for a positive\n",
      "Logistic Regression | 135\n",
      "instance, and it will also be very large if the model estimates a probability close to 1\n",
      "for a negative instance.\n",
      "\n",
      "Q: 0 and low probability for negative instances (y = 1) . This idea is captured by\n",
      "A: the\n",
      "cost function\n",
      "\n",
      "Q: the probability of positive instances (y = 1) and low probabilities for negative instances (y =\n",
      "A: param‐\n",
      "eter vector θ\n",
      "\n",
      "Q: x = 0 x = 0 x = 0 x = 0\n",
      "A: \n",
      "− log 1−\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 236:\n",
      "On the other hand, – log(t) is close to 0 when t is close to 1, so\n",
      "the cost will be close to 0 if the estimated probability is close to 0 for a negative\n",
      "instance or close to 1 for a positive instance, which is precisely what we want. The cost function over the whole training set is simply the average cost over all train‐\n",
      "ing instances. It can be written in a single expression (as you can verify easily), called\n",
      "the log loss, shown in Equation 4-17. Equation 4-17. Logistic Regression cost function (log loss)\n",
      "m\n",
      "J θ = − 1 ∑ yi log pi + 1−yi log 1−pi\n",
      "m\n",
      "i=1\n",
      "The bad news is that there is no known closed-form equation to compute the value of\n",
      "θ that minimizes this cost function (there is no equivalent of the Normal Equation). But the good news is that this cost function is convex, so Gradient Descent (or any\n",
      "other optimization algorithm) is guaranteed to find the global minimum (if the learn‐\n",
      "ing rate is not too large and you wait long enough).\n",
      "\n",
      "Q: 0 when t is close to 1, so the cost will be close to 0\n",
      "A: – log(t)\n",
      "\n",
      "Q: – log(t) is close to 0 when t is close to 1, so the\n",
      "A: the cost will be close to 0\n",
      "\n",
      "Q: ing instances. The cost function over the entire training set is simply the average cost over all\n",
      "A: train‐\n",
      "ing instances\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 237:\n",
      "The partial derivatives of the cost\n",
      "function with regards to the jth model parameter θ is given by Equation 4-18.\n",
      "j\n",
      "Equation 4-18. Logistic cost function partial derivatives\n",
      "m\n",
      "∂ J θ = 1 ∑ σ θT ·  i −yi xi\n",
      "∂θ m j\n",
      "j i=1\n",
      "This equation looks very much like Equation 4-5: for each instance it computes the\n",
      "prediction error and multiplies it by the jth feature value, and then it computes the\n",
      "average over all training instances. Once you have the gradient vector containing all\n",
      "the partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\n",
      "it: you now know how to train a Logistic Regression model. For Stochastic GD you\n",
      "would of course just take one instance at a time, and for Mini-batch GD you would\n",
      "use a mini-batch at a time. Decision Boundaries\n",
      "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\n",
      "contains the sepal and petal length and width of 150 iris flowers of three different\n",
      "species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22). 136 | Chapter 4: Training Models\n",
      "Figure 4-22.\n",
      "\n",
      "Q: T  i yi xi\n",
      "A: \n",
      "∂θ m j\n",
      "j i=1\n",
      "\n",
      "Q: questions: The partial derivatives of the cost function with regards to the jth model parameter\n",
      "A: \n",
      "m\n",
      "\n",
      "\n",
      "Q: i=1 This equation looks very much like Equation 4-5: for each instance it\n",
      "A: j\n",
      "j\n",
      "\n",
      "==================================================\n",
      "\n",
      "Passage 238:\n",
      "Flowers of three iris plant species16\n",
      "Let’s try to build a classifier to detect the Iris-Virginica type based only on the petal\n",
      "width feature. First let’s load the data:\n",
      ">>> from sklearn import datasets\n",
      ">>> iris = datasets.load_iris()\n",
      ">>> list(iris.keys())\n",
      "['data', 'target_names', 'feature_names', 'target', 'DESCR']\n",
      ">>> X = iris[\"data\"][:, 3:] # petal width\n",
      ">>> y = (iris[\"target\"] == 2).astype(np.int) # 1 if Iris-Virginica, else 0\n",
      "Now let’s train a Logistic Regression model:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(X, y)\n",
      "Let’s look at the model’s estimated probabilities for flowers with petal widths varying\n",
      "from 0 to 3 cm (Figure 4-23):\n",
      "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
      "y_proba = log_reg.predict_proba(X_new)\n",
      "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
      "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\n",
      "# + more Matplotlib code to make the image look pretty\n",
      "16 Photos reproduced from the corresponding Wikipedia pages. Iris-Virginica photo by Frank Mayfield (Crea‐\n",
      "tive Commons BY-SA 2.0), Iris-Versicolor photo by D. Gordon E. Robertson (Creative Commons BY-SA 3.0),\n",
      "and Iris-Setosa photo is public domain. Logistic Regression | 137\n",
      "Figure 4-23.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Call the function to generate and answer unique questions from the passages\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43manswer_unique_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_pipeline\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 83\u001b[0m, in \u001b[0;36manswer_unique_questions\u001b[1;34m(passages, qa_pipeline)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, passage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(passages):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpassage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m     questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m question \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m answered_questions:\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;66;03m# Get answer using the QA pipeline\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 56\u001b[0m, in \u001b[0;36mgenerate_questions_pipeline\u001b[1;34m(passage, min_questions)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_questions_pipeline\u001b[39m(passage, min_questions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     55\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate questions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 56\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mqg_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     questions \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<sep>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Remove duplicates and empty strings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:173\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[0;32m    178\u001b[0m     ):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1362\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1355\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         )\n\u001b[0;32m   1360\u001b[0m     )\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1369\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1368\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1369\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1269\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1268\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1269\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1270\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:202\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    200\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 202\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3261\u001b[0m     outputs,\n\u001b[0;32m   3262\u001b[0m     model_kwargs,\n\u001b[0;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3264\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1891\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1888\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1891\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1907\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1909\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1109\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         cache_position,\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:725\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    722\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> 725\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:339\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    338\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 339\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:285\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    284\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi(hidden_states)\n\u001b[1;32m--> 285\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[0;32m    291\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data for sentence tokenization\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Define paths\n",
    "pdf_path = r\"C:\\Users\\gurur\\OneDrive\\Documents\\python\\code\\Hands-On Machine Learning with Scikit-Learn and TensorFlow_ Concepts, Tools, and Techniques to Build Intelligent Systems ( PDFDrive ).pdf\"\n",
    "output_text_file = \"extracted_text.txt\"\n",
    "\n",
    "# Load the summarization and question generation pipelines\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "\n",
    "# Function to extract text from the PDF\n",
    "def extract_pdf_text(pdf_path):\n",
    "    document_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                document_text += text + \"\\n\"\n",
    "    return document_text\n",
    "\n",
    "# Extract text from the PDF\n",
    "document_text = extract_pdf_text(pdf_path)\n",
    "\n",
    "# Save extracted text to a file\n",
    "with open(output_text_file, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(document_text)\n",
    "\n",
    "# Print preview of the extracted text\n",
    "print(\"Preview of extracted text:\")\n",
    "print(document_text[:2000])\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = sent_tokenize(document_text)\n",
    "\n",
    "# Combine sentences into passages (max 200 words per passage)\n",
    "passages = []\n",
    "current_passage = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(current_passage.split()) + len(sentence.split()) < 200:  # Adjust limit if necessary\n",
    "        current_passage += \" \" + sentence\n",
    "    else:\n",
    "        passages.append(current_passage.strip())\n",
    "        current_passage = sentence\n",
    "if current_passage:\n",
    "    passages.append(current_passage.strip())\n",
    "\n",
    "# Function to generate questions from a passage\n",
    "def generate_questions_pipeline(passage, min_questions=3):\n",
    "    input_text = f\"generate questions: {passage}\"\n",
    "    results = qg_pipeline(input_text)\n",
    "    questions = results[0]['generated_text'].split('<sep>')\n",
    "    \n",
    "    # Remove duplicates and empty strings\n",
    "    questions = [q.strip() for q in questions if q.strip()]\n",
    "    \n",
    "    # If fewer than required questions, generate more from smaller parts\n",
    "    if len(questions) < min_questions:\n",
    "        passage_sentences = passage.split('. ')\n",
    "        for i in range(len(passage_sentences)):\n",
    "            if len(questions) >= min_questions:\n",
    "                break\n",
    "            additional_input = ' '.join(passage_sentences[i:i+2])\n",
    "            additional_results = qg_pipeline(f\"generate questions: {additional_input}\")\n",
    "            additional_questions = additional_results[0]['generated_text'].split('<sep>')\n",
    "            questions.extend([q.strip() for q in additional_questions if q.strip()])\n",
    "    \n",
    "    return questions[:min_questions]  # Return only the top questions\n",
    "\n",
    "# Generate questions from passages and answer them\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def answer_unique_questions(passages, qa_pipeline):\n",
    "    answered_questions = set()  # Store answered questions to avoid repetition\n",
    "\n",
    "    for idx, passage in enumerate(passages):\n",
    "        print(f\"Passage {idx + 1}:\\n{passage}\\n\")\n",
    "        questions = generate_questions_pipeline(passage)\n",
    "        \n",
    "        for question in questions:\n",
    "            if question not in answered_questions:\n",
    "                # Get answer using the QA pipeline\n",
    "                answer = qa_pipeline({'question': question, 'context': passage})\n",
    "                print(f\"Q: {question}\")\n",
    "                print(f\"A: {answer['answer']}\\n\")\n",
    "                answered_questions.add(question)  # Avoid answering the same question again\n",
    "        \n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Call the function to generate and answer unique questions from the passages\n",
    "answer_unique_questions(passages, qa_pipeline)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
